{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "k4rTr6szCy9s",
        "GpxND-LBC_Tf",
        "QRnQitcODO5B",
        "4zHz8umjDcyR",
        "9f7LzOiLDndQ",
        "sPFgAo8y0hEq"
      ],
      "authorship_tag": "ABX9TyPBlK+qbp3G/kVO3XfhYYGT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c2636afc1b047a3bc1f9930bf61f88d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40614a8b655e4eba849054719b0b9709",
              "IPY_MODEL_e10037051514402fbe9fb471bc78dd08",
              "IPY_MODEL_3da4e4e4c6f3425587cae18632cd1ac3"
            ],
            "layout": "IPY_MODEL_c76098701a20496298bf391523281cb8"
          }
        },
        "40614a8b655e4eba849054719b0b9709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_457bff1eae7b4aea8516684d5b1126b9",
            "placeholder": "​",
            "style": "IPY_MODEL_2eeaa9a6a13149dc9940726842237f31",
            "value": "100%"
          }
        },
        "e10037051514402fbe9fb471bc78dd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df2eaa8b882d43e78bfd3bf0cd8bcd78",
            "max": 77999237,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83461999aed0477cb7ae92973355a764",
            "value": 77999237
          }
        },
        "3da4e4e4c6f3425587cae18632cd1ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34670d8bc4544fa59075bee9a0737432",
            "placeholder": "​",
            "style": "IPY_MODEL_9c911d58a131413d8f68c81252e7a1a5",
            "value": " 74.4M/74.4M [00:06&lt;00:00, 11.9MB/s]"
          }
        },
        "c76098701a20496298bf391523281cb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "457bff1eae7b4aea8516684d5b1126b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eeaa9a6a13149dc9940726842237f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df2eaa8b882d43e78bfd3bf0cd8bcd78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83461999aed0477cb7ae92973355a764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "34670d8bc4544fa59075bee9a0737432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c911d58a131413d8f68c81252e7a1a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chinmaya-3141/Capstone-Project/blob/main/Capstone_Classification_EfficientNetB4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EfficientNet-B4**"
      ],
      "metadata": {
        "id": "BWeXcctAC5Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import necessary libraries"
      ],
      "metadata": {
        "id": "k4rTr6szCy9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!pip show torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2ebdJ-I796R",
        "outputId": "c0d11aae-37fc-4eb9-d809-a8d9e919345a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n",
            "Name: torch\n",
            "Version: 1.13.1+cu116\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3\n",
            "Location: /usr/local/lib/python3.9/dist-packages\n",
            "Requires: typing-extensions\n",
            "Required-by: fastai, torchaudio, torchtext, torchvision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Python 3.9.16\\\n",
        "Name: torch\\\n",
        "Version: 1.13.1+cu116\\\n",
        "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\\\n",
        "Home-page: https://pytorch.org/\\\n",
        "Author: PyTorch Team\\\n",
        "Author-email: packages@pytorch.org\\\n",
        "License: BSD-3\\\n",
        "Location: /usr/local/lib/python3.9/dist-packages\\\n",
        "Requires: typing-extensions\\\n",
        "Required-by: fastai, torchaudio, torchtext, torchvision\\\n"
      ],
      "metadata": {
        "id": "VH3xCD3JCl44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo -qq\n",
        "!pip install efficientnet_pytorch -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF8GkYFL6aqF",
        "outputId": "bf2ef1fd-41f1-4c0f-ede7-c2238e92ab5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2ZoL_Ug53pN",
        "outputId": "ab3865f3-99e4-4ce4-a033-a091aa1568ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from numba import cuda\n",
        "import PIL\n",
        "from time import time"
      ],
      "metadata": {
        "id": "jJ0vp-oA6Ep3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "MA5XhpDK6Lwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchsummary import summary as tssum\n",
        "from torchinfo import summary as tisum"
      ],
      "metadata": {
        "id": "0Et4zkk26Ent"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchvision.models import efficientnet_b7 as en7\n",
        "from efficientnet_pytorch import EfficientNet as en4"
      ],
      "metadata": {
        "id": "hiwXlcay7G6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and transform data"
      ],
      "metadata": {
        "id": "GpxND-LBC_Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Capstone Data (Shared)/Capstone Data/Resized Data/Resized Data_320x320'"
      ],
      "metadata": {
        "id": "BqaCHGFKBY46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transform for the data\n",
        "# Try fitting model without augmenting pneumonia data\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((380,380)),\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "AL0n71ZtB3Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datavar = ImageFolder(root=path, transform=transform)"
      ],
      "metadata": {
        "id": "Gs_OQnF5ByKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = torch.utils.data.random_split(datavar, [int(0.8*len(datavar)), len(datavar)-int(0.8*len(datavar))])"
      ],
      "metadata": {
        "id": "D50PmE9VBY1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the data loaders\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batchsize, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batchsize, shuffle=False)"
      ],
      "metadata": {
        "id": "LaMYDnlgBYys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "QRnQitcODO5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batchsize = 20"
      ],
      "metadata": {
        "id": "_kaoXjY4Skty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dimension=380"
      ],
      "metadata": {
        "id": "muDt9IEKSskq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "channels = 3"
      ],
      "metadata": {
        "id": "XKkqZF5RS061"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "id": "sOrrMSDa6ElA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b72a6a20-534c-4e54-8e85-6b9f02b12f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EfficientNetB7 requires 600*600 images\n",
        "\n",
        "model = en4.from_pretrained('efficientnet-b4', num_classes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "8c2636afc1b047a3bc1f9930bf61f88d",
            "40614a8b655e4eba849054719b0b9709",
            "e10037051514402fbe9fb471bc78dd08",
            "3da4e4e4c6f3425587cae18632cd1ac3",
            "c76098701a20496298bf391523281cb8",
            "457bff1eae7b4aea8516684d5b1126b9",
            "2eeaa9a6a13149dc9940726842237f31",
            "df2eaa8b882d43e78bfd3bf0cd8bcd78",
            "83461999aed0477cb7ae92973355a764",
            "34670d8bc4544fa59075bee9a0737432",
            "9c911d58a131413d8f68c81252e7a1a5"
          ]
        },
        "id": "Z92J0S7j6_-U",
        "outputId": "3d04d9b2-bb08-423d-8289-c7ae36fd63d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/74.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c2636afc1b047a3bc1f9930bf61f88d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model to device\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "I1Vv5Vjc6Eia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10489662-01b2-4d85-ddfb-54a83282f22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (_conv_stem): Conv2dStaticSamePadding(\n",
              "    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
              "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "  )\n",
              "  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_blocks): ModuleList(\n",
              "    (0): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (1): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (2): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (3): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (4): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (5): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (6): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (7): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (8): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (9): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (10): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (11): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (12): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (13): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (14): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (15): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (16): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (17): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (18): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (19): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (20): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (21): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (22): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (23): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (24): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (25): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (26): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (27): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (28): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (29): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (30): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (31): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "  )\n",
              "  (_conv_head): Conv2dStaticSamePadding(\n",
              "    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "    (static_padding): Identity()\n",
              "  )\n",
              "  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
              "  (_dropout): Dropout(p=0.4, inplace=False)\n",
              "  (_fc): Linear(in_features=1792, out_features=3, bias=True)\n",
              "  (_swish): MemoryEfficientSwish()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the summary of the model\n",
        "\n",
        "print(tssum(model, input_size=(channels,dimension,dimension)))"
      ],
      "metadata": {
        "id": "HfEk_LrX6Efl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c220868-3258-49fb-fd9b-db80025e6ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "         ZeroPad2d-1          [-1, 3, 381, 381]               0\n",
            "Conv2dStaticSamePadding-2         [-1, 48, 190, 190]           1,296\n",
            "       BatchNorm2d-3         [-1, 48, 190, 190]              96\n",
            "MemoryEfficientSwish-4         [-1, 48, 190, 190]               0\n",
            "         ZeroPad2d-5         [-1, 48, 192, 192]               0\n",
            "Conv2dStaticSamePadding-6         [-1, 48, 190, 190]             432\n",
            "       BatchNorm2d-7         [-1, 48, 190, 190]              96\n",
            "MemoryEfficientSwish-8         [-1, 48, 190, 190]               0\n",
            "          Identity-9             [-1, 48, 1, 1]               0\n",
            "Conv2dStaticSamePadding-10             [-1, 12, 1, 1]             588\n",
            "MemoryEfficientSwish-11             [-1, 12, 1, 1]               0\n",
            "         Identity-12             [-1, 12, 1, 1]               0\n",
            "Conv2dStaticSamePadding-13             [-1, 48, 1, 1]             624\n",
            "         Identity-14         [-1, 48, 190, 190]               0\n",
            "Conv2dStaticSamePadding-15         [-1, 24, 190, 190]           1,152\n",
            "      BatchNorm2d-16         [-1, 24, 190, 190]              48\n",
            "      MBConvBlock-17         [-1, 24, 190, 190]               0\n",
            "        ZeroPad2d-18         [-1, 24, 192, 192]               0\n",
            "Conv2dStaticSamePadding-19         [-1, 24, 190, 190]             216\n",
            "      BatchNorm2d-20         [-1, 24, 190, 190]              48\n",
            "MemoryEfficientSwish-21         [-1, 24, 190, 190]               0\n",
            "         Identity-22             [-1, 24, 1, 1]               0\n",
            "Conv2dStaticSamePadding-23              [-1, 6, 1, 1]             150\n",
            "MemoryEfficientSwish-24              [-1, 6, 1, 1]               0\n",
            "         Identity-25              [-1, 6, 1, 1]               0\n",
            "Conv2dStaticSamePadding-26             [-1, 24, 1, 1]             168\n",
            "         Identity-27         [-1, 24, 190, 190]               0\n",
            "Conv2dStaticSamePadding-28         [-1, 24, 190, 190]             576\n",
            "      BatchNorm2d-29         [-1, 24, 190, 190]              48\n",
            "      MBConvBlock-30         [-1, 24, 190, 190]               0\n",
            "         Identity-31         [-1, 24, 190, 190]               0\n",
            "Conv2dStaticSamePadding-32        [-1, 144, 190, 190]           3,456\n",
            "      BatchNorm2d-33        [-1, 144, 190, 190]             288\n",
            "MemoryEfficientSwish-34        [-1, 144, 190, 190]               0\n",
            "        ZeroPad2d-35        [-1, 144, 191, 191]               0\n",
            "Conv2dStaticSamePadding-36          [-1, 144, 95, 95]           1,296\n",
            "      BatchNorm2d-37          [-1, 144, 95, 95]             288\n",
            "MemoryEfficientSwish-38          [-1, 144, 95, 95]               0\n",
            "         Identity-39            [-1, 144, 1, 1]               0\n",
            "Conv2dStaticSamePadding-40              [-1, 6, 1, 1]             870\n",
            "MemoryEfficientSwish-41              [-1, 6, 1, 1]               0\n",
            "         Identity-42              [-1, 6, 1, 1]               0\n",
            "Conv2dStaticSamePadding-43            [-1, 144, 1, 1]           1,008\n",
            "         Identity-44          [-1, 144, 95, 95]               0\n",
            "Conv2dStaticSamePadding-45           [-1, 32, 95, 95]           4,608\n",
            "      BatchNorm2d-46           [-1, 32, 95, 95]              64\n",
            "      MBConvBlock-47           [-1, 32, 95, 95]               0\n",
            "         Identity-48           [-1, 32, 95, 95]               0\n",
            "Conv2dStaticSamePadding-49          [-1, 192, 95, 95]           6,144\n",
            "      BatchNorm2d-50          [-1, 192, 95, 95]             384\n",
            "MemoryEfficientSwish-51          [-1, 192, 95, 95]               0\n",
            "        ZeroPad2d-52          [-1, 192, 97, 97]               0\n",
            "Conv2dStaticSamePadding-53          [-1, 192, 95, 95]           1,728\n",
            "      BatchNorm2d-54          [-1, 192, 95, 95]             384\n",
            "MemoryEfficientSwish-55          [-1, 192, 95, 95]               0\n",
            "         Identity-56            [-1, 192, 1, 1]               0\n",
            "Conv2dStaticSamePadding-57              [-1, 8, 1, 1]           1,544\n",
            "MemoryEfficientSwish-58              [-1, 8, 1, 1]               0\n",
            "         Identity-59              [-1, 8, 1, 1]               0\n",
            "Conv2dStaticSamePadding-60            [-1, 192, 1, 1]           1,728\n",
            "         Identity-61          [-1, 192, 95, 95]               0\n",
            "Conv2dStaticSamePadding-62           [-1, 32, 95, 95]           6,144\n",
            "      BatchNorm2d-63           [-1, 32, 95, 95]              64\n",
            "      MBConvBlock-64           [-1, 32, 95, 95]               0\n",
            "         Identity-65           [-1, 32, 95, 95]               0\n",
            "Conv2dStaticSamePadding-66          [-1, 192, 95, 95]           6,144\n",
            "      BatchNorm2d-67          [-1, 192, 95, 95]             384\n",
            "MemoryEfficientSwish-68          [-1, 192, 95, 95]               0\n",
            "        ZeroPad2d-69          [-1, 192, 97, 97]               0\n",
            "Conv2dStaticSamePadding-70          [-1, 192, 95, 95]           1,728\n",
            "      BatchNorm2d-71          [-1, 192, 95, 95]             384\n",
            "MemoryEfficientSwish-72          [-1, 192, 95, 95]               0\n",
            "         Identity-73            [-1, 192, 1, 1]               0\n",
            "Conv2dStaticSamePadding-74              [-1, 8, 1, 1]           1,544\n",
            "MemoryEfficientSwish-75              [-1, 8, 1, 1]               0\n",
            "         Identity-76              [-1, 8, 1, 1]               0\n",
            "Conv2dStaticSamePadding-77            [-1, 192, 1, 1]           1,728\n",
            "         Identity-78          [-1, 192, 95, 95]               0\n",
            "Conv2dStaticSamePadding-79           [-1, 32, 95, 95]           6,144\n",
            "      BatchNorm2d-80           [-1, 32, 95, 95]              64\n",
            "      MBConvBlock-81           [-1, 32, 95, 95]               0\n",
            "         Identity-82           [-1, 32, 95, 95]               0\n",
            "Conv2dStaticSamePadding-83          [-1, 192, 95, 95]           6,144\n",
            "      BatchNorm2d-84          [-1, 192, 95, 95]             384\n",
            "MemoryEfficientSwish-85          [-1, 192, 95, 95]               0\n",
            "        ZeroPad2d-86          [-1, 192, 97, 97]               0\n",
            "Conv2dStaticSamePadding-87          [-1, 192, 95, 95]           1,728\n",
            "      BatchNorm2d-88          [-1, 192, 95, 95]             384\n",
            "MemoryEfficientSwish-89          [-1, 192, 95, 95]               0\n",
            "         Identity-90            [-1, 192, 1, 1]               0\n",
            "Conv2dStaticSamePadding-91              [-1, 8, 1, 1]           1,544\n",
            "MemoryEfficientSwish-92              [-1, 8, 1, 1]               0\n",
            "         Identity-93              [-1, 8, 1, 1]               0\n",
            "Conv2dStaticSamePadding-94            [-1, 192, 1, 1]           1,728\n",
            "         Identity-95          [-1, 192, 95, 95]               0\n",
            "Conv2dStaticSamePadding-96           [-1, 32, 95, 95]           6,144\n",
            "      BatchNorm2d-97           [-1, 32, 95, 95]              64\n",
            "      MBConvBlock-98           [-1, 32, 95, 95]               0\n",
            "         Identity-99           [-1, 32, 95, 95]               0\n",
            "Conv2dStaticSamePadding-100          [-1, 192, 95, 95]           6,144\n",
            "     BatchNorm2d-101          [-1, 192, 95, 95]             384\n",
            "MemoryEfficientSwish-102          [-1, 192, 95, 95]               0\n",
            "       ZeroPad2d-103          [-1, 192, 99, 99]               0\n",
            "Conv2dStaticSamePadding-104          [-1, 192, 48, 48]           4,800\n",
            "     BatchNorm2d-105          [-1, 192, 48, 48]             384\n",
            "MemoryEfficientSwish-106          [-1, 192, 48, 48]               0\n",
            "        Identity-107            [-1, 192, 1, 1]               0\n",
            "Conv2dStaticSamePadding-108              [-1, 8, 1, 1]           1,544\n",
            "MemoryEfficientSwish-109              [-1, 8, 1, 1]               0\n",
            "        Identity-110              [-1, 8, 1, 1]               0\n",
            "Conv2dStaticSamePadding-111            [-1, 192, 1, 1]           1,728\n",
            "        Identity-112          [-1, 192, 48, 48]               0\n",
            "Conv2dStaticSamePadding-113           [-1, 56, 48, 48]          10,752\n",
            "     BatchNorm2d-114           [-1, 56, 48, 48]             112\n",
            "     MBConvBlock-115           [-1, 56, 48, 48]               0\n",
            "        Identity-116           [-1, 56, 48, 48]               0\n",
            "Conv2dStaticSamePadding-117          [-1, 336, 48, 48]          18,816\n",
            "     BatchNorm2d-118          [-1, 336, 48, 48]             672\n",
            "MemoryEfficientSwish-119          [-1, 336, 48, 48]               0\n",
            "       ZeroPad2d-120          [-1, 336, 52, 52]               0\n",
            "Conv2dStaticSamePadding-121          [-1, 336, 48, 48]           8,400\n",
            "     BatchNorm2d-122          [-1, 336, 48, 48]             672\n",
            "MemoryEfficientSwish-123          [-1, 336, 48, 48]               0\n",
            "        Identity-124            [-1, 336, 1, 1]               0\n",
            "Conv2dStaticSamePadding-125             [-1, 14, 1, 1]           4,718\n",
            "MemoryEfficientSwish-126             [-1, 14, 1, 1]               0\n",
            "        Identity-127             [-1, 14, 1, 1]               0\n",
            "Conv2dStaticSamePadding-128            [-1, 336, 1, 1]           5,040\n",
            "        Identity-129          [-1, 336, 48, 48]               0\n",
            "Conv2dStaticSamePadding-130           [-1, 56, 48, 48]          18,816\n",
            "     BatchNorm2d-131           [-1, 56, 48, 48]             112\n",
            "     MBConvBlock-132           [-1, 56, 48, 48]               0\n",
            "        Identity-133           [-1, 56, 48, 48]               0\n",
            "Conv2dStaticSamePadding-134          [-1, 336, 48, 48]          18,816\n",
            "     BatchNorm2d-135          [-1, 336, 48, 48]             672\n",
            "MemoryEfficientSwish-136          [-1, 336, 48, 48]               0\n",
            "       ZeroPad2d-137          [-1, 336, 52, 52]               0\n",
            "Conv2dStaticSamePadding-138          [-1, 336, 48, 48]           8,400\n",
            "     BatchNorm2d-139          [-1, 336, 48, 48]             672\n",
            "MemoryEfficientSwish-140          [-1, 336, 48, 48]               0\n",
            "        Identity-141            [-1, 336, 1, 1]               0\n",
            "Conv2dStaticSamePadding-142             [-1, 14, 1, 1]           4,718\n",
            "MemoryEfficientSwish-143             [-1, 14, 1, 1]               0\n",
            "        Identity-144             [-1, 14, 1, 1]               0\n",
            "Conv2dStaticSamePadding-145            [-1, 336, 1, 1]           5,040\n",
            "        Identity-146          [-1, 336, 48, 48]               0\n",
            "Conv2dStaticSamePadding-147           [-1, 56, 48, 48]          18,816\n",
            "     BatchNorm2d-148           [-1, 56, 48, 48]             112\n",
            "     MBConvBlock-149           [-1, 56, 48, 48]               0\n",
            "        Identity-150           [-1, 56, 48, 48]               0\n",
            "Conv2dStaticSamePadding-151          [-1, 336, 48, 48]          18,816\n",
            "     BatchNorm2d-152          [-1, 336, 48, 48]             672\n",
            "MemoryEfficientSwish-153          [-1, 336, 48, 48]               0\n",
            "       ZeroPad2d-154          [-1, 336, 52, 52]               0\n",
            "Conv2dStaticSamePadding-155          [-1, 336, 48, 48]           8,400\n",
            "     BatchNorm2d-156          [-1, 336, 48, 48]             672\n",
            "MemoryEfficientSwish-157          [-1, 336, 48, 48]               0\n",
            "        Identity-158            [-1, 336, 1, 1]               0\n",
            "Conv2dStaticSamePadding-159             [-1, 14, 1, 1]           4,718\n",
            "MemoryEfficientSwish-160             [-1, 14, 1, 1]               0\n",
            "        Identity-161             [-1, 14, 1, 1]               0\n",
            "Conv2dStaticSamePadding-162            [-1, 336, 1, 1]           5,040\n",
            "        Identity-163          [-1, 336, 48, 48]               0\n",
            "Conv2dStaticSamePadding-164           [-1, 56, 48, 48]          18,816\n",
            "     BatchNorm2d-165           [-1, 56, 48, 48]             112\n",
            "     MBConvBlock-166           [-1, 56, 48, 48]               0\n",
            "        Identity-167           [-1, 56, 48, 48]               0\n",
            "Conv2dStaticSamePadding-168          [-1, 336, 48, 48]          18,816\n",
            "     BatchNorm2d-169          [-1, 336, 48, 48]             672\n",
            "MemoryEfficientSwish-170          [-1, 336, 48, 48]               0\n",
            "       ZeroPad2d-171          [-1, 336, 49, 49]               0\n",
            "Conv2dStaticSamePadding-172          [-1, 336, 24, 24]           3,024\n",
            "     BatchNorm2d-173          [-1, 336, 24, 24]             672\n",
            "MemoryEfficientSwish-174          [-1, 336, 24, 24]               0\n",
            "        Identity-175            [-1, 336, 1, 1]               0\n",
            "Conv2dStaticSamePadding-176             [-1, 14, 1, 1]           4,718\n",
            "MemoryEfficientSwish-177             [-1, 14, 1, 1]               0\n",
            "        Identity-178             [-1, 14, 1, 1]               0\n",
            "Conv2dStaticSamePadding-179            [-1, 336, 1, 1]           5,040\n",
            "        Identity-180          [-1, 336, 24, 24]               0\n",
            "Conv2dStaticSamePadding-181          [-1, 112, 24, 24]          37,632\n",
            "     BatchNorm2d-182          [-1, 112, 24, 24]             224\n",
            "     MBConvBlock-183          [-1, 112, 24, 24]               0\n",
            "        Identity-184          [-1, 112, 24, 24]               0\n",
            "Conv2dStaticSamePadding-185          [-1, 672, 24, 24]          75,264\n",
            "     BatchNorm2d-186          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-187          [-1, 672, 24, 24]               0\n",
            "       ZeroPad2d-188          [-1, 672, 26, 26]               0\n",
            "Conv2dStaticSamePadding-189          [-1, 672, 24, 24]           6,048\n",
            "     BatchNorm2d-190          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-191          [-1, 672, 24, 24]               0\n",
            "        Identity-192            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-193             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-194             [-1, 28, 1, 1]               0\n",
            "        Identity-195             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-196            [-1, 672, 1, 1]          19,488\n",
            "        Identity-197          [-1, 672, 24, 24]               0\n",
            "Conv2dStaticSamePadding-198          [-1, 112, 24, 24]          75,264\n",
            "     BatchNorm2d-199          [-1, 112, 24, 24]             224\n",
            "     MBConvBlock-200          [-1, 112, 24, 24]               0\n",
            "        Identity-201          [-1, 112, 24, 24]               0\n",
            "Conv2dStaticSamePadding-202          [-1, 672, 24, 24]          75,264\n",
            "     BatchNorm2d-203          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-204          [-1, 672, 24, 24]               0\n",
            "       ZeroPad2d-205          [-1, 672, 26, 26]               0\n",
            "Conv2dStaticSamePadding-206          [-1, 672, 24, 24]           6,048\n",
            "     BatchNorm2d-207          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-208          [-1, 672, 24, 24]               0\n",
            "        Identity-209            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-210             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-211             [-1, 28, 1, 1]               0\n",
            "        Identity-212             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-213            [-1, 672, 1, 1]          19,488\n",
            "        Identity-214          [-1, 672, 24, 24]               0\n",
            "Conv2dStaticSamePadding-215          [-1, 112, 24, 24]          75,264\n",
            "     BatchNorm2d-216          [-1, 112, 24, 24]             224\n",
            "     MBConvBlock-217          [-1, 112, 24, 24]               0\n",
            "        Identity-218          [-1, 112, 24, 24]               0\n",
            "Conv2dStaticSamePadding-219          [-1, 672, 24, 24]          75,264\n",
            "     BatchNorm2d-220          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-221          [-1, 672, 24, 24]               0\n",
            "       ZeroPad2d-222          [-1, 672, 26, 26]               0\n",
            "Conv2dStaticSamePadding-223          [-1, 672, 24, 24]           6,048\n",
            "     BatchNorm2d-224          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-225          [-1, 672, 24, 24]               0\n",
            "        Identity-226            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-227             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-228             [-1, 28, 1, 1]               0\n",
            "        Identity-229             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-230            [-1, 672, 1, 1]          19,488\n",
            "        Identity-231          [-1, 672, 24, 24]               0\n",
            "Conv2dStaticSamePadding-232          [-1, 112, 24, 24]          75,264\n",
            "     BatchNorm2d-233          [-1, 112, 24, 24]             224\n",
            "     MBConvBlock-234          [-1, 112, 24, 24]               0\n",
            "        Identity-235          [-1, 112, 24, 24]               0\n",
            "Conv2dStaticSamePadding-236          [-1, 672, 24, 24]          75,264\n",
            "     BatchNorm2d-237          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-238          [-1, 672, 24, 24]               0\n",
            "       ZeroPad2d-239          [-1, 672, 26, 26]               0\n",
            "Conv2dStaticSamePadding-240          [-1, 672, 24, 24]           6,048\n",
            "     BatchNorm2d-241          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-242          [-1, 672, 24, 24]               0\n",
            "        Identity-243            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-244             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-245             [-1, 28, 1, 1]               0\n",
            "        Identity-246             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-247            [-1, 672, 1, 1]          19,488\n",
            "        Identity-248          [-1, 672, 24, 24]               0\n",
            "Conv2dStaticSamePadding-249          [-1, 112, 24, 24]          75,264\n",
            "     BatchNorm2d-250          [-1, 112, 24, 24]             224\n",
            "     MBConvBlock-251          [-1, 112, 24, 24]               0\n",
            "        Identity-252          [-1, 112, 24, 24]               0\n",
            "Conv2dStaticSamePadding-253          [-1, 672, 24, 24]          75,264\n",
            "     BatchNorm2d-254          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-255          [-1, 672, 24, 24]               0\n",
            "       ZeroPad2d-256          [-1, 672, 26, 26]               0\n",
            "Conv2dStaticSamePadding-257          [-1, 672, 24, 24]           6,048\n",
            "     BatchNorm2d-258          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-259          [-1, 672, 24, 24]               0\n",
            "        Identity-260            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-261             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-262             [-1, 28, 1, 1]               0\n",
            "        Identity-263             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-264            [-1, 672, 1, 1]          19,488\n",
            "        Identity-265          [-1, 672, 24, 24]               0\n",
            "Conv2dStaticSamePadding-266          [-1, 112, 24, 24]          75,264\n",
            "     BatchNorm2d-267          [-1, 112, 24, 24]             224\n",
            "     MBConvBlock-268          [-1, 112, 24, 24]               0\n",
            "        Identity-269          [-1, 112, 24, 24]               0\n",
            "Conv2dStaticSamePadding-270          [-1, 672, 24, 24]          75,264\n",
            "     BatchNorm2d-271          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-272          [-1, 672, 24, 24]               0\n",
            "       ZeroPad2d-273          [-1, 672, 28, 28]               0\n",
            "Conv2dStaticSamePadding-274          [-1, 672, 24, 24]          16,800\n",
            "     BatchNorm2d-275          [-1, 672, 24, 24]           1,344\n",
            "MemoryEfficientSwish-276          [-1, 672, 24, 24]               0\n",
            "        Identity-277            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-278             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-279             [-1, 28, 1, 1]               0\n",
            "        Identity-280             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-281            [-1, 672, 1, 1]          19,488\n",
            "        Identity-282          [-1, 672, 24, 24]               0\n",
            "Conv2dStaticSamePadding-283          [-1, 160, 24, 24]         107,520\n",
            "     BatchNorm2d-284          [-1, 160, 24, 24]             320\n",
            "     MBConvBlock-285          [-1, 160, 24, 24]               0\n",
            "        Identity-286          [-1, 160, 24, 24]               0\n",
            "Conv2dStaticSamePadding-287          [-1, 960, 24, 24]         153,600\n",
            "     BatchNorm2d-288          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-289          [-1, 960, 24, 24]               0\n",
            "       ZeroPad2d-290          [-1, 960, 28, 28]               0\n",
            "Conv2dStaticSamePadding-291          [-1, 960, 24, 24]          24,000\n",
            "     BatchNorm2d-292          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-293          [-1, 960, 24, 24]               0\n",
            "        Identity-294            [-1, 960, 1, 1]               0\n",
            "Conv2dStaticSamePadding-295             [-1, 40, 1, 1]          38,440\n",
            "MemoryEfficientSwish-296             [-1, 40, 1, 1]               0\n",
            "        Identity-297             [-1, 40, 1, 1]               0\n",
            "Conv2dStaticSamePadding-298            [-1, 960, 1, 1]          39,360\n",
            "        Identity-299          [-1, 960, 24, 24]               0\n",
            "Conv2dStaticSamePadding-300          [-1, 160, 24, 24]         153,600\n",
            "     BatchNorm2d-301          [-1, 160, 24, 24]             320\n",
            "     MBConvBlock-302          [-1, 160, 24, 24]               0\n",
            "        Identity-303          [-1, 160, 24, 24]               0\n",
            "Conv2dStaticSamePadding-304          [-1, 960, 24, 24]         153,600\n",
            "     BatchNorm2d-305          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-306          [-1, 960, 24, 24]               0\n",
            "       ZeroPad2d-307          [-1, 960, 28, 28]               0\n",
            "Conv2dStaticSamePadding-308          [-1, 960, 24, 24]          24,000\n",
            "     BatchNorm2d-309          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-310          [-1, 960, 24, 24]               0\n",
            "        Identity-311            [-1, 960, 1, 1]               0\n",
            "Conv2dStaticSamePadding-312             [-1, 40, 1, 1]          38,440\n",
            "MemoryEfficientSwish-313             [-1, 40, 1, 1]               0\n",
            "        Identity-314             [-1, 40, 1, 1]               0\n",
            "Conv2dStaticSamePadding-315            [-1, 960, 1, 1]          39,360\n",
            "        Identity-316          [-1, 960, 24, 24]               0\n",
            "Conv2dStaticSamePadding-317          [-1, 160, 24, 24]         153,600\n",
            "     BatchNorm2d-318          [-1, 160, 24, 24]             320\n",
            "     MBConvBlock-319          [-1, 160, 24, 24]               0\n",
            "        Identity-320          [-1, 160, 24, 24]               0\n",
            "Conv2dStaticSamePadding-321          [-1, 960, 24, 24]         153,600\n",
            "     BatchNorm2d-322          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-323          [-1, 960, 24, 24]               0\n",
            "       ZeroPad2d-324          [-1, 960, 28, 28]               0\n",
            "Conv2dStaticSamePadding-325          [-1, 960, 24, 24]          24,000\n",
            "     BatchNorm2d-326          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-327          [-1, 960, 24, 24]               0\n",
            "        Identity-328            [-1, 960, 1, 1]               0\n",
            "Conv2dStaticSamePadding-329             [-1, 40, 1, 1]          38,440\n",
            "MemoryEfficientSwish-330             [-1, 40, 1, 1]               0\n",
            "        Identity-331             [-1, 40, 1, 1]               0\n",
            "Conv2dStaticSamePadding-332            [-1, 960, 1, 1]          39,360\n",
            "        Identity-333          [-1, 960, 24, 24]               0\n",
            "Conv2dStaticSamePadding-334          [-1, 160, 24, 24]         153,600\n",
            "     BatchNorm2d-335          [-1, 160, 24, 24]             320\n",
            "     MBConvBlock-336          [-1, 160, 24, 24]               0\n",
            "        Identity-337          [-1, 160, 24, 24]               0\n",
            "Conv2dStaticSamePadding-338          [-1, 960, 24, 24]         153,600\n",
            "     BatchNorm2d-339          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-340          [-1, 960, 24, 24]               0\n",
            "       ZeroPad2d-341          [-1, 960, 28, 28]               0\n",
            "Conv2dStaticSamePadding-342          [-1, 960, 24, 24]          24,000\n",
            "     BatchNorm2d-343          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-344          [-1, 960, 24, 24]               0\n",
            "        Identity-345            [-1, 960, 1, 1]               0\n",
            "Conv2dStaticSamePadding-346             [-1, 40, 1, 1]          38,440\n",
            "MemoryEfficientSwish-347             [-1, 40, 1, 1]               0\n",
            "        Identity-348             [-1, 40, 1, 1]               0\n",
            "Conv2dStaticSamePadding-349            [-1, 960, 1, 1]          39,360\n",
            "        Identity-350          [-1, 960, 24, 24]               0\n",
            "Conv2dStaticSamePadding-351          [-1, 160, 24, 24]         153,600\n",
            "     BatchNorm2d-352          [-1, 160, 24, 24]             320\n",
            "     MBConvBlock-353          [-1, 160, 24, 24]               0\n",
            "        Identity-354          [-1, 160, 24, 24]               0\n",
            "Conv2dStaticSamePadding-355          [-1, 960, 24, 24]         153,600\n",
            "     BatchNorm2d-356          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-357          [-1, 960, 24, 24]               0\n",
            "       ZeroPad2d-358          [-1, 960, 28, 28]               0\n",
            "Conv2dStaticSamePadding-359          [-1, 960, 24, 24]          24,000\n",
            "     BatchNorm2d-360          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-361          [-1, 960, 24, 24]               0\n",
            "        Identity-362            [-1, 960, 1, 1]               0\n",
            "Conv2dStaticSamePadding-363             [-1, 40, 1, 1]          38,440\n",
            "MemoryEfficientSwish-364             [-1, 40, 1, 1]               0\n",
            "        Identity-365             [-1, 40, 1, 1]               0\n",
            "Conv2dStaticSamePadding-366            [-1, 960, 1, 1]          39,360\n",
            "        Identity-367          [-1, 960, 24, 24]               0\n",
            "Conv2dStaticSamePadding-368          [-1, 160, 24, 24]         153,600\n",
            "     BatchNorm2d-369          [-1, 160, 24, 24]             320\n",
            "     MBConvBlock-370          [-1, 160, 24, 24]               0\n",
            "        Identity-371          [-1, 160, 24, 24]               0\n",
            "Conv2dStaticSamePadding-372          [-1, 960, 24, 24]         153,600\n",
            "     BatchNorm2d-373          [-1, 960, 24, 24]           1,920\n",
            "MemoryEfficientSwish-374          [-1, 960, 24, 24]               0\n",
            "       ZeroPad2d-375          [-1, 960, 27, 27]               0\n",
            "Conv2dStaticSamePadding-376          [-1, 960, 12, 12]          24,000\n",
            "     BatchNorm2d-377          [-1, 960, 12, 12]           1,920\n",
            "MemoryEfficientSwish-378          [-1, 960, 12, 12]               0\n",
            "        Identity-379            [-1, 960, 1, 1]               0\n",
            "Conv2dStaticSamePadding-380             [-1, 40, 1, 1]          38,440\n",
            "MemoryEfficientSwish-381             [-1, 40, 1, 1]               0\n",
            "        Identity-382             [-1, 40, 1, 1]               0\n",
            "Conv2dStaticSamePadding-383            [-1, 960, 1, 1]          39,360\n",
            "        Identity-384          [-1, 960, 12, 12]               0\n",
            "Conv2dStaticSamePadding-385          [-1, 272, 12, 12]         261,120\n",
            "     BatchNorm2d-386          [-1, 272, 12, 12]             544\n",
            "     MBConvBlock-387          [-1, 272, 12, 12]               0\n",
            "        Identity-388          [-1, 272, 12, 12]               0\n",
            "Conv2dStaticSamePadding-389         [-1, 1632, 12, 12]         443,904\n",
            "     BatchNorm2d-390         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-391         [-1, 1632, 12, 12]               0\n",
            "       ZeroPad2d-392         [-1, 1632, 16, 16]               0\n",
            "Conv2dStaticSamePadding-393         [-1, 1632, 12, 12]          40,800\n",
            "     BatchNorm2d-394         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-395         [-1, 1632, 12, 12]               0\n",
            "        Identity-396           [-1, 1632, 1, 1]               0\n",
            "Conv2dStaticSamePadding-397             [-1, 68, 1, 1]         111,044\n",
            "MemoryEfficientSwish-398             [-1, 68, 1, 1]               0\n",
            "        Identity-399             [-1, 68, 1, 1]               0\n",
            "Conv2dStaticSamePadding-400           [-1, 1632, 1, 1]         112,608\n",
            "        Identity-401         [-1, 1632, 12, 12]               0\n",
            "Conv2dStaticSamePadding-402          [-1, 272, 12, 12]         443,904\n",
            "     BatchNorm2d-403          [-1, 272, 12, 12]             544\n",
            "     MBConvBlock-404          [-1, 272, 12, 12]               0\n",
            "        Identity-405          [-1, 272, 12, 12]               0\n",
            "Conv2dStaticSamePadding-406         [-1, 1632, 12, 12]         443,904\n",
            "     BatchNorm2d-407         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-408         [-1, 1632, 12, 12]               0\n",
            "       ZeroPad2d-409         [-1, 1632, 16, 16]               0\n",
            "Conv2dStaticSamePadding-410         [-1, 1632, 12, 12]          40,800\n",
            "     BatchNorm2d-411         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-412         [-1, 1632, 12, 12]               0\n",
            "        Identity-413           [-1, 1632, 1, 1]               0\n",
            "Conv2dStaticSamePadding-414             [-1, 68, 1, 1]         111,044\n",
            "MemoryEfficientSwish-415             [-1, 68, 1, 1]               0\n",
            "        Identity-416             [-1, 68, 1, 1]               0\n",
            "Conv2dStaticSamePadding-417           [-1, 1632, 1, 1]         112,608\n",
            "        Identity-418         [-1, 1632, 12, 12]               0\n",
            "Conv2dStaticSamePadding-419          [-1, 272, 12, 12]         443,904\n",
            "     BatchNorm2d-420          [-1, 272, 12, 12]             544\n",
            "     MBConvBlock-421          [-1, 272, 12, 12]               0\n",
            "        Identity-422          [-1, 272, 12, 12]               0\n",
            "Conv2dStaticSamePadding-423         [-1, 1632, 12, 12]         443,904\n",
            "     BatchNorm2d-424         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-425         [-1, 1632, 12, 12]               0\n",
            "       ZeroPad2d-426         [-1, 1632, 16, 16]               0\n",
            "Conv2dStaticSamePadding-427         [-1, 1632, 12, 12]          40,800\n",
            "     BatchNorm2d-428         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-429         [-1, 1632, 12, 12]               0\n",
            "        Identity-430           [-1, 1632, 1, 1]               0\n",
            "Conv2dStaticSamePadding-431             [-1, 68, 1, 1]         111,044\n",
            "MemoryEfficientSwish-432             [-1, 68, 1, 1]               0\n",
            "        Identity-433             [-1, 68, 1, 1]               0\n",
            "Conv2dStaticSamePadding-434           [-1, 1632, 1, 1]         112,608\n",
            "        Identity-435         [-1, 1632, 12, 12]               0\n",
            "Conv2dStaticSamePadding-436          [-1, 272, 12, 12]         443,904\n",
            "     BatchNorm2d-437          [-1, 272, 12, 12]             544\n",
            "     MBConvBlock-438          [-1, 272, 12, 12]               0\n",
            "        Identity-439          [-1, 272, 12, 12]               0\n",
            "Conv2dStaticSamePadding-440         [-1, 1632, 12, 12]         443,904\n",
            "     BatchNorm2d-441         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-442         [-1, 1632, 12, 12]               0\n",
            "       ZeroPad2d-443         [-1, 1632, 16, 16]               0\n",
            "Conv2dStaticSamePadding-444         [-1, 1632, 12, 12]          40,800\n",
            "     BatchNorm2d-445         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-446         [-1, 1632, 12, 12]               0\n",
            "        Identity-447           [-1, 1632, 1, 1]               0\n",
            "Conv2dStaticSamePadding-448             [-1, 68, 1, 1]         111,044\n",
            "MemoryEfficientSwish-449             [-1, 68, 1, 1]               0\n",
            "        Identity-450             [-1, 68, 1, 1]               0\n",
            "Conv2dStaticSamePadding-451           [-1, 1632, 1, 1]         112,608\n",
            "        Identity-452         [-1, 1632, 12, 12]               0\n",
            "Conv2dStaticSamePadding-453          [-1, 272, 12, 12]         443,904\n",
            "     BatchNorm2d-454          [-1, 272, 12, 12]             544\n",
            "     MBConvBlock-455          [-1, 272, 12, 12]               0\n",
            "        Identity-456          [-1, 272, 12, 12]               0\n",
            "Conv2dStaticSamePadding-457         [-1, 1632, 12, 12]         443,904\n",
            "     BatchNorm2d-458         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-459         [-1, 1632, 12, 12]               0\n",
            "       ZeroPad2d-460         [-1, 1632, 16, 16]               0\n",
            "Conv2dStaticSamePadding-461         [-1, 1632, 12, 12]          40,800\n",
            "     BatchNorm2d-462         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-463         [-1, 1632, 12, 12]               0\n",
            "        Identity-464           [-1, 1632, 1, 1]               0\n",
            "Conv2dStaticSamePadding-465             [-1, 68, 1, 1]         111,044\n",
            "MemoryEfficientSwish-466             [-1, 68, 1, 1]               0\n",
            "        Identity-467             [-1, 68, 1, 1]               0\n",
            "Conv2dStaticSamePadding-468           [-1, 1632, 1, 1]         112,608\n",
            "        Identity-469         [-1, 1632, 12, 12]               0\n",
            "Conv2dStaticSamePadding-470          [-1, 272, 12, 12]         443,904\n",
            "     BatchNorm2d-471          [-1, 272, 12, 12]             544\n",
            "     MBConvBlock-472          [-1, 272, 12, 12]               0\n",
            "        Identity-473          [-1, 272, 12, 12]               0\n",
            "Conv2dStaticSamePadding-474         [-1, 1632, 12, 12]         443,904\n",
            "     BatchNorm2d-475         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-476         [-1, 1632, 12, 12]               0\n",
            "       ZeroPad2d-477         [-1, 1632, 16, 16]               0\n",
            "Conv2dStaticSamePadding-478         [-1, 1632, 12, 12]          40,800\n",
            "     BatchNorm2d-479         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-480         [-1, 1632, 12, 12]               0\n",
            "        Identity-481           [-1, 1632, 1, 1]               0\n",
            "Conv2dStaticSamePadding-482             [-1, 68, 1, 1]         111,044\n",
            "MemoryEfficientSwish-483             [-1, 68, 1, 1]               0\n",
            "        Identity-484             [-1, 68, 1, 1]               0\n",
            "Conv2dStaticSamePadding-485           [-1, 1632, 1, 1]         112,608\n",
            "        Identity-486         [-1, 1632, 12, 12]               0\n",
            "Conv2dStaticSamePadding-487          [-1, 272, 12, 12]         443,904\n",
            "     BatchNorm2d-488          [-1, 272, 12, 12]             544\n",
            "     MBConvBlock-489          [-1, 272, 12, 12]               0\n",
            "        Identity-490          [-1, 272, 12, 12]               0\n",
            "Conv2dStaticSamePadding-491         [-1, 1632, 12, 12]         443,904\n",
            "     BatchNorm2d-492         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-493         [-1, 1632, 12, 12]               0\n",
            "       ZeroPad2d-494         [-1, 1632, 16, 16]               0\n",
            "Conv2dStaticSamePadding-495         [-1, 1632, 12, 12]          40,800\n",
            "     BatchNorm2d-496         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-497         [-1, 1632, 12, 12]               0\n",
            "        Identity-498           [-1, 1632, 1, 1]               0\n",
            "Conv2dStaticSamePadding-499             [-1, 68, 1, 1]         111,044\n",
            "MemoryEfficientSwish-500             [-1, 68, 1, 1]               0\n",
            "        Identity-501             [-1, 68, 1, 1]               0\n",
            "Conv2dStaticSamePadding-502           [-1, 1632, 1, 1]         112,608\n",
            "        Identity-503         [-1, 1632, 12, 12]               0\n",
            "Conv2dStaticSamePadding-504          [-1, 272, 12, 12]         443,904\n",
            "     BatchNorm2d-505          [-1, 272, 12, 12]             544\n",
            "     MBConvBlock-506          [-1, 272, 12, 12]               0\n",
            "        Identity-507          [-1, 272, 12, 12]               0\n",
            "Conv2dStaticSamePadding-508         [-1, 1632, 12, 12]         443,904\n",
            "     BatchNorm2d-509         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-510         [-1, 1632, 12, 12]               0\n",
            "       ZeroPad2d-511         [-1, 1632, 14, 14]               0\n",
            "Conv2dStaticSamePadding-512         [-1, 1632, 12, 12]          14,688\n",
            "     BatchNorm2d-513         [-1, 1632, 12, 12]           3,264\n",
            "MemoryEfficientSwish-514         [-1, 1632, 12, 12]               0\n",
            "        Identity-515           [-1, 1632, 1, 1]               0\n",
            "Conv2dStaticSamePadding-516             [-1, 68, 1, 1]         111,044\n",
            "MemoryEfficientSwish-517             [-1, 68, 1, 1]               0\n",
            "        Identity-518             [-1, 68, 1, 1]               0\n",
            "Conv2dStaticSamePadding-519           [-1, 1632, 1, 1]         112,608\n",
            "        Identity-520         [-1, 1632, 12, 12]               0\n",
            "Conv2dStaticSamePadding-521          [-1, 448, 12, 12]         731,136\n",
            "     BatchNorm2d-522          [-1, 448, 12, 12]             896\n",
            "     MBConvBlock-523          [-1, 448, 12, 12]               0\n",
            "        Identity-524          [-1, 448, 12, 12]               0\n",
            "Conv2dStaticSamePadding-525         [-1, 2688, 12, 12]       1,204,224\n",
            "     BatchNorm2d-526         [-1, 2688, 12, 12]           5,376\n",
            "MemoryEfficientSwish-527         [-1, 2688, 12, 12]               0\n",
            "       ZeroPad2d-528         [-1, 2688, 14, 14]               0\n",
            "Conv2dStaticSamePadding-529         [-1, 2688, 12, 12]          24,192\n",
            "     BatchNorm2d-530         [-1, 2688, 12, 12]           5,376\n",
            "MemoryEfficientSwish-531         [-1, 2688, 12, 12]               0\n",
            "        Identity-532           [-1, 2688, 1, 1]               0\n",
            "Conv2dStaticSamePadding-533            [-1, 112, 1, 1]         301,168\n",
            "MemoryEfficientSwish-534            [-1, 112, 1, 1]               0\n",
            "        Identity-535            [-1, 112, 1, 1]               0\n",
            "Conv2dStaticSamePadding-536           [-1, 2688, 1, 1]         303,744\n",
            "        Identity-537         [-1, 2688, 12, 12]               0\n",
            "Conv2dStaticSamePadding-538          [-1, 448, 12, 12]       1,204,224\n",
            "     BatchNorm2d-539          [-1, 448, 12, 12]             896\n",
            "     MBConvBlock-540          [-1, 448, 12, 12]               0\n",
            "        Identity-541          [-1, 448, 12, 12]               0\n",
            "Conv2dStaticSamePadding-542         [-1, 1792, 12, 12]         802,816\n",
            "     BatchNorm2d-543         [-1, 1792, 12, 12]           3,584\n",
            "MemoryEfficientSwish-544         [-1, 1792, 12, 12]               0\n",
            "AdaptiveAvgPool2d-545           [-1, 1792, 1, 1]               0\n",
            "         Dropout-546                 [-1, 1792]               0\n",
            "          Linear-547                    [-1, 3]           5,379\n",
            "================================================================\n",
            "Total params: 17,553,995\n",
            "Trainable params: 17,553,995\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.65\n",
            "Forward/backward pass size (MB): 1542.03\n",
            "Params size (MB): 66.96\n",
            "Estimated Total Size (MB): 1610.65\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tisum(\\\n",
        "            model,\\\n",
        "            input_size = (batchsize,channels,dimension,dimension)\\\n",
        "            )\\\n",
        "      )"
      ],
      "metadata": {
        "id": "E-k_-YPF6Ec0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e538b3ae-2b39-46cf-b2e5-325a4cae4bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "Layer (type:depth-idx)                             Output Shape              Param #\n",
            "====================================================================================================\n",
            "EfficientNet                                       [20, 3]                   --\n",
            "├─Conv2dStaticSamePadding: 1-1                     [20, 48, 190, 190]        1,296\n",
            "│    └─ZeroPad2d: 2-1                              [20, 3, 381, 381]         --\n",
            "├─BatchNorm2d: 1-2                                 [20, 48, 190, 190]        96\n",
            "├─MemoryEfficientSwish: 1-3                        [20, 48, 190, 190]        --\n",
            "├─ModuleList: 1-4                                  --                        --\n",
            "│    └─MBConvBlock: 2-2                            [20, 24, 190, 190]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-1           [20, 48, 190, 190]        432\n",
            "│    │    └─BatchNorm2d: 3-2                       [20, 48, 190, 190]        96\n",
            "│    │    └─MemoryEfficientSwish: 3-3              [20, 48, 190, 190]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-4           [20, 12, 1, 1]            588\n",
            "│    │    └─MemoryEfficientSwish: 3-5              [20, 12, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-6           [20, 48, 1, 1]            624\n",
            "│    │    └─Conv2dStaticSamePadding: 3-7           [20, 24, 190, 190]        1,152\n",
            "│    │    └─BatchNorm2d: 3-8                       [20, 24, 190, 190]        48\n",
            "│    └─MBConvBlock: 2-3                            [20, 24, 190, 190]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-9           [20, 24, 190, 190]        216\n",
            "│    │    └─BatchNorm2d: 3-10                      [20, 24, 190, 190]        48\n",
            "│    │    └─MemoryEfficientSwish: 3-11             [20, 24, 190, 190]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-12          [20, 6, 1, 1]             150\n",
            "│    │    └─MemoryEfficientSwish: 3-13             [20, 6, 1, 1]             --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-14          [20, 24, 1, 1]            168\n",
            "│    │    └─Conv2dStaticSamePadding: 3-15          [20, 24, 190, 190]        576\n",
            "│    │    └─BatchNorm2d: 3-16                      [20, 24, 190, 190]        48\n",
            "│    └─MBConvBlock: 2-4                            [20, 32, 95, 95]          --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-17          [20, 144, 190, 190]       3,456\n",
            "│    │    └─BatchNorm2d: 3-18                      [20, 144, 190, 190]       288\n",
            "│    │    └─MemoryEfficientSwish: 3-19             [20, 144, 190, 190]       --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-20          [20, 144, 95, 95]         1,296\n",
            "│    │    └─BatchNorm2d: 3-21                      [20, 144, 95, 95]         288\n",
            "│    │    └─MemoryEfficientSwish: 3-22             [20, 144, 95, 95]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-23          [20, 6, 1, 1]             870\n",
            "│    │    └─MemoryEfficientSwish: 3-24             [20, 6, 1, 1]             --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-25          [20, 144, 1, 1]           1,008\n",
            "│    │    └─Conv2dStaticSamePadding: 3-26          [20, 32, 95, 95]          4,608\n",
            "│    │    └─BatchNorm2d: 3-27                      [20, 32, 95, 95]          64\n",
            "│    └─MBConvBlock: 2-5                            [20, 32, 95, 95]          --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-28          [20, 192, 95, 95]         6,144\n",
            "│    │    └─BatchNorm2d: 3-29                      [20, 192, 95, 95]         384\n",
            "│    │    └─MemoryEfficientSwish: 3-30             [20, 192, 95, 95]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-31          [20, 192, 95, 95]         1,728\n",
            "│    │    └─BatchNorm2d: 3-32                      [20, 192, 95, 95]         384\n",
            "│    │    └─MemoryEfficientSwish: 3-33             [20, 192, 95, 95]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-34          [20, 8, 1, 1]             1,544\n",
            "│    │    └─MemoryEfficientSwish: 3-35             [20, 8, 1, 1]             --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-36          [20, 192, 1, 1]           1,728\n",
            "│    │    └─Conv2dStaticSamePadding: 3-37          [20, 32, 95, 95]          6,144\n",
            "│    │    └─BatchNorm2d: 3-38                      [20, 32, 95, 95]          64\n",
            "│    └─MBConvBlock: 2-6                            [20, 32, 95, 95]          --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-39          [20, 192, 95, 95]         6,144\n",
            "│    │    └─BatchNorm2d: 3-40                      [20, 192, 95, 95]         384\n",
            "│    │    └─MemoryEfficientSwish: 3-41             [20, 192, 95, 95]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-42          [20, 192, 95, 95]         1,728\n",
            "│    │    └─BatchNorm2d: 3-43                      [20, 192, 95, 95]         384\n",
            "│    │    └─MemoryEfficientSwish: 3-44             [20, 192, 95, 95]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-45          [20, 8, 1, 1]             1,544\n",
            "│    │    └─MemoryEfficientSwish: 3-46             [20, 8, 1, 1]             --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-47          [20, 192, 1, 1]           1,728\n",
            "│    │    └─Conv2dStaticSamePadding: 3-48          [20, 32, 95, 95]          6,144\n",
            "│    │    └─BatchNorm2d: 3-49                      [20, 32, 95, 95]          64\n",
            "│    └─MBConvBlock: 2-7                            [20, 32, 95, 95]          --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-50          [20, 192, 95, 95]         6,144\n",
            "│    │    └─BatchNorm2d: 3-51                      [20, 192, 95, 95]         384\n",
            "│    │    └─MemoryEfficientSwish: 3-52             [20, 192, 95, 95]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-53          [20, 192, 95, 95]         1,728\n",
            "│    │    └─BatchNorm2d: 3-54                      [20, 192, 95, 95]         384\n",
            "│    │    └─MemoryEfficientSwish: 3-55             [20, 192, 95, 95]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-56          [20, 8, 1, 1]             1,544\n",
            "│    │    └─MemoryEfficientSwish: 3-57             [20, 8, 1, 1]             --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-58          [20, 192, 1, 1]           1,728\n",
            "│    │    └─Conv2dStaticSamePadding: 3-59          [20, 32, 95, 95]          6,144\n",
            "│    │    └─BatchNorm2d: 3-60                      [20, 32, 95, 95]          64\n",
            "│    └─MBConvBlock: 2-8                            [20, 56, 48, 48]          --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-61          [20, 192, 95, 95]         6,144\n",
            "│    │    └─BatchNorm2d: 3-62                      [20, 192, 95, 95]         384\n",
            "│    │    └─MemoryEfficientSwish: 3-63             [20, 192, 95, 95]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-64          [20, 192, 48, 48]         4,800\n",
            "│    │    └─BatchNorm2d: 3-65                      [20, 192, 48, 48]         384\n",
            "│    │    └─MemoryEfficientSwish: 3-66             [20, 192, 48, 48]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-67          [20, 8, 1, 1]             1,544\n",
            "│    │    └─MemoryEfficientSwish: 3-68             [20, 8, 1, 1]             --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-69          [20, 192, 1, 1]           1,728\n",
            "│    │    └─Conv2dStaticSamePadding: 3-70          [20, 56, 48, 48]          10,752\n",
            "│    │    └─BatchNorm2d: 3-71                      [20, 56, 48, 48]          112\n",
            "│    └─MBConvBlock: 2-9                            [20, 56, 48, 48]          --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-72          [20, 336, 48, 48]         18,816\n",
            "│    │    └─BatchNorm2d: 3-73                      [20, 336, 48, 48]         672\n",
            "│    │    └─MemoryEfficientSwish: 3-74             [20, 336, 48, 48]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-75          [20, 336, 48, 48]         8,400\n",
            "│    │    └─BatchNorm2d: 3-76                      [20, 336, 48, 48]         672\n",
            "│    │    └─MemoryEfficientSwish: 3-77             [20, 336, 48, 48]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-78          [20, 14, 1, 1]            4,718\n",
            "│    │    └─MemoryEfficientSwish: 3-79             [20, 14, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-80          [20, 336, 1, 1]           5,040\n",
            "│    │    └─Conv2dStaticSamePadding: 3-81          [20, 56, 48, 48]          18,816\n",
            "│    │    └─BatchNorm2d: 3-82                      [20, 56, 48, 48]          112\n",
            "│    └─MBConvBlock: 2-10                           [20, 56, 48, 48]          --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-83          [20, 336, 48, 48]         18,816\n",
            "│    │    └─BatchNorm2d: 3-84                      [20, 336, 48, 48]         672\n",
            "│    │    └─MemoryEfficientSwish: 3-85             [20, 336, 48, 48]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-86          [20, 336, 48, 48]         8,400\n",
            "│    │    └─BatchNorm2d: 3-87                      [20, 336, 48, 48]         672\n",
            "│    │    └─MemoryEfficientSwish: 3-88             [20, 336, 48, 48]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-89          [20, 14, 1, 1]            4,718\n",
            "│    │    └─MemoryEfficientSwish: 3-90             [20, 14, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-91          [20, 336, 1, 1]           5,040\n",
            "│    │    └─Conv2dStaticSamePadding: 3-92          [20, 56, 48, 48]          18,816\n",
            "│    │    └─BatchNorm2d: 3-93                      [20, 56, 48, 48]          112\n",
            "│    └─MBConvBlock: 2-11                           [20, 56, 48, 48]          --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-94          [20, 336, 48, 48]         18,816\n",
            "│    │    └─BatchNorm2d: 3-95                      [20, 336, 48, 48]         672\n",
            "│    │    └─MemoryEfficientSwish: 3-96             [20, 336, 48, 48]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-97          [20, 336, 48, 48]         8,400\n",
            "│    │    └─BatchNorm2d: 3-98                      [20, 336, 48, 48]         672\n",
            "│    │    └─MemoryEfficientSwish: 3-99             [20, 336, 48, 48]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-100         [20, 14, 1, 1]            4,718\n",
            "│    │    └─MemoryEfficientSwish: 3-101            [20, 14, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-102         [20, 336, 1, 1]           5,040\n",
            "│    │    └─Conv2dStaticSamePadding: 3-103         [20, 56, 48, 48]          18,816\n",
            "│    │    └─BatchNorm2d: 3-104                     [20, 56, 48, 48]          112\n",
            "│    └─MBConvBlock: 2-12                           [20, 112, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-105         [20, 336, 48, 48]         18,816\n",
            "│    │    └─BatchNorm2d: 3-106                     [20, 336, 48, 48]         672\n",
            "│    │    └─MemoryEfficientSwish: 3-107            [20, 336, 48, 48]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-108         [20, 336, 24, 24]         3,024\n",
            "│    │    └─BatchNorm2d: 3-109                     [20, 336, 24, 24]         672\n",
            "│    │    └─MemoryEfficientSwish: 3-110            [20, 336, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-111         [20, 14, 1, 1]            4,718\n",
            "│    │    └─MemoryEfficientSwish: 3-112            [20, 14, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-113         [20, 336, 1, 1]           5,040\n",
            "│    │    └─Conv2dStaticSamePadding: 3-114         [20, 112, 24, 24]         37,632\n",
            "│    │    └─BatchNorm2d: 3-115                     [20, 112, 24, 24]         224\n",
            "│    └─MBConvBlock: 2-13                           [20, 112, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-116         [20, 672, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-117                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-118            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-119         [20, 672, 24, 24]         6,048\n",
            "│    │    └─BatchNorm2d: 3-120                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-121            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-122         [20, 28, 1, 1]            18,844\n",
            "│    │    └─MemoryEfficientSwish: 3-123            [20, 28, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-124         [20, 672, 1, 1]           19,488\n",
            "│    │    └─Conv2dStaticSamePadding: 3-125         [20, 112, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-126                     [20, 112, 24, 24]         224\n",
            "│    └─MBConvBlock: 2-14                           [20, 112, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-127         [20, 672, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-128                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-129            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-130         [20, 672, 24, 24]         6,048\n",
            "│    │    └─BatchNorm2d: 3-131                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-132            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-133         [20, 28, 1, 1]            18,844\n",
            "│    │    └─MemoryEfficientSwish: 3-134            [20, 28, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-135         [20, 672, 1, 1]           19,488\n",
            "│    │    └─Conv2dStaticSamePadding: 3-136         [20, 112, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-137                     [20, 112, 24, 24]         224\n",
            "│    └─MBConvBlock: 2-15                           [20, 112, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-138         [20, 672, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-139                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-140            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-141         [20, 672, 24, 24]         6,048\n",
            "│    │    └─BatchNorm2d: 3-142                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-143            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-144         [20, 28, 1, 1]            18,844\n",
            "│    │    └─MemoryEfficientSwish: 3-145            [20, 28, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-146         [20, 672, 1, 1]           19,488\n",
            "│    │    └─Conv2dStaticSamePadding: 3-147         [20, 112, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-148                     [20, 112, 24, 24]         224\n",
            "│    └─MBConvBlock: 2-16                           [20, 112, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-149         [20, 672, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-150                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-151            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-152         [20, 672, 24, 24]         6,048\n",
            "│    │    └─BatchNorm2d: 3-153                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-154            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-155         [20, 28, 1, 1]            18,844\n",
            "│    │    └─MemoryEfficientSwish: 3-156            [20, 28, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-157         [20, 672, 1, 1]           19,488\n",
            "│    │    └─Conv2dStaticSamePadding: 3-158         [20, 112, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-159                     [20, 112, 24, 24]         224\n",
            "│    └─MBConvBlock: 2-17                           [20, 112, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-160         [20, 672, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-161                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-162            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-163         [20, 672, 24, 24]         6,048\n",
            "│    │    └─BatchNorm2d: 3-164                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-165            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-166         [20, 28, 1, 1]            18,844\n",
            "│    │    └─MemoryEfficientSwish: 3-167            [20, 28, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-168         [20, 672, 1, 1]           19,488\n",
            "│    │    └─Conv2dStaticSamePadding: 3-169         [20, 112, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-170                     [20, 112, 24, 24]         224\n",
            "│    └─MBConvBlock: 2-18                           [20, 160, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-171         [20, 672, 24, 24]         75,264\n",
            "│    │    └─BatchNorm2d: 3-172                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-173            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-174         [20, 672, 24, 24]         16,800\n",
            "│    │    └─BatchNorm2d: 3-175                     [20, 672, 24, 24]         1,344\n",
            "│    │    └─MemoryEfficientSwish: 3-176            [20, 672, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-177         [20, 28, 1, 1]            18,844\n",
            "│    │    └─MemoryEfficientSwish: 3-178            [20, 28, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-179         [20, 672, 1, 1]           19,488\n",
            "│    │    └─Conv2dStaticSamePadding: 3-180         [20, 160, 24, 24]         107,520\n",
            "│    │    └─BatchNorm2d: 3-181                     [20, 160, 24, 24]         320\n",
            "│    └─MBConvBlock: 2-19                           [20, 160, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-182         [20, 960, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-183                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-184            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-185         [20, 960, 24, 24]         24,000\n",
            "│    │    └─BatchNorm2d: 3-186                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-187            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-188         [20, 40, 1, 1]            38,440\n",
            "│    │    └─MemoryEfficientSwish: 3-189            [20, 40, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-190         [20, 960, 1, 1]           39,360\n",
            "│    │    └─Conv2dStaticSamePadding: 3-191         [20, 160, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-192                     [20, 160, 24, 24]         320\n",
            "│    └─MBConvBlock: 2-20                           [20, 160, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-193         [20, 960, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-194                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-195            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-196         [20, 960, 24, 24]         24,000\n",
            "│    │    └─BatchNorm2d: 3-197                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-198            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-199         [20, 40, 1, 1]            38,440\n",
            "│    │    └─MemoryEfficientSwish: 3-200            [20, 40, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-201         [20, 960, 1, 1]           39,360\n",
            "│    │    └─Conv2dStaticSamePadding: 3-202         [20, 160, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-203                     [20, 160, 24, 24]         320\n",
            "│    └─MBConvBlock: 2-21                           [20, 160, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-204         [20, 960, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-205                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-206            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-207         [20, 960, 24, 24]         24,000\n",
            "│    │    └─BatchNorm2d: 3-208                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-209            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-210         [20, 40, 1, 1]            38,440\n",
            "│    │    └─MemoryEfficientSwish: 3-211            [20, 40, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-212         [20, 960, 1, 1]           39,360\n",
            "│    │    └─Conv2dStaticSamePadding: 3-213         [20, 160, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-214                     [20, 160, 24, 24]         320\n",
            "│    └─MBConvBlock: 2-22                           [20, 160, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-215         [20, 960, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-216                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-217            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-218         [20, 960, 24, 24]         24,000\n",
            "│    │    └─BatchNorm2d: 3-219                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-220            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-221         [20, 40, 1, 1]            38,440\n",
            "│    │    └─MemoryEfficientSwish: 3-222            [20, 40, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-223         [20, 960, 1, 1]           39,360\n",
            "│    │    └─Conv2dStaticSamePadding: 3-224         [20, 160, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-225                     [20, 160, 24, 24]         320\n",
            "│    └─MBConvBlock: 2-23                           [20, 160, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-226         [20, 960, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-227                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-228            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-229         [20, 960, 24, 24]         24,000\n",
            "│    │    └─BatchNorm2d: 3-230                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-231            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-232         [20, 40, 1, 1]            38,440\n",
            "│    │    └─MemoryEfficientSwish: 3-233            [20, 40, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-234         [20, 960, 1, 1]           39,360\n",
            "│    │    └─Conv2dStaticSamePadding: 3-235         [20, 160, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-236                     [20, 160, 24, 24]         320\n",
            "│    └─MBConvBlock: 2-24                           [20, 272, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-237         [20, 960, 24, 24]         153,600\n",
            "│    │    └─BatchNorm2d: 3-238                     [20, 960, 24, 24]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-239            [20, 960, 24, 24]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-240         [20, 960, 12, 12]         24,000\n",
            "│    │    └─BatchNorm2d: 3-241                     [20, 960, 12, 12]         1,920\n",
            "│    │    └─MemoryEfficientSwish: 3-242            [20, 960, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-243         [20, 40, 1, 1]            38,440\n",
            "│    │    └─MemoryEfficientSwish: 3-244            [20, 40, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-245         [20, 960, 1, 1]           39,360\n",
            "│    │    └─Conv2dStaticSamePadding: 3-246         [20, 272, 12, 12]         261,120\n",
            "│    │    └─BatchNorm2d: 3-247                     [20, 272, 12, 12]         544\n",
            "│    └─MBConvBlock: 2-25                           [20, 272, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-248         [20, 1632, 12, 12]        443,904\n",
            "│    │    └─BatchNorm2d: 3-249                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-250            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-251         [20, 1632, 12, 12]        40,800\n",
            "│    │    └─BatchNorm2d: 3-252                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-253            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-254         [20, 68, 1, 1]            111,044\n",
            "│    │    └─MemoryEfficientSwish: 3-255            [20, 68, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-256         [20, 1632, 1, 1]          112,608\n",
            "│    │    └─Conv2dStaticSamePadding: 3-257         [20, 272, 12, 12]         443,904\n",
            "│    │    └─BatchNorm2d: 3-258                     [20, 272, 12, 12]         544\n",
            "│    └─MBConvBlock: 2-26                           [20, 272, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-259         [20, 1632, 12, 12]        443,904\n",
            "│    │    └─BatchNorm2d: 3-260                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-261            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-262         [20, 1632, 12, 12]        40,800\n",
            "│    │    └─BatchNorm2d: 3-263                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-264            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-265         [20, 68, 1, 1]            111,044\n",
            "│    │    └─MemoryEfficientSwish: 3-266            [20, 68, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-267         [20, 1632, 1, 1]          112,608\n",
            "│    │    └─Conv2dStaticSamePadding: 3-268         [20, 272, 12, 12]         443,904\n",
            "│    │    └─BatchNorm2d: 3-269                     [20, 272, 12, 12]         544\n",
            "│    └─MBConvBlock: 2-27                           [20, 272, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-270         [20, 1632, 12, 12]        443,904\n",
            "│    │    └─BatchNorm2d: 3-271                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-272            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-273         [20, 1632, 12, 12]        40,800\n",
            "│    │    └─BatchNorm2d: 3-274                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-275            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-276         [20, 68, 1, 1]            111,044\n",
            "│    │    └─MemoryEfficientSwish: 3-277            [20, 68, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-278         [20, 1632, 1, 1]          112,608\n",
            "│    │    └─Conv2dStaticSamePadding: 3-279         [20, 272, 12, 12]         443,904\n",
            "│    │    └─BatchNorm2d: 3-280                     [20, 272, 12, 12]         544\n",
            "│    └─MBConvBlock: 2-28                           [20, 272, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-281         [20, 1632, 12, 12]        443,904\n",
            "│    │    └─BatchNorm2d: 3-282                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-283            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-284         [20, 1632, 12, 12]        40,800\n",
            "│    │    └─BatchNorm2d: 3-285                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-286            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-287         [20, 68, 1, 1]            111,044\n",
            "│    │    └─MemoryEfficientSwish: 3-288            [20, 68, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-289         [20, 1632, 1, 1]          112,608\n",
            "│    │    └─Conv2dStaticSamePadding: 3-290         [20, 272, 12, 12]         443,904\n",
            "│    │    └─BatchNorm2d: 3-291                     [20, 272, 12, 12]         544\n",
            "│    └─MBConvBlock: 2-29                           [20, 272, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-292         [20, 1632, 12, 12]        443,904\n",
            "│    │    └─BatchNorm2d: 3-293                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-294            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-295         [20, 1632, 12, 12]        40,800\n",
            "│    │    └─BatchNorm2d: 3-296                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-297            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-298         [20, 68, 1, 1]            111,044\n",
            "│    │    └─MemoryEfficientSwish: 3-299            [20, 68, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-300         [20, 1632, 1, 1]          112,608\n",
            "│    │    └─Conv2dStaticSamePadding: 3-301         [20, 272, 12, 12]         443,904\n",
            "│    │    └─BatchNorm2d: 3-302                     [20, 272, 12, 12]         544\n",
            "│    └─MBConvBlock: 2-30                           [20, 272, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-303         [20, 1632, 12, 12]        443,904\n",
            "│    │    └─BatchNorm2d: 3-304                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-305            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-306         [20, 1632, 12, 12]        40,800\n",
            "│    │    └─BatchNorm2d: 3-307                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-308            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-309         [20, 68, 1, 1]            111,044\n",
            "│    │    └─MemoryEfficientSwish: 3-310            [20, 68, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-311         [20, 1632, 1, 1]          112,608\n",
            "│    │    └─Conv2dStaticSamePadding: 3-312         [20, 272, 12, 12]         443,904\n",
            "│    │    └─BatchNorm2d: 3-313                     [20, 272, 12, 12]         544\n",
            "│    └─MBConvBlock: 2-31                           [20, 272, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-314         [20, 1632, 12, 12]        443,904\n",
            "│    │    └─BatchNorm2d: 3-315                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-316            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-317         [20, 1632, 12, 12]        40,800\n",
            "│    │    └─BatchNorm2d: 3-318                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-319            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-320         [20, 68, 1, 1]            111,044\n",
            "│    │    └─MemoryEfficientSwish: 3-321            [20, 68, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-322         [20, 1632, 1, 1]          112,608\n",
            "│    │    └─Conv2dStaticSamePadding: 3-323         [20, 272, 12, 12]         443,904\n",
            "│    │    └─BatchNorm2d: 3-324                     [20, 272, 12, 12]         544\n",
            "│    └─MBConvBlock: 2-32                           [20, 448, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-325         [20, 1632, 12, 12]        443,904\n",
            "│    │    └─BatchNorm2d: 3-326                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-327            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-328         [20, 1632, 12, 12]        14,688\n",
            "│    │    └─BatchNorm2d: 3-329                     [20, 1632, 12, 12]        3,264\n",
            "│    │    └─MemoryEfficientSwish: 3-330            [20, 1632, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-331         [20, 68, 1, 1]            111,044\n",
            "│    │    └─MemoryEfficientSwish: 3-332            [20, 68, 1, 1]            --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-333         [20, 1632, 1, 1]          112,608\n",
            "│    │    └─Conv2dStaticSamePadding: 3-334         [20, 448, 12, 12]         731,136\n",
            "│    │    └─BatchNorm2d: 3-335                     [20, 448, 12, 12]         896\n",
            "│    └─MBConvBlock: 2-33                           [20, 448, 12, 12]         --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-336         [20, 2688, 12, 12]        1,204,224\n",
            "│    │    └─BatchNorm2d: 3-337                     [20, 2688, 12, 12]        5,376\n",
            "│    │    └─MemoryEfficientSwish: 3-338            [20, 2688, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-339         [20, 2688, 12, 12]        24,192\n",
            "│    │    └─BatchNorm2d: 3-340                     [20, 2688, 12, 12]        5,376\n",
            "│    │    └─MemoryEfficientSwish: 3-341            [20, 2688, 12, 12]        --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-342         [20, 112, 1, 1]           301,168\n",
            "│    │    └─MemoryEfficientSwish: 3-343            [20, 112, 1, 1]           --\n",
            "│    │    └─Conv2dStaticSamePadding: 3-344         [20, 2688, 1, 1]          303,744\n",
            "│    │    └─Conv2dStaticSamePadding: 3-345         [20, 448, 12, 12]         1,204,224\n",
            "│    │    └─BatchNorm2d: 3-346                     [20, 448, 12, 12]         896\n",
            "├─Conv2dStaticSamePadding: 1-5                     [20, 1792, 12, 12]        802,816\n",
            "│    └─Identity: 2-34                              [20, 448, 12, 12]         --\n",
            "├─BatchNorm2d: 1-6                                 [20, 1792, 12, 12]        3,584\n",
            "├─MemoryEfficientSwish: 1-7                        [20, 1792, 12, 12]        --\n",
            "├─AdaptiveAvgPool2d: 1-8                           [20, 1792, 1, 1]          --\n",
            "├─Dropout: 1-9                                     [20, 1792]                --\n",
            "├─Linear: 1-10                                     [20, 3]                   5,379\n",
            "====================================================================================================\n",
            "Total params: 17,553,995\n",
            "Trainable params: 17,553,995\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (M): 2.61\n",
            "====================================================================================================\n",
            "Input size (MB): 34.66\n",
            "Forward/backward pass size (MB): 7913.44\n",
            "Params size (MB): 0.52\n",
            "Estimated Total Size (MB): 7948.62\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=1e-4"
      ],
      "metadata": {
        "id": "deZaBsqiLfdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# lr_decay=0.99\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "nGnCpmab6EaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_accuracy=[]\n",
        "history_loss=[]\n",
        "epochs = 11"
      ],
      "metadata": {
        "id": "zFD8Yci36EXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a class list\n",
        "\n",
        "eye = torch.eye(3).to(device)\n",
        "classes=[0,1,2]"
      ],
      "metadata": {
        "id": "c_jig7wnMSXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "    running_loss = 0.0\n",
        "    correct=0\n",
        "    total=0\n",
        "    class_correct = list(0. for _ in classes)\n",
        "    class_total = list(0. for _ in classes)\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        t0 = time()\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        labels = eye[labels]\n",
        "        optimizer.zero_grad()\n",
        "        #torch.cuda.empty_cache()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        _, labels = torch.max(labels, 1)\n",
        "        c = (predicted == labels.data).squeeze()\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        accuracy = float(correct) / float(total)\n",
        "        \n",
        "        history_accuracy.append(accuracy)\n",
        "        history_loss.append(loss)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        for j in range(labels.size(0)):\n",
        "            label = labels[j]\n",
        "            class_correct[label] += c[j].item()\n",
        "            class_total[label] += 1\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        print( \"Epoch : \",epoch+1,\" Batch : \", i+1,\" Loss :  \",running_loss/(i+1),\" Accuracy : \",accuracy,\"Time \",round(time()-t0, 2),\"s\" )\n",
        "    for k in range(len(classes)):\n",
        "        if(class_total[k]!=0):\n",
        "            print('Accuracy of %5s : %2d %%' % (classes[k], 100 * class_correct[k] / class_total[k]))\n",
        "        \n",
        "    print('[%d epoch] Accuracy of the network on the Training images: %d %%' % (epoch+1, 100 * correct / total))\n",
        "    \n",
        "    if epoch%3==0 or epoch==0:\n",
        "        file=f\"EN4-epoch {epoch+1} model.pth\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, file)\n",
        "        \n",
        "file=f\"EN4-epoch {epoch+1} model.pth\"\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, file)"
      ],
      "metadata": {
        "id": "df7RJ3u76EUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4edc0899-66ab-40c8-bded-7a445df30da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :  1  Batch :  1  Loss :   1.115805983543396  Accuracy :  0.35 Time  1.84 s\n",
            "Epoch :  1  Batch :  2  Loss :   1.115351378917694  Accuracy :  0.325 Time  1.04 s\n",
            "Epoch :  1  Batch :  3  Loss :   1.090968648592631  Accuracy :  0.4166666666666667 Time  1.03 s\n",
            "Epoch :  1  Batch :  4  Loss :   1.076417326927185  Accuracy :  0.45 Time  1.07 s\n",
            "Epoch :  1  Batch :  5  Loss :   1.0653160572052003  Accuracy :  0.46 Time  1.03 s\n",
            "Epoch :  1  Batch :  6  Loss :   1.0623130400975545  Accuracy :  0.4583333333333333 Time  1.06 s\n",
            "Epoch :  1  Batch :  7  Loss :   1.0481503009796143  Accuracy :  0.4785714285714286 Time  1.06 s\n",
            "Epoch :  1  Batch :  8  Loss :   1.0339316800236702  Accuracy :  0.51875 Time  1.07 s\n",
            "Epoch :  1  Batch :  9  Loss :   1.0220649242401123  Accuracy :  0.5388888888888889 Time  1.06 s\n",
            "Epoch :  1  Batch :  10  Loss :   1.0070029199123383  Accuracy :  0.57 Time  1.06 s\n",
            "Epoch :  1  Batch :  11  Loss :   1.0000828883864663  Accuracy :  0.5772727272727273 Time  1.04 s\n",
            "Epoch :  1  Batch :  12  Loss :   0.9893427987893423  Accuracy :  0.5958333333333333 Time  1.07 s\n",
            "Epoch :  1  Batch :  13  Loss :   0.9863339249904339  Accuracy :  0.6 Time  1.05 s\n",
            "Epoch :  1  Batch :  14  Loss :   0.9731794553143638  Accuracy :  0.6142857142857143 Time  1.05 s\n",
            "Epoch :  1  Batch :  15  Loss :   0.9512704094250997  Accuracy :  0.6366666666666667 Time  1.07 s\n",
            "Epoch :  1  Batch :  16  Loss :   0.9375715851783752  Accuracy :  0.65625 Time  1.06 s\n",
            "Epoch :  1  Batch :  17  Loss :   0.9277711124981151  Accuracy :  0.6647058823529411 Time  1.06 s\n",
            "Epoch :  1  Batch :  18  Loss :   0.9107991456985474  Accuracy :  0.6777777777777778 Time  1.08 s\n",
            "Epoch :  1  Batch :  19  Loss :   0.8943573079611126  Accuracy :  0.6868421052631579 Time  1.06 s\n",
            "Epoch :  1  Batch :  20  Loss :   0.8825404971837998  Accuracy :  0.6975 Time  1.08 s\n",
            "Epoch :  1  Batch :  21  Loss :   0.86933833360672  Accuracy :  0.7047619047619048 Time  1.07 s\n",
            "Epoch :  1  Batch :  22  Loss :   0.8598655001683668  Accuracy :  0.7068181818181818 Time  1.08 s\n",
            "Epoch :  1  Batch :  23  Loss :   0.8422389833823495  Accuracy :  0.717391304347826 Time  1.06 s\n",
            "Epoch :  1  Batch :  24  Loss :   0.8298668190836906  Accuracy :  0.7229166666666667 Time  1.06 s\n",
            "Epoch :  1  Batch :  25  Loss :   0.8160005843639374  Accuracy :  0.732 Time  1.07 s\n",
            "Epoch :  1  Batch :  26  Loss :   0.802433505654335  Accuracy :  0.7384615384615385 Time  1.07 s\n",
            "Epoch :  1  Batch :  27  Loss :   0.7922833241798259  Accuracy :  0.7388888888888889 Time  1.07 s\n",
            "Epoch :  1  Batch :  28  Loss :   0.7913941985794476  Accuracy :  0.7339285714285714 Time  1.05 s\n",
            "Epoch :  1  Batch :  29  Loss :   0.7790664958542791  Accuracy :  0.7413793103448276 Time  1.07 s\n",
            "Epoch :  1  Batch :  30  Loss :   0.7674957931041717  Accuracy :  0.745 Time  1.06 s\n",
            "Epoch :  1  Batch :  31  Loss :   0.7553153307207169  Accuracy :  0.75 Time  1.06 s\n",
            "Epoch :  1  Batch :  32  Loss :   0.7409453494474292  Accuracy :  0.7578125 Time  1.09 s\n",
            "Epoch :  1  Batch :  33  Loss :   0.7320326322858984  Accuracy :  0.7621212121212121 Time  1.06 s\n",
            "Epoch :  1  Batch :  34  Loss :   0.7293382032829172  Accuracy :  0.7602941176470588 Time  1.06 s\n",
            "Epoch :  1  Batch :  35  Loss :   0.7197564627443042  Accuracy :  0.7642857142857142 Time  1.08 s\n",
            "Epoch :  1  Batch :  36  Loss :   0.7168752940164672  Accuracy :  0.7652777777777777 Time  1.08 s\n",
            "Epoch :  1  Batch :  37  Loss :   0.706950153047974  Accuracy :  0.768918918918919 Time  1.07 s\n",
            "Epoch :  1  Batch :  38  Loss :   0.6981932167944155  Accuracy :  0.7723684210526316 Time  1.09 s\n",
            "Epoch :  1  Batch :  39  Loss :   0.6925277748169043  Accuracy :  0.7743589743589744 Time  1.06 s\n",
            "Epoch :  1  Batch :  40  Loss :   0.6848987385630607  Accuracy :  0.775 Time  1.07 s\n",
            "Epoch :  1  Batch :  41  Loss :   0.6747594831920252  Accuracy :  0.7780487804878049 Time  1.08 s\n",
            "Epoch :  1  Batch :  42  Loss :   0.6649986427454722  Accuracy :  0.7821428571428571 Time  1.08 s\n",
            "Epoch :  1  Batch :  43  Loss :   0.6613578262717225  Accuracy :  0.7837209302325582 Time  1.09 s\n",
            "Epoch :  1  Batch :  44  Loss :   0.6543258367614313  Accuracy :  0.7852272727272728 Time  1.06 s\n",
            "Epoch :  1  Batch :  45  Loss :   0.6461316910054948  Accuracy :  0.7877777777777778 Time  1.08 s\n",
            "Epoch :  1  Batch :  46  Loss :   0.6378611345653948  Accuracy :  0.7913043478260869 Time  1.06 s\n",
            "Epoch :  1  Batch :  47  Loss :   0.6291963033219601  Accuracy :  0.7946808510638298 Time  1.08 s\n",
            "Epoch :  1  Batch :  48  Loss :   0.6230068802833557  Accuracy :  0.796875 Time  1.06 s\n",
            "Epoch :  1  Batch :  49  Loss :   0.615021217538386  Accuracy :  0.7989795918367347 Time  1.07 s\n",
            "Epoch :  1  Batch :  50  Loss :   0.6068826016783714  Accuracy :  0.803 Time  1.07 s\n",
            "Epoch :  1  Batch :  51  Loss :   0.6034491924094219  Accuracy :  0.803921568627451 Time  1.06 s\n",
            "Epoch :  1  Batch :  52  Loss :   0.605835400808316  Accuracy :  0.8038461538461539 Time  1.07 s\n",
            "Epoch :  1  Batch :  53  Loss :   0.6003733021470735  Accuracy :  0.8056603773584906 Time  1.07 s\n",
            "Epoch :  1  Batch :  54  Loss :   0.5958812995641319  Accuracy :  0.8074074074074075 Time  1.07 s\n",
            "Epoch :  1  Batch :  55  Loss :   0.5921913014216856  Accuracy :  0.8081818181818182 Time  1.06 s\n",
            "Epoch :  1  Batch :  56  Loss :   0.5851990088288274  Accuracy :  0.8107142857142857 Time  1.07 s\n",
            "Epoch :  1  Batch :  57  Loss :   0.5774349324535906  Accuracy :  0.8140350877192982 Time  1.09 s\n",
            "Epoch :  1  Batch :  58  Loss :   0.5715405771444584  Accuracy :  0.8155172413793104 Time  1.06 s\n",
            "Epoch :  1  Batch :  59  Loss :   0.5685140500634404  Accuracy :  0.8169491525423729 Time  1.06 s\n",
            "Epoch :  1  Batch :  60  Loss :   0.5650685692826907  Accuracy :  0.8175 Time  1.08 s\n",
            "Epoch :  1  Batch :  61  Loss :   0.5574390562098535  Accuracy :  0.8204918032786885 Time  1.06 s\n",
            "Epoch :  1  Batch :  62  Loss :   0.5533125916075322  Accuracy :  0.8217741935483871 Time  1.07 s\n",
            "Epoch :  1  Batch :  63  Loss :   0.5494002885525189  Accuracy :  0.823015873015873 Time  1.06 s\n",
            "Epoch :  1  Batch :  64  Loss :   0.546212108922191  Accuracy :  0.82421875 Time  1.06 s\n",
            "Epoch :  1  Batch :  65  Loss :   0.5463683459621209  Accuracy :  0.823076923076923 Time  1.08 s\n",
            "Epoch :  1  Batch :  66  Loss :   0.5431462220396056  Accuracy :  0.8242424242424242 Time  1.09 s\n",
            "Epoch :  1  Batch :  67  Loss :   0.539353777779572  Accuracy :  0.8246268656716418 Time  1.08 s\n",
            "Epoch :  1  Batch :  68  Loss :   0.5355135602328707  Accuracy :  0.825735294117647 Time  1.08 s\n",
            "Epoch :  1  Batch :  69  Loss :   0.5302090043391007  Accuracy :  0.827536231884058 Time  1.07 s\n",
            "Epoch :  1  Batch :  70  Loss :   0.5249707746718612  Accuracy :  0.8292857142857143 Time  1.07 s\n",
            "Epoch :  1  Batch :  71  Loss :   0.5195788406360318  Accuracy :  0.8309859154929577 Time  1.09 s\n",
            "Epoch :  1  Batch :  72  Loss :   0.5165492723592453  Accuracy :  0.8319444444444445 Time  1.06 s\n",
            "Epoch :  1  Batch :  73  Loss :   0.5130102884483664  Accuracy :  0.8328767123287671 Time  1.08 s\n",
            "Epoch :  1  Batch :  74  Loss :   0.5081581025308853  Accuracy :  0.8351351351351352 Time  1.07 s\n",
            "Epoch :  1  Batch :  75  Loss :   0.504089689552784  Accuracy :  0.8366666666666667 Time  1.07 s\n",
            "Epoch :  1  Batch :  76  Loss :   0.4984575623744412  Accuracy :  0.8388157894736842 Time  1.07 s\n",
            "Epoch :  1  Batch :  77  Loss :   0.4936877280086666  Accuracy :  0.8402597402597403 Time  1.06 s\n",
            "Epoch :  1  Batch :  78  Loss :   0.49007154351625687  Accuracy :  0.841025641025641 Time  1.06 s\n",
            "Epoch :  1  Batch :  79  Loss :   0.4872632600084136  Accuracy :  0.8417721518987342 Time  1.08 s\n",
            "Epoch :  1  Batch :  80  Loss :   0.4854637525975704  Accuracy :  0.841875 Time  1.08 s\n",
            "Epoch :  1  Batch :  81  Loss :   0.48100582215889  Accuracy :  0.8432098765432099 Time  1.07 s\n",
            "Epoch :  1  Batch :  82  Loss :   0.4797965470792317  Accuracy :  0.8426829268292683 Time  1.08 s\n",
            "Epoch :  1  Batch :  83  Loss :   0.47955322741384965  Accuracy :  0.841566265060241 Time  1.08 s\n",
            "Epoch :  1  Batch :  84  Loss :   0.4763057207954781  Accuracy :  0.8428571428571429 Time  1.06 s\n",
            "Epoch :  1  Batch :  85  Loss :   0.4714113132042043  Accuracy :  0.8447058823529412 Time  1.09 s\n",
            "Epoch :  1  Batch :  86  Loss :   0.47024911104939704  Accuracy :  0.8453488372093023 Time  1.09 s\n",
            "Epoch :  1  Batch :  87  Loss :   0.46682874156140736  Accuracy :  0.8459770114942529 Time  1.07 s\n",
            "Epoch :  1  Batch :  88  Loss :   0.46398839642378414  Accuracy :  0.8471590909090909 Time  1.09 s\n",
            "Epoch :  1  Batch :  89  Loss :   0.4601254274838426  Accuracy :  0.848314606741573 Time  1.08 s\n",
            "Epoch :  1  Batch :  90  Loss :   0.45777971156769326  Accuracy :  0.8488888888888889 Time  1.08 s\n",
            "Epoch :  1  Batch :  91  Loss :   0.45380742160173565  Accuracy :  0.8505494505494505 Time  1.07 s\n",
            "Epoch :  1  Batch :  92  Loss :   0.45016936002218205  Accuracy :  0.8516304347826087 Time  1.08 s\n",
            "Epoch :  1  Batch :  93  Loss :   0.4469663576092771  Accuracy :  0.8526881720430107 Time  1.07 s\n",
            "Epoch :  1  Batch :  94  Loss :   0.44665561093294875  Accuracy :  0.8526595744680852 Time  1.07 s\n",
            "Epoch :  1  Batch :  95  Loss :   0.44455652001656987  Accuracy :  0.8531578947368421 Time  1.08 s\n",
            "Epoch :  1  Batch :  96  Loss :   0.4439592754157881  Accuracy :  0.853125 Time  1.06 s\n",
            "Epoch :  1  Batch :  97  Loss :   0.44239436612301264  Accuracy :  0.8541237113402061 Time  1.07 s\n",
            "Epoch :  1  Batch :  98  Loss :   0.43883043953350614  Accuracy :  0.8556122448979592 Time  1.08 s\n",
            "Epoch :  1  Batch :  99  Loss :   0.43570995932877665  Accuracy :  0.8565656565656565 Time  1.07 s\n",
            "Epoch :  1  Batch :  100  Loss :   0.4327734586596489  Accuracy :  0.8575 Time  1.08 s\n",
            "Epoch :  1  Batch :  101  Loss :   0.43321548181005043  Accuracy :  0.8569306930693069 Time  1.08 s\n",
            "Epoch :  1  Batch :  102  Loss :   0.43256907544884027  Accuracy :  0.8563725490196078 Time  1.07 s\n",
            "Epoch :  1  Batch :  103  Loss :   0.4295505461038895  Accuracy :  0.8572815533980582 Time  1.07 s\n",
            "Epoch :  1  Batch :  104  Loss :   0.4261447281505053  Accuracy :  0.8586538461538461 Time  1.06 s\n",
            "Epoch :  1  Batch :  105  Loss :   0.4252560977424894  Accuracy :  0.8590476190476191 Time  1.09 s\n",
            "Epoch :  1  Batch :  106  Loss :   0.42260704540981436  Accuracy :  0.8599056603773585 Time  1.08 s\n",
            "Epoch :  1  Batch :  107  Loss :   0.4218744485177726  Accuracy :  0.8598130841121495 Time  1.06 s\n",
            "Epoch :  1  Batch :  108  Loss :   0.4207967699125961  Accuracy :  0.8601851851851852 Time  1.07 s\n",
            "Epoch :  1  Batch :  109  Loss :   0.4184544869792571  Accuracy :  0.8610091743119266 Time  1.08 s\n",
            "Epoch :  1  Batch :  110  Loss :   0.4159369192340157  Accuracy :  0.8618181818181818 Time  1.05 s\n",
            "Epoch :  1  Batch :  111  Loss :   0.4133525047753308  Accuracy :  0.8626126126126126 Time  1.08 s\n",
            "Epoch :  1  Batch :  112  Loss :   0.4115630994950022  Accuracy :  0.8629464285714286 Time  1.08 s\n",
            "Epoch :  1  Batch :  113  Loss :   0.40909813529094763  Accuracy :  0.863716814159292 Time  1.08 s\n",
            "Epoch :  1  Batch :  114  Loss :   0.4059785568531145  Accuracy :  0.8649122807017544 Time  1.07 s\n",
            "Epoch :  1  Batch :  115  Loss :   0.4036211156974668  Accuracy :  0.8656521739130435 Time  1.08 s\n",
            "Epoch :  1  Batch :  116  Loss :   0.4009157342772032  Accuracy :  0.8668103448275862 Time  1.07 s\n",
            "Epoch :  1  Batch :  117  Loss :   0.3988390323570651  Accuracy :  0.8675213675213675 Time  1.06 s\n",
            "Epoch :  1  Batch :  118  Loss :   0.3961718764345525  Accuracy :  0.8682203389830508 Time  1.08 s\n",
            "Epoch :  1  Batch :  119  Loss :   0.3940834565823819  Accuracy :  0.8689075630252101 Time  1.06 s\n",
            "Epoch :  1  Batch :  120  Loss :   0.3928916277984778  Accuracy :  0.8695833333333334 Time  1.07 s\n",
            "Epoch :  1  Batch :  121  Loss :   0.39058112562441627  Accuracy :  0.8702479338842976 Time  1.07 s\n",
            "Epoch :  1  Batch :  122  Loss :   0.38857902739135947  Accuracy :  0.8704918032786885 Time  1.06 s\n",
            "Epoch :  1  Batch :  123  Loss :   0.38584732206735184  Accuracy :  0.8715447154471545 Time  1.07 s\n",
            "Epoch :  1  Batch :  124  Loss :   0.38628340461441585  Accuracy :  0.8709677419354839 Time  1.06 s\n",
            "Epoch :  1  Batch :  125  Loss :   0.3844643789231777  Accuracy :  0.8716 Time  1.09 s\n",
            "Epoch :  1  Batch :  126  Loss :   0.38231112462069305  Accuracy :  0.8722222222222222 Time  1.06 s\n",
            "Epoch :  1  Batch :  127  Loss :   0.3801263834489143  Accuracy :  0.8732283464566929 Time  1.06 s\n",
            "Epoch :  1  Batch :  128  Loss :   0.37855868830229156  Accuracy :  0.873828125 Time  1.09 s\n",
            "Epoch :  1  Batch :  129  Loss :   0.37922927803655926  Accuracy :  0.8736434108527131 Time  1.07 s\n",
            "Epoch :  1  Batch :  130  Loss :   0.3791304212063551  Accuracy :  0.8738461538461538 Time  1.06 s\n",
            "Epoch :  1  Batch :  131  Loss :   0.379108784279523  Accuracy :  0.8732824427480916 Time  1.09 s\n",
            "Epoch :  1  Batch :  132  Loss :   0.37735467175529763  Accuracy :  0.8738636363636364 Time  1.06 s\n",
            "Epoch :  1  Batch :  133  Loss :   0.37634425377823355  Accuracy :  0.8736842105263158 Time  1.07 s\n",
            "Epoch :  1  Batch :  134  Loss :   0.37446668174173403  Accuracy :  0.8742537313432835 Time  1.07 s\n",
            "Epoch :  1  Batch :  135  Loss :   0.3738549169290949  Accuracy :  0.8744444444444445 Time  1.08 s\n",
            "Epoch :  1  Batch :  136  Loss :   0.3733740327058031  Accuracy :  0.8742647058823529 Time  1.07 s\n",
            "Epoch :  1  Batch :  137  Loss :   0.3711838207486337  Accuracy :  0.8751824817518248 Time  1.07 s\n",
            "Epoch :  1  Batch :  138  Loss :   0.3691528069962194  Accuracy :  0.8760869565217392 Time  1.08 s\n",
            "Epoch :  1  Batch :  139  Loss :   0.3667158562448814  Accuracy :  0.8769784172661871 Time  1.07 s\n",
            "Epoch :  1  Batch :  140  Loss :   0.36599564794451  Accuracy :  0.8775 Time  1.06 s\n",
            "Epoch :  1  Batch :  141  Loss :   0.36548809133839943  Accuracy :  0.8776595744680851 Time  1.1 s\n",
            "Epoch :  1  Batch :  142  Loss :   0.36352771051018173  Accuracy :  0.8785211267605634 Time  1.08 s\n",
            "Epoch :  1  Batch :  143  Loss :   0.36178071007549345  Accuracy :  0.879020979020979 Time  1.08 s\n",
            "Epoch :  1  Batch :  144  Loss :   0.36050153246873784  Accuracy :  0.8795138888888889 Time  1.08 s\n",
            "Epoch :  1  Batch :  145  Loss :   0.3593426578260701  Accuracy :  0.88 Time  1.06 s\n",
            "Epoch :  1  Batch :  146  Loss :   0.3579769948216742  Accuracy :  0.8801369863013698 Time  1.08 s\n",
            "Epoch :  1  Batch :  147  Loss :   0.35633960794530756  Accuracy :  0.8806122448979592 Time  1.1 s\n",
            "Epoch :  1  Batch :  148  Loss :   0.35490555510025573  Accuracy :  0.8810810810810811 Time  1.07 s\n",
            "Epoch :  1  Batch :  149  Loss :   0.3540622959250972  Accuracy :  0.8815436241610738 Time  1.06 s\n",
            "Epoch :  1  Batch :  150  Loss :   0.3526122740159432  Accuracy :  0.882 Time  1.08 s\n",
            "Epoch :  1  Batch :  151  Loss :   0.3528246310422357  Accuracy :  0.8817880794701987 Time  1.08 s\n",
            "Epoch :  1  Batch :  152  Loss :   0.3510969299087791  Accuracy :  0.8825657894736842 Time  1.07 s\n",
            "Epoch :  1  Batch :  153  Loss :   0.349972256477557  Accuracy :  0.8830065359477124 Time  1.08 s\n",
            "Epoch :  1  Batch :  154  Loss :   0.3486885107424739  Accuracy :  0.8834415584415585 Time  1.07 s\n",
            "Epoch :  1  Batch :  155  Loss :   0.3466406637382123  Accuracy :  0.8841935483870967 Time  1.09 s\n",
            "Epoch :  1  Batch :  156  Loss :   0.3450059224732029  Accuracy :  0.8849358974358974 Time  1.06 s\n",
            "Epoch :  1  Batch :  157  Loss :   0.3438166621716539  Accuracy :  0.8853503184713376 Time  1.08 s\n",
            "Epoch :  1  Batch :  158  Loss :   0.3428417671283212  Accuracy :  0.8854430379746835 Time  1.07 s\n",
            "Epoch :  1  Batch :  159  Loss :   0.3419728986450336  Accuracy :  0.8855345911949686 Time  1.07 s\n",
            "Epoch :  1  Batch :  160  Loss :   0.34048627393785863  Accuracy :  0.8859375 Time  1.08 s\n",
            "Epoch :  1  Batch :  161  Loss :   0.33927254556961683  Accuracy :  0.8860248447204969 Time  1.07 s\n",
            "Epoch :  1  Batch :  162  Loss :   0.33862381108841044  Accuracy :  0.8861111111111111 Time  1.09 s\n",
            "Epoch :  1  Batch :  163  Loss :   0.33847973795116315  Accuracy :  0.8858895705521472 Time  1.07 s\n",
            "Epoch :  1  Batch :  164  Loss :   0.3383946124600565  Accuracy :  0.8859756097560976 Time  1.06 s\n",
            "Epoch :  1  Batch :  165  Loss :   0.336719864200462  Accuracy :  0.8866666666666667 Time  1.08 s\n",
            "Epoch :  1  Batch :  166  Loss :   0.3350123919950551  Accuracy :  0.8873493975903615 Time  1.1 s\n",
            "Epoch :  1  Batch :  167  Loss :   0.3335825544140653  Accuracy :  0.8877245508982036 Time  1.08 s\n",
            "Epoch :  1  Batch :  168  Loss :   0.33240616190735073  Accuracy :  0.887797619047619 Time  1.06 s\n",
            "Epoch :  1  Batch :  169  Loss :   0.331541353979936  Accuracy :  0.8881656804733727 Time  1.07 s\n",
            "Epoch :  1  Batch :  170  Loss :   0.3316846965209526  Accuracy :  0.888235294117647 Time  1.06 s\n",
            "Epoch :  1  Batch :  171  Loss :   0.3311271510843994  Accuracy :  0.8885964912280702 Time  1.07 s\n",
            "Epoch :  1  Batch :  172  Loss :   0.3296836091118843  Accuracy :  0.8892441860465117 Time  1.09 s\n",
            "Epoch :  1  Batch :  173  Loss :   0.3288353557316209  Accuracy :  0.8893063583815028 Time  1.09 s\n",
            "Epoch :  1  Batch :  174  Loss :   0.32714977023330916  Accuracy :  0.8899425287356322 Time  1.06 s\n",
            "Epoch :  1  Batch :  175  Loss :   0.32669464649898666  Accuracy :  0.89 Time  1.07 s\n",
            "Epoch :  1  Batch :  176  Loss :   0.3259996728844602  Accuracy :  0.8900568181818181 Time  1.07 s\n",
            "Epoch :  1  Batch :  177  Loss :   0.32452369319264496  Accuracy :  0.8903954802259887 Time  1.07 s\n",
            "Epoch :  1  Batch :  178  Loss :   0.3240445162682386  Accuracy :  0.8907303370786517 Time  1.07 s\n",
            "Epoch :  1  Batch :  179  Loss :   0.32259101972150406  Accuracy :  0.891340782122905 Time  1.09 s\n",
            "Epoch :  1  Batch :  180  Loss :   0.32107074693259263  Accuracy :  0.8919444444444444 Time  1.08 s\n",
            "Epoch :  1  Batch :  181  Loss :   0.3206536653168623  Accuracy :  0.8922651933701657 Time  1.07 s\n",
            "Epoch :  1  Batch :  182  Loss :   0.3190507924204672  Accuracy :  0.8928571428571429 Time  1.06 s\n",
            "Epoch :  1  Batch :  183  Loss :   0.317596671685495  Accuracy :  0.8934426229508197 Time  1.07 s\n",
            "Epoch :  1  Batch :  184  Loss :   0.31656963119040366  Accuracy :  0.8934782608695652 Time  1.08 s\n",
            "Epoch :  1  Batch :  185  Loss :   0.3150774834929286  Accuracy :  0.894054054054054 Time  1.05 s\n",
            "Epoch :  1  Batch :  186  Loss :   0.31366398866458606  Accuracy :  0.8946236559139785 Time  1.07 s\n",
            "Epoch :  1  Batch :  187  Loss :   0.31264065811659564  Accuracy :  0.8946524064171123 Time  1.06 s\n",
            "Epoch :  1  Batch :  188  Loss :   0.3112972991422136  Accuracy :  0.8952127659574468 Time  1.07 s\n",
            "Epoch :  1  Batch :  189  Loss :   0.3110804380405517  Accuracy :  0.894973544973545 Time  1.09 s\n",
            "Epoch :  1  Batch :  190  Loss :   0.3110070003490699  Accuracy :  0.8952631578947369 Time  1.07 s\n",
            "Epoch :  1  Batch :  191  Loss :   0.3096742736302433  Accuracy :  0.8958115183246074 Time  1.08 s\n",
            "Epoch :  1  Batch :  192  Loss :   0.30849025400433067  Accuracy :  0.89609375 Time  1.06 s\n",
            "Epoch :  1  Batch :  193  Loss :   0.3082004787274902  Accuracy :  0.8958549222797928 Time  1.09 s\n",
            "Epoch :  1  Batch :  194  Loss :   0.30845833395022093  Accuracy :  0.895618556701031 Time  1.06 s\n",
            "Epoch :  1  Batch :  195  Loss :   0.3080807316952791  Accuracy :  0.8958974358974359 Time  1.06 s\n",
            "Epoch :  1  Batch :  196  Loss :   0.30818380881100893  Accuracy :  0.8956632653061225 Time  1.09 s\n",
            "Epoch :  1  Batch :  197  Loss :   0.3068352717773866  Accuracy :  0.8961928934010153 Time  1.06 s\n",
            "Epoch :  1  Batch :  198  Loss :   0.30653794811605806  Accuracy :  0.8962121212121212 Time  1.07 s\n",
            "Epoch :  1  Batch :  199  Loss :   0.30540438604789166  Accuracy :  0.8964824120603015 Time  1.09 s\n",
            "Epoch :  1  Batch :  200  Loss :   0.30414165910333396  Accuracy :  0.897 Time  1.08 s\n",
            "Epoch :  1  Batch :  201  Loss :   0.3031006506574688  Accuracy :  0.8972636815920398 Time  1.08 s\n",
            "Epoch :  1  Batch :  202  Loss :   0.302147011724439  Accuracy :  0.8975247524752475 Time  1.08 s\n",
            "Epoch :  1  Batch :  203  Loss :   0.302145704920656  Accuracy :  0.8975369458128079 Time  1.08 s\n",
            "Epoch :  1  Batch :  204  Loss :   0.3009058570708422  Accuracy :  0.8980392156862745 Time  1.1 s\n",
            "Epoch :  1  Batch :  205  Loss :   0.2996092068531164  Accuracy :  0.8985365853658537 Time  1.07 s\n",
            "Epoch :  1  Batch :  206  Loss :   0.2989174214924134  Accuracy :  0.8985436893203883 Time  1.08 s\n",
            "Epoch :  1  Batch :  207  Loss :   0.29782160682882663  Accuracy :  0.8990338164251208 Time  1.08 s\n",
            "Epoch :  1  Batch :  208  Loss :   0.29699126385653823  Accuracy :  0.8990384615384616 Time  1.06 s\n",
            "Epoch :  1  Batch :  209  Loss :   0.29576932004336537  Accuracy :  0.8995215311004785 Time  1.09 s\n",
            "Epoch :  1  Batch :  210  Loss :   0.2972986790750708  Accuracy :  0.8985714285714286 Time  1.08 s\n",
            "Epoch :  1  Batch :  211  Loss :   0.29608728116934335  Accuracy :  0.8990521327014218 Time  1.08 s\n",
            "Epoch :  1  Batch :  212  Loss :   0.29716624690325194  Accuracy :  0.8985849056603774 Time  1.06 s\n",
            "Epoch :  1  Batch :  213  Loss :   0.29737467978132165  Accuracy :  0.8985915492957747 Time  1.09 s\n",
            "Epoch :  1  Batch :  214  Loss :   0.2968811377291089  Accuracy :  0.8985981308411215 Time  1.07 s\n",
            "Epoch :  1  Batch :  215  Loss :   0.29631798286424127  Accuracy :  0.8986046511627906 Time  1.09 s\n",
            "Epoch :  1  Batch :  216  Loss :   0.29584214924317265  Accuracy :  0.8988425925925926 Time  1.06 s\n",
            "Epoch :  1  Batch :  217  Loss :   0.29466053343931653  Accuracy :  0.8993087557603686 Time  1.07 s\n",
            "Epoch :  1  Batch :  218  Loss :   0.2940412915798776  Accuracy :  0.8995412844036698 Time  1.09 s\n",
            "Epoch :  1  Batch :  219  Loss :   0.2929585724218523  Accuracy :  0.9 Time  1.06 s\n",
            "Epoch :  1  Batch :  220  Loss :   0.2919483763758432  Accuracy :  0.9004545454545455 Time  1.08 s\n",
            "Epoch :  1  Batch :  221  Loss :   0.2910597591210005  Accuracy :  0.9006787330316742 Time  1.06 s\n",
            "Epoch :  1  Batch :  222  Loss :   0.2898079767730926  Accuracy :  0.9011261261261261 Time  1.06 s\n",
            "Epoch :  1  Batch :  223  Loss :   0.2888586044086001  Accuracy :  0.9013452914798207 Time  1.08 s\n",
            "Epoch :  1  Batch :  224  Loss :   0.288011583304199  Accuracy :  0.9015625 Time  1.08 s\n",
            "Epoch :  1  Batch :  225  Loss :   0.28733207683182427  Accuracy :  0.9017777777777778 Time  1.06 s\n",
            "Epoch :  1  Batch :  226  Loss :   0.2866446733911546  Accuracy :  0.9019911504424779 Time  1.09 s\n",
            "Epoch :  1  Batch :  227  Loss :   0.2859912307292945  Accuracy :  0.9022026431718062 Time  1.08 s\n",
            "Epoch :  1  Batch :  228  Loss :   0.2850016071420294  Accuracy :  0.9026315789473685 Time  1.07 s\n",
            "Epoch :  1  Batch :  229  Loss :   0.28453647031745666  Accuracy :  0.9026200873362445 Time  1.08 s\n",
            "Epoch :  1  Batch :  230  Loss :   0.2840235166130183  Accuracy :  0.9028260869565218 Time  1.06 s\n",
            "Epoch :  1  Batch :  231  Loss :   0.2829392618052178  Accuracy :  0.9032467532467533 Time  1.08 s\n",
            "Epoch :  1  Batch :  232  Loss :   0.2824433206877639  Accuracy :  0.903448275862069 Time  1.08 s\n",
            "Epoch :  1  Batch :  233  Loss :   0.28174913091417164  Accuracy :  0.9036480686695278 Time  1.08 s\n",
            "Epoch :  1  Batch :  234  Loss :   0.2812507250187043  Accuracy :  0.9038461538461539 Time  1.07 s\n",
            "Epoch :  1  Batch :  235  Loss :   0.280918214252179  Accuracy :  0.9038297872340425 Time  1.08 s\n",
            "Epoch :  1  Batch :  236  Loss :   0.27999390802598734  Accuracy :  0.9040254237288136 Time  1.08 s\n",
            "Epoch :  1  Batch :  237  Loss :   0.2790511745354357  Accuracy :  0.9044303797468355 Time  1.08 s\n",
            "Epoch :  1  Batch :  238  Loss :   0.278150999656401  Accuracy :  0.9048319327731092 Time  1.09 s\n",
            "Epoch :  1  Batch :  239  Loss :   0.27767821607495574  Accuracy :  0.905020920502092 Time  1.07 s\n",
            "Epoch :  1  Batch :  240  Loss :   0.2779118889050248  Accuracy :  0.9047916666666667 Time  1.07 s\n",
            "Epoch :  1  Batch :  241  Loss :   0.2769286098978888  Accuracy :  0.9051867219917012 Time  1.1 s\n",
            "Epoch :  1  Batch :  242  Loss :   0.2758613814799931  Accuracy :  0.9055785123966942 Time  1.07 s\n",
            "Epoch :  1  Batch :  243  Loss :   0.27481815519784825  Accuracy :  0.9059670781893004 Time  1.08 s\n",
            "Epoch :  1  Batch :  244  Loss :   0.27456409863165776  Accuracy :  0.9059426229508196 Time  1.06 s\n",
            "Epoch :  1  Batch :  245  Loss :   0.2736788992560944  Accuracy :  0.9063265306122449 Time  1.06 s\n",
            "Epoch :  1  Batch :  246  Loss :   0.2727242693706317  Accuracy :  0.9067073170731708 Time  1.06 s\n",
            "Epoch :  1  Batch :  247  Loss :   0.27280823205058513  Accuracy :  0.9064777327935223 Time  1.07 s\n",
            "Epoch :  1  Batch :  248  Loss :   0.27189070240579427  Accuracy :  0.9068548387096774 Time  1.08 s\n",
            "Epoch :  1  Batch :  249  Loss :   0.27111125179757195  Accuracy :  0.9070281124497992 Time  1.08 s\n",
            "Epoch :  1  Batch :  250  Loss :   0.2701606056578457  Accuracy :  0.9074 Time  1.09 s\n",
            "Epoch :  1  Batch :  251  Loss :   0.2692494449127777  Accuracy :  0.9077689243027889 Time  1.08 s\n",
            "Epoch :  1  Batch :  252  Loss :   0.26826261992507155  Accuracy :  0.9081349206349206 Time  1.09 s\n",
            "Epoch :  1  Batch :  253  Loss :   0.26751296802722063  Accuracy :  0.908498023715415 Time  1.07 s\n",
            "Epoch :  1  Batch :  254  Loss :   0.267259390133719  Accuracy :  0.9086614173228347 Time  1.06 s\n",
            "Epoch :  1  Batch :  255  Loss :   0.2663618702697111  Accuracy :  0.9090196078431373 Time  1.09 s\n",
            "Epoch :  1  Batch :  256  Loss :   0.2654381009233475  Accuracy :  0.909375 Time  1.07 s\n",
            "Epoch :  1  Batch :  257  Loss :   0.26469486828980393  Accuracy :  0.9095330739299611 Time  1.07 s\n",
            "Epoch :  1  Batch :  258  Loss :   0.26519622375408924  Accuracy :  0.9093023255813953 Time  1.06 s\n",
            "Epoch :  1  Batch :  259  Loss :   0.2650758431807275  Accuracy :  0.9094594594594595 Time  1.08 s\n",
            "Epoch :  1  Batch :  260  Loss :   0.2644899569344349  Accuracy :  0.9096153846153846 Time  1.07 s\n",
            "Epoch :  1  Batch :  261  Loss :   0.26393682411532865  Accuracy :  0.9099616858237548 Time  1.07 s\n",
            "Epoch :  1  Batch :  262  Loss :   0.26314684760379997  Accuracy :  0.9103053435114504 Time  1.08 s\n",
            "Epoch :  1  Batch :  263  Loss :   0.26222602024801894  Accuracy :  0.9106463878326996 Time  1.07 s\n",
            "Epoch :  1  Batch :  264  Loss :   0.2615608752627547  Accuracy :  0.9109848484848485 Time  1.09 s\n",
            "Epoch :  1  Batch :  265  Loss :   0.2607179996715683  Accuracy :  0.9113207547169812 Time  1.07 s\n",
            "Epoch :  1  Batch :  266  Loss :   0.25994548574440124  Accuracy :  0.9116541353383458 Time  1.06 s\n",
            "Epoch :  1  Batch :  267  Loss :   0.25931684555981155  Accuracy :  0.9117977528089888 Time  1.07 s\n",
            "Epoch :  1  Batch :  268  Loss :   0.2587973038742402  Accuracy :  0.9119402985074627 Time  1.06 s\n",
            "Epoch :  1  Batch :  269  Loss :   0.25794825574459307  Accuracy :  0.912267657992565 Time  1.07 s\n",
            "Epoch :  1  Batch :  270  Loss :   0.25748702151424907  Accuracy :  0.9124074074074074 Time  1.08 s\n",
            "Epoch :  1  Batch :  271  Loss :   0.2567865667259836  Accuracy :  0.9125461254612546 Time  1.06 s\n",
            "Epoch :  1  Batch :  272  Loss :   0.257403810032616  Accuracy :  0.9123161764705883 Time  1.07 s\n",
            "Epoch :  1  Batch :  273  Loss :   0.2572279095056129  Accuracy :  0.9120879120879121 Time  1.06 s\n",
            "Epoch :  1  Batch :  274  Loss :   0.25637626649648715  Accuracy :  0.9124087591240876 Time  1.07 s\n",
            "Epoch :  1  Batch :  275  Loss :   0.25550239137966524  Accuracy :  0.9127272727272727 Time  1.07 s\n",
            "Epoch :  1  Batch :  276  Loss :   0.2547449417230066  Accuracy :  0.9130434782608695 Time  1.07 s\n",
            "Epoch :  1  Batch :  277  Loss :   0.25410499515529683  Accuracy :  0.9133574007220217 Time  1.07 s\n",
            "Epoch :  1  Batch :  278  Loss :   0.25331879403482344  Accuracy :  0.9136690647482014 Time  1.09 s\n",
            "Epoch :  1  Batch :  279  Loss :   0.253236830571673  Accuracy :  0.9134408602150538 Time  1.06 s\n",
            "Epoch :  1  Batch :  280  Loss :   0.25248118688551974  Accuracy :  0.91375 Time  1.09 s\n",
            "Epoch :  1  Batch :  281  Loss :   0.25188696354269663  Accuracy :  0.9138790035587189 Time  1.07 s\n",
            "Epoch :  1  Batch :  282  Loss :   0.251329922893395  Accuracy :  0.9140070921985816 Time  1.08 s\n",
            "Epoch :  1  Batch :  283  Loss :   0.25108107310051736  Accuracy :  0.9141342756183746 Time  1.08 s\n",
            "Epoch :  1  Batch :  284  Loss :   0.25043068625833054  Accuracy :  0.9142605633802817 Time  1.09 s\n",
            "Epoch :  1  Batch :  285  Loss :   0.2501838673527042  Accuracy :  0.9142105263157895 Time  1.07 s\n",
            "Epoch :  1  Batch :  286  Loss :   0.2497401083949823  Accuracy :  0.9143356643356644 Time  1.09 s\n",
            "Epoch :  1  Batch :  287  Loss :   0.24969993839656954  Accuracy :  0.9144599303135889 Time  1.07 s\n",
            "Epoch :  1  Batch :  288  Loss :   0.2491166726330347  Accuracy :  0.9145833333333333 Time  1.1 s\n",
            "Epoch :  1  Batch :  289  Loss :   0.2484501283583602  Accuracy :  0.9147058823529411 Time  1.08 s\n",
            "Epoch :  1  Batch :  290  Loss :   0.24802250144360907  Accuracy :  0.915 Time  1.07 s\n",
            "Epoch :  1  Batch :  291  Loss :   0.24762031662876355  Accuracy :  0.9149484536082474 Time  1.09 s\n",
            "Epoch :  1  Batch :  292  Loss :   0.24691546783935636  Accuracy :  0.9152397260273972 Time  1.06 s\n",
            "Epoch :  1  Batch :  293  Loss :   0.2476751389770225  Accuracy :  0.9150170648464164 Time  1.09 s\n",
            "Epoch :  1  Batch :  294  Loss :   0.2469026413682823  Accuracy :  0.9153061224489796 Time  1.08 s\n",
            "Epoch :  1  Batch :  295  Loss :   0.24644563190828442  Accuracy :  0.9154237288135593 Time  1.06 s\n",
            "Epoch :  1  Batch :  296  Loss :   0.24611999515154576  Accuracy :  0.9153716216216217 Time  1.1 s\n",
            "Epoch :  1  Batch :  297  Loss :   0.24563901189413637  Accuracy :  0.9154882154882155 Time  1.06 s\n",
            "Epoch :  1  Batch :  298  Loss :   0.24500998602429994  Accuracy :  0.9156040268456376 Time  1.07 s\n",
            "Epoch :  1  Batch :  299  Loss :   0.24428382741106694  Accuracy :  0.915886287625418 Time  1.09 s\n",
            "Epoch :  1  Batch :  300  Loss :   0.24449676201678813  Accuracy :  0.9156666666666666 Time  1.08 s\n",
            "Epoch :  1  Batch :  301  Loss :   0.2451082387431664  Accuracy :  0.9156146179401994 Time  1.05 s\n",
            "Epoch :  1  Batch :  302  Loss :   0.24439381960076312  Accuracy :  0.9158940397350993 Time  1.06 s\n",
            "Epoch :  1  Batch :  303  Loss :   0.243669211203632  Accuracy :  0.9161716171617161 Time  1.06 s\n",
            "Epoch :  1  Batch :  304  Loss :   0.2430815535236003  Accuracy :  0.9162828947368421 Time  1.06 s\n",
            "Epoch :  1  Batch :  305  Loss :   0.24230560268290707  Accuracy :  0.9165573770491803 Time  1.05 s\n",
            "Epoch :  1  Batch :  306  Loss :   0.243014033798591  Accuracy :  0.9163398692810457 Time  1.08 s\n",
            "Epoch :  1  Batch :  307  Loss :   0.24226616760139455  Accuracy :  0.9164763458401305 Time  0.6 s\n",
            "Accuracy of     0 : 93 %\n",
            "Accuracy of     1 : 90 %\n",
            "Accuracy of     2 : 88 %\n",
            "[1 epoch] Accuracy of the network on the Training images: 91 %\n",
            "Epoch :  2  Batch :  1  Loss :   0.016184289008378983  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  2  Batch :  2  Loss :   0.10938500799238682  Accuracy :  0.975 Time  1.06 s\n",
            "Epoch :  2  Batch :  3  Loss :   0.07991190440952778  Accuracy :  0.9833333333333333 Time  1.05 s\n",
            "Epoch :  2  Batch :  4  Loss :   0.071913608815521  Accuracy :  0.9875 Time  1.05 s\n",
            "Epoch :  2  Batch :  5  Loss :   0.08627483062446117  Accuracy :  0.98 Time  1.05 s\n",
            "Epoch :  2  Batch :  6  Loss :   0.0734722053942581  Accuracy :  0.9833333333333333 Time  1.06 s\n",
            "Epoch :  2  Batch :  7  Loss :   0.10023710863398654  Accuracy :  0.9714285714285714 Time  1.05 s\n",
            "Epoch :  2  Batch :  8  Loss :   0.09225534403230995  Accuracy :  0.975 Time  1.06 s\n",
            "Epoch :  2  Batch :  9  Loss :   0.09878222923725843  Accuracy :  0.9722222222222222 Time  1.06 s\n",
            "Epoch :  2  Batch :  10  Loss :   0.09964538542553783  Accuracy :  0.97 Time  1.06 s\n",
            "Epoch :  2  Batch :  11  Loss :   0.09831235583194277  Accuracy :  0.9727272727272728 Time  1.07 s\n",
            "Epoch :  2  Batch :  12  Loss :   0.09908481710590422  Accuracy :  0.9708333333333333 Time  1.07 s\n",
            "Epoch :  2  Batch :  13  Loss :   0.09389664569439796  Accuracy :  0.9730769230769231 Time  1.08 s\n",
            "Epoch :  2  Batch :  14  Loss :   0.10129181608291608  Accuracy :  0.9714285714285714 Time  1.07 s\n",
            "Epoch :  2  Batch :  15  Loss :   0.10285639706999064  Accuracy :  0.97 Time  1.07 s\n",
            "Epoch :  2  Batch :  16  Loss :   0.10116139374440536  Accuracy :  0.971875 Time  1.06 s\n",
            "Epoch :  2  Batch :  17  Loss :   0.09807981917744174  Accuracy :  0.9735294117647059 Time  1.06 s\n",
            "Epoch :  2  Batch :  18  Loss :   0.09462061850354075  Accuracy :  0.975 Time  1.06 s\n",
            "Epoch :  2  Batch :  19  Loss :   0.09409181258984302  Accuracy :  0.9736842105263158 Time  1.07 s\n",
            "Epoch :  2  Batch :  20  Loss :   0.0919460333418101  Accuracy :  0.975 Time  1.06 s\n",
            "Epoch :  2  Batch :  21  Loss :   0.10256270917930774  Accuracy :  0.969047619047619 Time  1.06 s\n",
            "Epoch :  2  Batch :  22  Loss :   0.1107184297235852  Accuracy :  0.9659090909090909 Time  1.07 s\n",
            "Epoch :  2  Batch :  23  Loss :   0.10695728696072879  Accuracy :  0.967391304347826 Time  1.06 s\n",
            "Epoch :  2  Batch :  24  Loss :   0.1048163998639211  Accuracy :  0.96875 Time  1.08 s\n",
            "Epoch :  2  Batch :  25  Loss :   0.10127514373511076  Accuracy :  0.97 Time  1.06 s\n",
            "Epoch :  2  Batch :  26  Loss :   0.09876238937991169  Accuracy :  0.9711538461538461 Time  1.05 s\n",
            "Epoch :  2  Batch :  27  Loss :   0.09722111315500957  Accuracy :  0.9703703703703703 Time  1.05 s\n",
            "Epoch :  2  Batch :  28  Loss :   0.0963750872428396  Accuracy :  0.9714285714285714 Time  1.05 s\n",
            "Epoch :  2  Batch :  29  Loss :   0.09387541796755174  Accuracy :  0.9724137931034482 Time  1.05 s\n",
            "Epoch :  2  Batch :  30  Loss :   0.09533542348071933  Accuracy :  0.9716666666666667 Time  1.05 s\n",
            "Epoch :  2  Batch :  31  Loss :   0.09425242152065039  Accuracy :  0.9725806451612903 Time  1.05 s\n",
            "Epoch :  2  Batch :  32  Loss :   0.09483634473872371  Accuracy :  0.9703125 Time  1.05 s\n",
            "Epoch :  2  Batch :  33  Loss :   0.09440023432288205  Accuracy :  0.9696969696969697 Time  1.05 s\n",
            "Epoch :  2  Batch :  34  Loss :   0.09244174085667028  Accuracy :  0.9705882352941176 Time  1.07 s\n",
            "Epoch :  2  Batch :  35  Loss :   0.09037273198898349  Accuracy :  0.9714285714285714 Time  1.06 s\n",
            "Epoch :  2  Batch :  36  Loss :   0.09244571473552948  Accuracy :  0.9708333333333333 Time  1.04 s\n",
            "Epoch :  2  Batch :  37  Loss :   0.09488867396941862  Accuracy :  0.9702702702702702 Time  1.04 s\n",
            "Epoch :  2  Batch :  38  Loss :   0.09522915996709153  Accuracy :  0.9697368421052631 Time  1.04 s\n",
            "Epoch :  2  Batch :  39  Loss :   0.09477153241347808  Accuracy :  0.9692307692307692 Time  1.05 s\n",
            "Epoch :  2  Batch :  40  Loss :   0.09367911017034203  Accuracy :  0.97 Time  1.04 s\n",
            "Epoch :  2  Batch :  41  Loss :   0.09172315872842218  Accuracy :  0.9707317073170731 Time  1.04 s\n",
            "Epoch :  2  Batch :  42  Loss :   0.08986469207420236  Accuracy :  0.9714285714285714 Time  1.04 s\n",
            "Epoch :  2  Batch :  43  Loss :   0.0897231352363908  Accuracy :  0.9709302325581395 Time  1.04 s\n",
            "Epoch :  2  Batch :  44  Loss :   0.08829925768077374  Accuracy :  0.9715909090909091 Time  1.06 s\n",
            "Epoch :  2  Batch :  45  Loss :   0.0867470420897007  Accuracy :  0.9722222222222222 Time  1.06 s\n",
            "Epoch :  2  Batch :  46  Loss :   0.08593410901401353  Accuracy :  0.9717391304347827 Time  1.05 s\n",
            "Epoch :  2  Batch :  47  Loss :   0.08458332812532465  Accuracy :  0.9723404255319149 Time  1.04 s\n",
            "Epoch :  2  Batch :  48  Loss :   0.08405138575471938  Accuracy :  0.9729166666666667 Time  1.04 s\n",
            "Epoch :  2  Batch :  49  Loss :   0.08330537386390628  Accuracy :  0.9734693877551021 Time  1.05 s\n",
            "Epoch :  2  Batch :  50  Loss :   0.08333479888737201  Accuracy :  0.974 Time  1.05 s\n",
            "Epoch :  2  Batch :  51  Loss :   0.08192845779087614  Accuracy :  0.9745098039215686 Time  1.04 s\n",
            "Epoch :  2  Batch :  52  Loss :   0.08121641576648332  Accuracy :  0.975 Time  1.04 s\n",
            "Epoch :  2  Batch :  53  Loss :   0.08044389180206465  Accuracy :  0.9754716981132076 Time  1.05 s\n",
            "Epoch :  2  Batch :  54  Loss :   0.07917648602139067  Accuracy :  0.975925925925926 Time  1.04 s\n",
            "Epoch :  2  Batch :  55  Loss :   0.07839280400763858  Accuracy :  0.9763636363636363 Time  1.05 s\n",
            "Epoch :  2  Batch :  56  Loss :   0.07769749054153051  Accuracy :  0.9767857142857143 Time  1.06 s\n",
            "Epoch :  2  Batch :  57  Loss :   0.07734118068688794  Accuracy :  0.9771929824561404 Time  1.06 s\n",
            "Epoch :  2  Batch :  58  Loss :   0.0765043815662121  Accuracy :  0.9775862068965517 Time  1.04 s\n",
            "Epoch :  2  Batch :  59  Loss :   0.07543734044341718  Accuracy :  0.9779661016949153 Time  1.04 s\n",
            "Epoch :  2  Batch :  60  Loss :   0.07792114367087682  Accuracy :  0.9766666666666667 Time  1.04 s\n",
            "Epoch :  2  Batch :  61  Loss :   0.07764068685594153  Accuracy :  0.9770491803278688 Time  1.04 s\n",
            "Epoch :  2  Batch :  62  Loss :   0.07661048419052555  Accuracy :  0.9774193548387097 Time  1.05 s\n",
            "Epoch :  2  Batch :  63  Loss :   0.07628784618443912  Accuracy :  0.976984126984127 Time  1.05 s\n",
            "Epoch :  2  Batch :  64  Loss :   0.07534683574340306  Accuracy :  0.97734375 Time  1.04 s\n",
            "Epoch :  2  Batch :  65  Loss :   0.07451785974777662  Accuracy :  0.9776923076923076 Time  1.04 s\n",
            "Epoch :  2  Batch :  66  Loss :   0.07453467324376106  Accuracy :  0.9772727272727273 Time  1.06 s\n",
            "Epoch :  2  Batch :  67  Loss :   0.07473891239557694  Accuracy :  0.9768656716417911 Time  1.06 s\n",
            "Epoch :  2  Batch :  68  Loss :   0.07454928996808388  Accuracy :  0.9772058823529411 Time  1.06 s\n",
            "Epoch :  2  Batch :  69  Loss :   0.07369899293542773  Accuracy :  0.9775362318840579 Time  1.05 s\n",
            "Epoch :  2  Batch :  70  Loss :   0.07321395520120859  Accuracy :  0.9778571428571429 Time  1.05 s\n",
            "Epoch :  2  Batch :  71  Loss :   0.07276412380308332  Accuracy :  0.9781690140845071 Time  1.05 s\n",
            "Epoch :  2  Batch :  72  Loss :   0.07254515404606031  Accuracy :  0.9777777777777777 Time  1.05 s\n",
            "Epoch :  2  Batch :  73  Loss :   0.07208642396718672  Accuracy :  0.9780821917808219 Time  1.05 s\n",
            "Epoch :  2  Batch :  74  Loss :   0.07127008558175452  Accuracy :  0.9783783783783784 Time  1.05 s\n",
            "Epoch :  2  Batch :  75  Loss :   0.07037030369664232  Accuracy :  0.9786666666666667 Time  1.05 s\n",
            "Epoch :  2  Batch :  76  Loss :   0.06997074167807832  Accuracy :  0.9789473684210527 Time  1.05 s\n",
            "Epoch :  2  Batch :  77  Loss :   0.06922679489430089  Accuracy :  0.9792207792207792 Time  1.06 s\n",
            "Epoch :  2  Batch :  78  Loss :   0.0702293750961335  Accuracy :  0.9788461538461538 Time  1.06 s\n",
            "Epoch :  2  Batch :  79  Loss :   0.06965778393765228  Accuracy :  0.9791139240506329 Time  1.07 s\n",
            "Epoch :  2  Batch :  80  Loss :   0.06899472587392666  Accuracy :  0.979375 Time  1.07 s\n",
            "Epoch :  2  Batch :  81  Loss :   0.06820702357332648  Accuracy :  0.9796296296296296 Time  1.04 s\n",
            "Epoch :  2  Batch :  82  Loss :   0.07179535514243492  Accuracy :  0.9786585365853658 Time  1.05 s\n",
            "Epoch :  2  Batch :  83  Loss :   0.07146556955952961  Accuracy :  0.9789156626506024 Time  1.05 s\n",
            "Epoch :  2  Batch :  84  Loss :   0.07082054775119537  Accuracy :  0.9791666666666666 Time  1.05 s\n",
            "Epoch :  2  Batch :  85  Loss :   0.07024688159718233  Accuracy :  0.9794117647058823 Time  1.05 s\n",
            "Epoch :  2  Batch :  86  Loss :   0.07120345428932545  Accuracy :  0.9784883720930233 Time  1.04 s\n",
            "Epoch :  2  Batch :  87  Loss :   0.07081998554968286  Accuracy :  0.978735632183908 Time  1.05 s\n",
            "Epoch :  2  Batch :  88  Loss :   0.0703529306602749  Accuracy :  0.9789772727272728 Time  1.06 s\n",
            "Epoch :  2  Batch :  89  Loss :   0.069677605042548  Accuracy :  0.9792134831460674 Time  1.07 s\n",
            "Epoch :  2  Batch :  90  Loss :   0.06895244920419322  Accuracy :  0.9794444444444445 Time  1.06 s\n",
            "Epoch :  2  Batch :  91  Loss :   0.0686321625342736  Accuracy :  0.9796703296703296 Time  1.05 s\n",
            "Epoch :  2  Batch :  92  Loss :   0.06797713992874259  Accuracy :  0.9798913043478261 Time  1.05 s\n",
            "Epoch :  2  Batch :  93  Loss :   0.06742546785502665  Accuracy :  0.9801075268817204 Time  1.05 s\n",
            "Epoch :  2  Batch :  94  Loss :   0.06680809910547861  Accuracy :  0.9803191489361702 Time  1.05 s\n",
            "Epoch :  2  Batch :  95  Loss :   0.06637175771358766  Accuracy :  0.9805263157894737 Time  1.05 s\n",
            "Epoch :  2  Batch :  96  Loss :   0.06633037307377283  Accuracy :  0.9802083333333333 Time  1.06 s\n",
            "Epoch :  2  Batch :  97  Loss :   0.06578655822260171  Accuracy :  0.9804123711340206 Time  1.07 s\n",
            "Epoch :  2  Batch :  98  Loss :   0.06531984346671676  Accuracy :  0.9806122448979592 Time  1.06 s\n",
            "Epoch :  2  Batch :  99  Loss :   0.06527121435625083  Accuracy :  0.9808080808080808 Time  1.06 s\n",
            "Epoch :  2  Batch :  100  Loss :   0.06485036442987621  Accuracy :  0.981 Time  1.06 s\n",
            "Epoch :  2  Batch :  101  Loss :   0.06425442652186693  Accuracy :  0.9811881188118812 Time  1.06 s\n",
            "Epoch :  2  Batch :  102  Loss :   0.06398291720132179  Accuracy :  0.9813725490196078 Time  1.05 s\n",
            "Epoch :  2  Batch :  103  Loss :   0.06412432316093913  Accuracy :  0.9810679611650486 Time  1.05 s\n",
            "Epoch :  2  Batch :  104  Loss :   0.06353626845297046  Accuracy :  0.98125 Time  1.05 s\n",
            "Epoch :  2  Batch :  105  Loss :   0.06309834380767175  Accuracy :  0.9814285714285714 Time  1.04 s\n",
            "Epoch :  2  Batch :  106  Loss :   0.06320937050787627  Accuracy :  0.9811320754716981 Time  1.05 s\n",
            "Epoch :  2  Batch :  107  Loss :   0.0627687460716779  Accuracy :  0.9813084112149533 Time  1.04 s\n",
            "Epoch :  2  Batch :  108  Loss :   0.06293874131343155  Accuracy :  0.9810185185185185 Time  1.05 s\n",
            "Epoch :  2  Batch :  109  Loss :   0.06250911365784363  Accuracy :  0.9811926605504587 Time  1.05 s\n",
            "Epoch :  2  Batch :  110  Loss :   0.06235559033229947  Accuracy :  0.9813636363636363 Time  1.06 s\n",
            "Epoch :  2  Batch :  111  Loss :   0.06182895597229871  Accuracy :  0.9815315315315315 Time  1.06 s\n",
            "Epoch :  2  Batch :  112  Loss :   0.061403632677476186  Accuracy :  0.9816964285714286 Time  1.06 s\n",
            "Epoch :  2  Batch :  113  Loss :   0.06098658152839449  Accuracy :  0.981858407079646 Time  1.06 s\n",
            "Epoch :  2  Batch :  114  Loss :   0.060915751838157964  Accuracy :  0.9815789473684211 Time  1.04 s\n",
            "Epoch :  2  Batch :  115  Loss :   0.06043354597388079  Accuracy :  0.9817391304347826 Time  1.05 s\n",
            "Epoch :  2  Batch :  116  Loss :   0.06001818240901198  Accuracy :  0.9818965517241379 Time  1.05 s\n",
            "Epoch :  2  Batch :  117  Loss :   0.05953988801632236  Accuracy :  0.982051282051282 Time  1.04 s\n",
            "Epoch :  2  Batch :  118  Loss :   0.060151381254338235  Accuracy :  0.9817796610169491 Time  1.05 s\n",
            "Epoch :  2  Batch :  119  Loss :   0.05971104325530972  Accuracy :  0.9819327731092437 Time  1.04 s\n",
            "Epoch :  2  Batch :  120  Loss :   0.059276847729537015  Accuracy :  0.9820833333333333 Time  1.04 s\n",
            "Epoch :  2  Batch :  121  Loss :   0.05881822029752118  Accuracy :  0.9822314049586777 Time  1.06 s\n",
            "Epoch :  2  Batch :  122  Loss :   0.05855906594995043  Accuracy :  0.9823770491803279 Time  1.07 s\n",
            "Epoch :  2  Batch :  123  Loss :   0.05869668763607922  Accuracy :  0.982520325203252 Time  1.06 s\n",
            "Epoch :  2  Batch :  124  Loss :   0.058581415434262804  Accuracy :  0.9826612903225806 Time  1.05 s\n",
            "Epoch :  2  Batch :  125  Loss :   0.058887043168768284  Accuracy :  0.9824 Time  1.05 s\n",
            "Epoch :  2  Batch :  126  Loss :   0.058536985835489184  Accuracy :  0.9825396825396825 Time  1.05 s\n",
            "Epoch :  2  Batch :  127  Loss :   0.058095868761233224  Accuracy :  0.9826771653543307 Time  1.04 s\n",
            "Epoch :  2  Batch :  128  Loss :   0.05910289880921482  Accuracy :  0.982421875 Time  1.05 s\n",
            "Epoch :  2  Batch :  129  Loss :   0.05932468568270867  Accuracy :  0.982170542635659 Time  1.05 s\n",
            "Epoch :  2  Batch :  130  Loss :   0.05908966502532936  Accuracy :  0.9823076923076923 Time  1.04 s\n",
            "Epoch :  2  Batch :  131  Loss :   0.058879959471665945  Accuracy :  0.982442748091603 Time  1.04 s\n",
            "Epoch :  2  Batch :  132  Loss :   0.05870981158505222  Accuracy :  0.9825757575757575 Time  1.07 s\n",
            "Epoch :  2  Batch :  133  Loss :   0.05830019065424016  Accuracy :  0.9827067669172932 Time  1.06 s\n",
            "Epoch :  2  Batch :  134  Loss :   0.05856615335527641  Accuracy :  0.9824626865671642 Time  1.06 s\n",
            "Epoch :  2  Batch :  135  Loss :   0.058159776464863505  Accuracy :  0.9825925925925926 Time  1.05 s\n",
            "Epoch :  2  Batch :  136  Loss :   0.05803090511129566  Accuracy :  0.9827205882352941 Time  1.04 s\n",
            "Epoch :  2  Batch :  137  Loss :   0.05763791800123116  Accuracy :  0.9828467153284671 Time  1.04 s\n",
            "Epoch :  2  Batch :  138  Loss :   0.05740742634583697  Accuracy :  0.9829710144927536 Time  1.05 s\n",
            "Epoch :  2  Batch :  139  Loss :   0.057285216759233704  Accuracy :  0.9830935251798562 Time  1.05 s\n",
            "Epoch :  2  Batch :  140  Loss :   0.05696077913079145  Accuracy :  0.9832142857142857 Time  1.04 s\n",
            "Epoch :  2  Batch :  141  Loss :   0.056804193139195124  Accuracy :  0.9833333333333333 Time  1.05 s\n",
            "Epoch :  2  Batch :  142  Loss :   0.059617869131280185  Accuracy :  0.9827464788732394 Time  1.05 s\n",
            "Epoch :  2  Batch :  143  Loss :   0.059940599473323544  Accuracy :  0.9825174825174825 Time  1.06 s\n",
            "Epoch :  2  Batch :  144  Loss :   0.0616302732329738  Accuracy :  0.9819444444444444 Time  1.06 s\n",
            "Epoch :  2  Batch :  145  Loss :   0.0614590367832189  Accuracy :  0.9820689655172414 Time  1.06 s\n",
            "Epoch :  2  Batch :  146  Loss :   0.06149683519719449  Accuracy :  0.9818493150684932 Time  1.05 s\n",
            "Epoch :  2  Batch :  147  Loss :   0.061105708738941014  Accuracy :  0.9819727891156462 Time  1.05 s\n",
            "Epoch :  2  Batch :  148  Loss :   0.060722037279865124  Accuracy :  0.9820945945945946 Time  1.05 s\n",
            "Epoch :  2  Batch :  149  Loss :   0.061226370386566795  Accuracy :  0.9818791946308725 Time  1.04 s\n",
            "Epoch :  2  Batch :  150  Loss :   0.0608395769353956  Accuracy :  0.982 Time  1.04 s\n",
            "Epoch :  2  Batch :  151  Loss :   0.060494270857226175  Accuracy :  0.9821192052980132 Time  1.04 s\n",
            "Epoch :  2  Batch :  152  Loss :   0.06014942570503703  Accuracy :  0.9822368421052632 Time  1.04 s\n",
            "Epoch :  2  Batch :  153  Loss :   0.05980225898982847  Accuracy :  0.9823529411764705 Time  1.05 s\n",
            "Epoch :  2  Batch :  154  Loss :   0.060167134854361995  Accuracy :  0.9821428571428571 Time  1.06 s\n",
            "Epoch :  2  Batch :  155  Loss :   0.06296539698997813  Accuracy :  0.9809677419354839 Time  1.06 s\n",
            "Epoch :  2  Batch :  156  Loss :   0.06328075955239817  Accuracy :  0.9807692307692307 Time  1.06 s\n",
            "Epoch :  2  Batch :  157  Loss :   0.06325987983305173  Accuracy :  0.9808917197452229 Time  1.05 s\n",
            "Epoch :  2  Batch :  158  Loss :   0.06290849633043326  Accuracy :  0.9810126582278481 Time  1.05 s\n",
            "Epoch :  2  Batch :  159  Loss :   0.0625901324908782  Accuracy :  0.9811320754716981 Time  1.04 s\n",
            "Epoch :  2  Batch :  160  Loss :   0.06248279819847084  Accuracy :  0.98125 Time  1.04 s\n",
            "Epoch :  2  Batch :  161  Loss :   0.06213068408799005  Accuracy :  0.9813664596273292 Time  1.05 s\n",
            "Epoch :  2  Batch :  162  Loss :   0.0631330429237152  Accuracy :  0.9811728395061728 Time  1.05 s\n",
            "Epoch :  2  Batch :  163  Loss :   0.06287955433027419  Accuracy :  0.9812883435582822 Time  1.04 s\n",
            "Epoch :  2  Batch :  164  Loss :   0.06259646659387594  Accuracy :  0.9814024390243903 Time  1.04 s\n",
            "Epoch :  2  Batch :  165  Loss :   0.06259166013280099  Accuracy :  0.9812121212121212 Time  1.07 s\n",
            "Epoch :  2  Batch :  166  Loss :   0.06233271363707461  Accuracy :  0.9813253012048193 Time  1.07 s\n",
            "Epoch :  2  Batch :  167  Loss :   0.06226618906683818  Accuracy :  0.981437125748503 Time  1.06 s\n",
            "Epoch :  2  Batch :  168  Loss :   0.06198051017764512  Accuracy :  0.981547619047619 Time  1.05 s\n",
            "Epoch :  2  Batch :  169  Loss :   0.06166998491231242  Accuracy :  0.9816568047337279 Time  1.05 s\n",
            "Epoch :  2  Batch :  170  Loss :   0.061380720070070206  Accuracy :  0.981764705882353 Time  1.05 s\n",
            "Epoch :  2  Batch :  171  Loss :   0.06112156603321956  Accuracy :  0.9818713450292398 Time  1.04 s\n",
            "Epoch :  2  Batch :  172  Loss :   0.060781401043302964  Accuracy :  0.9819767441860465 Time  1.05 s\n",
            "Epoch :  2  Batch :  173  Loss :   0.060688734100324056  Accuracy :  0.9820809248554914 Time  1.05 s\n",
            "Epoch :  2  Batch :  174  Loss :   0.06065142488952769  Accuracy :  0.9818965517241379 Time  1.05 s\n",
            "Epoch :  2  Batch :  175  Loss :   0.06047907182414617  Accuracy :  0.982 Time  1.05 s\n",
            "Epoch :  2  Batch :  176  Loss :   0.06033057249839079  Accuracy :  0.9821022727272727 Time  1.06 s\n",
            "Epoch :  2  Batch :  177  Loss :   0.06025396687594258  Accuracy :  0.9822033898305085 Time  1.06 s\n",
            "Epoch :  2  Batch :  178  Loss :   0.059956647081593616  Accuracy :  0.9823033707865169 Time  1.06 s\n",
            "Epoch :  2  Batch :  179  Loss :   0.05994857739730004  Accuracy :  0.982122905027933 Time  1.05 s\n",
            "Epoch :  2  Batch :  180  Loss :   0.05967815085055513  Accuracy :  0.9822222222222222 Time  1.05 s\n",
            "Epoch :  2  Batch :  181  Loss :   0.059408139352537484  Accuracy :  0.9823204419889503 Time  1.06 s\n",
            "Epoch :  2  Batch :  182  Loss :   0.0592174706033912  Accuracy :  0.9824175824175824 Time  1.05 s\n",
            "Epoch :  2  Batch :  183  Loss :   0.0590294510548517  Accuracy :  0.9825136612021858 Time  1.05 s\n",
            "Epoch :  2  Batch :  184  Loss :   0.058803788242031536  Accuracy :  0.9826086956521739 Time  1.05 s\n",
            "Epoch :  2  Batch :  185  Loss :   0.05880590952882493  Accuracy :  0.9824324324324324 Time  1.05 s\n",
            "Epoch :  2  Batch :  186  Loss :   0.05862734137072919  Accuracy :  0.9825268817204301 Time  1.05 s\n",
            "Epoch :  2  Batch :  187  Loss :   0.05872943510057375  Accuracy :  0.9823529411764705 Time  1.06 s\n",
            "Epoch :  2  Batch :  188  Loss :   0.0588566557717252  Accuracy :  0.9824468085106383 Time  1.06 s\n",
            "Epoch :  2  Batch :  189  Loss :   0.058646897876526785  Accuracy :  0.9825396825396825 Time  1.06 s\n",
            "Epoch :  2  Batch :  190  Loss :   0.058362774852369174  Accuracy :  0.9826315789473684 Time  1.05 s\n",
            "Epoch :  2  Batch :  191  Loss :   0.05810016642583056  Accuracy :  0.9827225130890053 Time  1.05 s\n",
            "Epoch :  2  Batch :  192  Loss :   0.05898359510320006  Accuracy :  0.9825520833333333 Time  1.05 s\n",
            "Epoch :  2  Batch :  193  Loss :   0.059903041988012856  Accuracy :  0.9823834196891191 Time  1.05 s\n",
            "Epoch :  2  Batch :  194  Loss :   0.05986764248623753  Accuracy :  0.9824742268041237 Time  1.04 s\n",
            "Epoch :  2  Batch :  195  Loss :   0.05961149063868783  Accuracy :  0.9825641025641025 Time  1.04 s\n",
            "Epoch :  2  Batch :  196  Loss :   0.0593921908638345  Accuracy :  0.9826530612244898 Time  1.05 s\n",
            "Epoch :  2  Batch :  197  Loss :   0.06021583360397649  Accuracy :  0.98248730964467 Time  1.05 s\n",
            "Epoch :  2  Batch :  198  Loss :   0.06101397590739935  Accuracy :  0.9823232323232324 Time  1.06 s\n",
            "Epoch :  2  Batch :  199  Loss :   0.06077776720803512  Accuracy :  0.9824120603015075 Time  1.06 s\n",
            "Epoch :  2  Batch :  200  Loss :   0.060826717640738936  Accuracy :  0.9825 Time  1.06 s\n",
            "Epoch :  2  Batch :  201  Loss :   0.06054916902588894  Accuracy :  0.9825870646766169 Time  1.05 s\n",
            "Epoch :  2  Batch :  202  Loss :   0.06093146820497321  Accuracy :  0.9824257425742574 Time  1.05 s\n",
            "Epoch :  2  Batch :  203  Loss :   0.060723103993466775  Accuracy :  0.982512315270936 Time  1.04 s\n",
            "Epoch :  2  Batch :  204  Loss :   0.06048224164930848  Accuracy :  0.9825980392156862 Time  1.04 s\n",
            "Epoch :  2  Batch :  205  Loss :   0.060230136483271676  Accuracy :  0.9826829268292683 Time  1.05 s\n",
            "Epoch :  2  Batch :  206  Loss :   0.05998895991168627  Accuracy :  0.9827669902912621 Time  1.05 s\n",
            "Epoch :  2  Batch :  207  Loss :   0.06002643223231038  Accuracy :  0.9826086956521739 Time  1.04 s\n",
            "Epoch :  2  Batch :  208  Loss :   0.060570946404298484  Accuracy :  0.9824519230769231 Time  1.04 s\n",
            "Epoch :  2  Batch :  209  Loss :   0.060404147393555874  Accuracy :  0.9825358851674642 Time  1.04 s\n",
            "Epoch :  2  Batch :  210  Loss :   0.06021665630285584  Accuracy :  0.9826190476190476 Time  1.07 s\n",
            "Epoch :  2  Batch :  211  Loss :   0.06014058567027416  Accuracy :  0.9827014218009479 Time  1.06 s\n",
            "Epoch :  2  Batch :  212  Loss :   0.06037696775374055  Accuracy :  0.9825471698113207 Time  1.04 s\n",
            "Epoch :  2  Batch :  213  Loss :   0.060336545750306385  Accuracy :  0.9826291079812206 Time  1.05 s\n",
            "Epoch :  2  Batch :  214  Loss :   0.060253266534533036  Accuracy :  0.9827102803738318 Time  1.05 s\n",
            "Epoch :  2  Batch :  215  Loss :   0.06009557489089148  Accuracy :  0.9827906976744186 Time  1.05 s\n",
            "Epoch :  2  Batch :  216  Loss :   0.05987257937495424  Accuracy :  0.9828703703703704 Time  1.04 s\n",
            "Epoch :  2  Batch :  217  Loss :   0.05961416173224179  Accuracy :  0.9829493087557604 Time  1.04 s\n",
            "Epoch :  2  Batch :  218  Loss :   0.059374915909047726  Accuracy :  0.9830275229357798 Time  1.05 s\n",
            "Epoch :  2  Batch :  219  Loss :   0.05929753114044088  Accuracy :  0.9831050228310503 Time  1.04 s\n",
            "Epoch :  2  Batch :  220  Loss :   0.05909511660204523  Accuracy :  0.9831818181818182 Time  1.05 s\n",
            "Epoch :  2  Batch :  221  Loss :   0.059424757518135515  Accuracy :  0.9830316742081447 Time  1.05 s\n",
            "Epoch :  2  Batch :  222  Loss :   0.06020279648097081  Accuracy :  0.9826576576576577 Time  1.06 s\n",
            "Epoch :  2  Batch :  223  Loss :   0.060017473844293456  Accuracy :  0.9827354260089686 Time  1.06 s\n",
            "Epoch :  2  Batch :  224  Loss :   0.05978528344400859  Accuracy :  0.9828125 Time  1.05 s\n",
            "Epoch :  2  Batch :  225  Loss :   0.05965040318564408  Accuracy :  0.9828888888888889 Time  1.05 s\n",
            "Epoch :  2  Batch :  226  Loss :   0.059402357830163253  Accuracy :  0.9829646017699115 Time  1.04 s\n",
            "Epoch :  2  Batch :  227  Loss :   0.05920176526597221  Accuracy :  0.9830396475770925 Time  1.04 s\n",
            "Epoch :  2  Batch :  228  Loss :   0.05903269432427917  Accuracy :  0.9831140350877193 Time  1.04 s\n",
            "Epoch :  2  Batch :  229  Loss :   0.05896931210142361  Accuracy :  0.9831877729257642 Time  1.05 s\n",
            "Epoch :  2  Batch :  230  Loss :   0.05910296440326973  Accuracy :  0.9830434782608696 Time  1.05 s\n",
            "Epoch :  2  Batch :  231  Loss :   0.060424513246025484  Accuracy :  0.9829004329004329 Time  1.04 s\n",
            "Epoch :  2  Batch :  232  Loss :   0.06029868928980532  Accuracy :  0.9829741379310345 Time  1.06 s\n",
            "Epoch :  2  Batch :  233  Loss :   0.06033570108567504  Accuracy :  0.9830472103004292 Time  1.06 s\n",
            "Epoch :  2  Batch :  234  Loss :   0.0601264115753305  Accuracy :  0.9831196581196581 Time  1.06 s\n",
            "Epoch :  2  Batch :  235  Loss :   0.05997912028328852  Accuracy :  0.9831914893617021 Time  1.04 s\n",
            "Epoch :  2  Batch :  236  Loss :   0.05977536766134771  Accuracy :  0.9832627118644067 Time  1.05 s\n",
            "Epoch :  2  Batch :  237  Loss :   0.05971066446047481  Accuracy :  0.9833333333333333 Time  1.04 s\n",
            "Epoch :  2  Batch :  238  Loss :   0.05949759503098659  Accuracy :  0.9834033613445378 Time  1.05 s\n",
            "Epoch :  2  Batch :  239  Loss :   0.05937909928957102  Accuracy :  0.9834728033472804 Time  1.05 s\n",
            "Epoch :  2  Batch :  240  Loss :   0.05970688603702001  Accuracy :  0.9833333333333333 Time  1.05 s\n",
            "Epoch :  2  Batch :  241  Loss :   0.059589713721060776  Accuracy :  0.983402489626556 Time  1.05 s\n",
            "Epoch :  2  Batch :  242  Loss :   0.05945229260366379  Accuracy :  0.9834710743801653 Time  1.04 s\n",
            "Epoch :  2  Batch :  243  Loss :   0.05925849391868024  Accuracy :  0.9835390946502057 Time  1.06 s\n",
            "Epoch :  2  Batch :  244  Loss :   0.059165292913781205  Accuracy :  0.9836065573770492 Time  1.06 s\n",
            "Epoch :  2  Batch :  245  Loss :   0.05894095251450733  Accuracy :  0.9836734693877551 Time  1.07 s\n",
            "Epoch :  2  Batch :  246  Loss :   0.058721979601096694  Accuracy :  0.983739837398374 Time  1.04 s\n",
            "Epoch :  2  Batch :  247  Loss :   0.05865354673665484  Accuracy :  0.9838056680161943 Time  1.05 s\n",
            "Epoch :  2  Batch :  248  Loss :   0.05852520248604818  Accuracy :  0.9838709677419355 Time  1.05 s\n",
            "Epoch :  2  Batch :  249  Loss :   0.05831367975983574  Accuracy :  0.9839357429718876 Time  1.04 s\n",
            "Epoch :  2  Batch :  250  Loss :   0.058511227952316405  Accuracy :  0.9838 Time  1.04 s\n",
            "Epoch :  2  Batch :  251  Loss :   0.05829118513520733  Accuracy :  0.9838645418326694 Time  1.05 s\n",
            "Epoch :  2  Batch :  252  Loss :   0.05817173688214213  Accuracy :  0.9839285714285714 Time  1.04 s\n",
            "Epoch :  2  Batch :  253  Loss :   0.05894881022335748  Accuracy :  0.983794466403162 Time  1.05 s\n",
            "Epoch :  2  Batch :  254  Loss :   0.059054896154800265  Accuracy :  0.9836614173228346 Time  1.06 s\n",
            "Epoch :  2  Batch :  255  Loss :   0.05885374079659289  Accuracy :  0.9837254901960785 Time  1.06 s\n",
            "Epoch :  2  Batch :  256  Loss :   0.05903412169936928  Accuracy :  0.98359375 Time  1.06 s\n",
            "Epoch :  2  Batch :  257  Loss :   0.058827845916036035  Accuracy :  0.9836575875486381 Time  1.05 s\n",
            "Epoch :  2  Batch :  258  Loss :   0.058699755597310936  Accuracy :  0.9837209302325581 Time  1.05 s\n",
            "Epoch :  2  Batch :  259  Loss :   0.059343551814153385  Accuracy :  0.9833976833976834 Time  1.04 s\n",
            "Epoch :  2  Batch :  260  Loss :   0.05916232800541016  Accuracy :  0.9834615384615385 Time  1.05 s\n",
            "Epoch :  2  Batch :  261  Loss :   0.0591440626573517  Accuracy :  0.9833333333333333 Time  1.04 s\n",
            "Epoch :  2  Batch :  262  Loss :   0.05895898322652997  Accuracy :  0.9833969465648855 Time  1.05 s\n",
            "Epoch :  2  Batch :  263  Loss :   0.060224253266426546  Accuracy :  0.9830798479087453 Time  1.05 s\n",
            "Epoch :  2  Batch :  264  Loss :   0.06012194713041412  Accuracy :  0.9831439393939394 Time  1.04 s\n",
            "Epoch :  2  Batch :  265  Loss :   0.06030381548938886  Accuracy :  0.9830188679245283 Time  1.06 s\n",
            "Epoch :  2  Batch :  266  Loss :   0.06010232745718799  Accuracy :  0.9830827067669173 Time  1.06 s\n",
            "Epoch :  2  Batch :  267  Loss :   0.05993664034813196  Accuracy :  0.9831460674157303 Time  1.06 s\n",
            "Epoch :  2  Batch :  268  Loss :   0.05976332597253816  Accuracy :  0.9832089552238806 Time  1.05 s\n",
            "Epoch :  2  Batch :  269  Loss :   0.05969652479175187  Accuracy :  0.983271375464684 Time  1.05 s\n",
            "Epoch :  2  Batch :  270  Loss :   0.059508126508444545  Accuracy :  0.9833333333333333 Time  1.05 s\n",
            "Epoch :  2  Batch :  271  Loss :   0.059340053911551116  Accuracy :  0.9833948339483395 Time  1.05 s\n",
            "Epoch :  2  Batch :  272  Loss :   0.05997867524445824  Accuracy :  0.9830882352941176 Time  1.05 s\n",
            "Epoch :  2  Batch :  273  Loss :   0.05990026422841099  Accuracy :  0.9831501831501831 Time  1.04 s\n",
            "Epoch :  2  Batch :  274  Loss :   0.06016404954988482  Accuracy :  0.983029197080292 Time  1.05 s\n",
            "Epoch :  2  Batch :  275  Loss :   0.05996464245698669  Accuracy :  0.9830909090909091 Time  1.05 s\n",
            "Epoch :  2  Batch :  276  Loss :   0.05987088275614424  Accuracy :  0.9831521739130434 Time  1.06 s\n",
            "Epoch :  2  Batch :  277  Loss :   0.05968553476062492  Accuracy :  0.9832129963898917 Time  1.06 s\n",
            "Epoch :  2  Batch :  278  Loss :   0.05951263962929948  Accuracy :  0.983273381294964 Time  1.07 s\n",
            "Epoch :  2  Batch :  279  Loss :   0.05931956403451474  Accuracy :  0.9833333333333333 Time  1.07 s\n",
            "Epoch :  2  Batch :  280  Loss :   0.059129787218158265  Accuracy :  0.9833928571428572 Time  1.04 s\n",
            "Epoch :  2  Batch :  281  Loss :   0.05912559996011047  Accuracy :  0.9832740213523131 Time  1.04 s\n",
            "Epoch :  2  Batch :  282  Loss :   0.05903616671265493  Accuracy :  0.9833333333333333 Time  1.04 s\n",
            "Epoch :  2  Batch :  283  Loss :   0.05889477607935614  Accuracy :  0.9833922261484099 Time  1.05 s\n",
            "Epoch :  2  Batch :  284  Loss :   0.05886943273174144  Accuracy :  0.9834507042253521 Time  1.05 s\n",
            "Epoch :  2  Batch :  285  Loss :   0.058719943602683775  Accuracy :  0.9835087719298246 Time  1.05 s\n",
            "Epoch :  2  Batch :  286  Loss :   0.05868108643030005  Accuracy :  0.9835664335664336 Time  1.05 s\n",
            "Epoch :  2  Batch :  287  Loss :   0.05860926366154792  Accuracy :  0.9836236933797909 Time  1.06 s\n",
            "Epoch :  2  Batch :  288  Loss :   0.058441586241113126  Accuracy :  0.9836805555555556 Time  1.06 s\n",
            "Epoch :  2  Batch :  289  Loss :   0.060612449872027116  Accuracy :  0.9833910034602076 Time  1.06 s\n",
            "Epoch :  2  Batch :  290  Loss :   0.06074590063011595  Accuracy :  0.9832758620689656 Time  1.04 s\n",
            "Epoch :  2  Batch :  291  Loss :   0.06059991668660155  Accuracy :  0.9833333333333333 Time  1.05 s\n",
            "Epoch :  2  Batch :  292  Loss :   0.061051696849897606  Accuracy :  0.9832191780821918 Time  1.05 s\n",
            "Epoch :  2  Batch :  293  Loss :   0.06087283384048542  Accuracy :  0.9832764505119453 Time  1.05 s\n",
            "Epoch :  2  Batch :  294  Loss :   0.060710231528053576  Accuracy :  0.9833333333333333 Time  1.04 s\n",
            "Epoch :  2  Batch :  295  Loss :   0.060530389678048885  Accuracy :  0.9833898305084746 Time  1.05 s\n",
            "Epoch :  2  Batch :  296  Loss :   0.06034318827142989  Accuracy :  0.9834459459459459 Time  1.05 s\n",
            "Epoch :  2  Batch :  297  Loss :   0.060422103776775225  Accuracy :  0.9835016835016835 Time  1.04 s\n",
            "Epoch :  2  Batch :  298  Loss :   0.060578420270709386  Accuracy :  0.9833892617449664 Time  1.07 s\n",
            "Epoch :  2  Batch :  299  Loss :   0.06041768890646688  Accuracy :  0.9834448160535118 Time  1.06 s\n",
            "Epoch :  2  Batch :  300  Loss :   0.06033122542935113  Accuracy :  0.9835 Time  1.08 s\n",
            "Epoch :  2  Batch :  301  Loss :   0.060213800429976067  Accuracy :  0.9835548172757476 Time  1.07 s\n",
            "Epoch :  2  Batch :  302  Loss :   0.06056029689714115  Accuracy :  0.9834437086092715 Time  1.06 s\n",
            "Epoch :  2  Batch :  303  Loss :   0.06038117972310177  Accuracy :  0.9834983498349835 Time  1.05 s\n",
            "Epoch :  2  Batch :  304  Loss :   0.06019951171842158  Accuracy :  0.9835526315789473 Time  1.05 s\n",
            "Epoch :  2  Batch :  305  Loss :   0.06042841237160514  Accuracy :  0.9834426229508196 Time  1.05 s\n",
            "Epoch :  2  Batch :  306  Loss :   0.06026350148935236  Accuracy :  0.9834967320261437 Time  1.05 s\n",
            "Epoch :  2  Batch :  307  Loss :   0.06047048004410845  Accuracy :  0.9835236541598695 Time  0.54 s\n",
            "Accuracy of     0 : 98 %\n",
            "Accuracy of     1 : 98 %\n",
            "Accuracy of     2 : 96 %\n",
            "[2 epoch] Accuracy of the network on the Training images: 98 %\n",
            "Epoch :  3  Batch :  1  Loss :   0.02121342159807682  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  3  Batch :  2  Loss :   0.03436190728098154  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  3  Batch :  3  Loss :   0.04987619134287039  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  3  Batch :  4  Loss :   0.19123275438323617  Accuracy :  0.975 Time  1.06 s\n",
            "Epoch :  3  Batch :  5  Loss :   0.15407832991331816  Accuracy :  0.98 Time  1.06 s\n",
            "Epoch :  3  Batch :  6  Loss :   0.1499456923144559  Accuracy :  0.975 Time  1.07 s\n",
            "Epoch :  3  Batch :  7  Loss :   0.13145377606685674  Accuracy :  0.9785714285714285 Time  1.05 s\n",
            "Epoch :  3  Batch :  8  Loss :   0.11640096548944712  Accuracy :  0.98125 Time  1.04 s\n",
            "Epoch :  3  Batch :  9  Loss :   0.10896007426910931  Accuracy :  0.9833333333333333 Time  1.04 s\n",
            "Epoch :  3  Batch :  10  Loss :   0.09965998921543359  Accuracy :  0.985 Time  1.05 s\n",
            "Epoch :  3  Batch :  11  Loss :   0.09136637088588694  Accuracy :  0.9863636363636363 Time  1.06 s\n",
            "Epoch :  3  Batch :  12  Loss :   0.08994191877233486  Accuracy :  0.9833333333333333 Time  1.05 s\n",
            "Epoch :  3  Batch :  13  Loss :   0.08375578036961648  Accuracy :  0.9846153846153847 Time  1.05 s\n",
            "Epoch :  3  Batch :  14  Loss :   0.07956603122875094  Accuracy :  0.9857142857142858 Time  1.04 s\n",
            "Epoch :  3  Batch :  15  Loss :   0.07525884006172419  Accuracy :  0.9866666666666667 Time  1.06 s\n",
            "Epoch :  3  Batch :  16  Loss :   0.07294168014777824  Accuracy :  0.9875 Time  1.07 s\n",
            "Epoch :  3  Batch :  17  Loss :   0.07238349159631659  Accuracy :  0.9882352941176471 Time  1.06 s\n",
            "Epoch :  3  Batch :  18  Loss :   0.06878227445607384  Accuracy :  0.9888888888888889 Time  1.04 s\n",
            "Epoch :  3  Batch :  19  Loss :   0.06620854703023245  Accuracy :  0.9894736842105263 Time  1.04 s\n",
            "Epoch :  3  Batch :  20  Loss :   0.06473787217400968  Accuracy :  0.99 Time  1.04 s\n",
            "Epoch :  3  Batch :  21  Loss :   0.06421201476561171  Accuracy :  0.9904761904761905 Time  1.05 s\n",
            "Epoch :  3  Batch :  22  Loss :   0.06304816050793637  Accuracy :  0.990909090909091 Time  1.05 s\n",
            "Epoch :  3  Batch :  23  Loss :   0.061384342570343746  Accuracy :  0.991304347826087 Time  1.04 s\n",
            "Epoch :  3  Batch :  24  Loss :   0.06219112186226994  Accuracy :  0.9895833333333334 Time  1.05 s\n",
            "Epoch :  3  Batch :  25  Loss :   0.061496323607861995  Accuracy :  0.99 Time  1.04 s\n",
            "Epoch :  3  Batch :  26  Loss :   0.059441473335027695  Accuracy :  0.9903846153846154 Time  1.06 s\n",
            "Epoch :  3  Batch :  27  Loss :   0.058344545808655245  Accuracy :  0.9907407407407407 Time  1.06 s\n",
            "Epoch :  3  Batch :  28  Loss :   0.0567349850732301  Accuracy :  0.9910714285714286 Time  1.05 s\n",
            "Epoch :  3  Batch :  29  Loss :   0.055152865447874726  Accuracy :  0.9913793103448276 Time  1.04 s\n",
            "Epoch :  3  Batch :  30  Loss :   0.05384962614625692  Accuracy :  0.9916666666666667 Time  1.05 s\n",
            "Epoch :  3  Batch :  31  Loss :   0.055987360076077526  Accuracy :  0.9903225806451613 Time  1.04 s\n",
            "Epoch :  3  Batch :  32  Loss :   0.05441647538100369  Accuracy :  0.990625 Time  1.05 s\n",
            "Epoch :  3  Batch :  33  Loss :   0.05291577120960662  Accuracy :  0.990909090909091 Time  1.05 s\n",
            "Epoch :  3  Batch :  34  Loss :   0.05184739348752534  Accuracy :  0.9911764705882353 Time  1.05 s\n",
            "Epoch :  3  Batch :  35  Loss :   0.05491053379539933  Accuracy :  0.99 Time  1.04 s\n",
            "Epoch :  3  Batch :  36  Loss :   0.053923441381711096  Accuracy :  0.9902777777777778 Time  1.05 s\n",
            "Epoch :  3  Batch :  37  Loss :   0.052802290238842774  Accuracy :  0.9905405405405405 Time  1.05 s\n",
            "Epoch :  3  Batch :  38  Loss :   0.05324668611252779  Accuracy :  0.9894736842105263 Time  1.06 s\n",
            "Epoch :  3  Batch :  39  Loss :   0.05253169196060835  Accuracy :  0.9897435897435898 Time  1.06 s\n",
            "Epoch :  3  Batch :  40  Loss :   0.055376505362801254  Accuracy :  0.98875 Time  1.05 s\n",
            "Epoch :  3  Batch :  41  Loss :   0.05466836434221122  Accuracy :  0.9890243902439024 Time  1.05 s\n",
            "Epoch :  3  Batch :  42  Loss :   0.05482849636159483  Accuracy :  0.9880952380952381 Time  1.05 s\n",
            "Epoch :  3  Batch :  43  Loss :   0.05684468026684467  Accuracy :  0.9872093023255814 Time  1.05 s\n",
            "Epoch :  3  Batch :  44  Loss :   0.05564445920754224  Accuracy :  0.9875 Time  1.05 s\n",
            "Epoch :  3  Batch :  45  Loss :   0.05483175643409292  Accuracy :  0.9877777777777778 Time  1.05 s\n",
            "Epoch :  3  Batch :  46  Loss :   0.05368446147211058  Accuracy :  0.9880434782608696 Time  1.05 s\n",
            "Epoch :  3  Batch :  47  Loss :   0.05271141791339726  Accuracy :  0.9882978723404255 Time  1.05 s\n",
            "Epoch :  3  Batch :  48  Loss :   0.05341303043436104  Accuracy :  0.9875 Time  1.05 s\n",
            "Epoch :  3  Batch :  49  Loss :   0.052408325471630206  Accuracy :  0.9877551020408163 Time  1.06 s\n",
            "Epoch :  3  Batch :  50  Loss :   0.051410625958815216  Accuracy :  0.988 Time  1.06 s\n",
            "Epoch :  3  Batch :  51  Loss :   0.050993177501083  Accuracy :  0.9882352941176471 Time  1.05 s\n",
            "Epoch :  3  Batch :  52  Loss :   0.05007667363119813  Accuracy :  0.9884615384615385 Time  1.05 s\n",
            "Epoch :  3  Batch :  53  Loss :   0.04959810827419443  Accuracy :  0.9886792452830189 Time  1.04 s\n",
            "Epoch :  3  Batch :  54  Loss :   0.04936246173801245  Accuracy :  0.9888888888888889 Time  1.05 s\n",
            "Epoch :  3  Batch :  55  Loss :   0.048621470769020646  Accuracy :  0.9890909090909091 Time  1.04 s\n",
            "Epoch :  3  Batch :  56  Loss :   0.0477879362650648  Accuracy :  0.9892857142857143 Time  1.05 s\n",
            "Epoch :  3  Batch :  57  Loss :   0.047123577472296334  Accuracy :  0.9894736842105263 Time  1.05 s\n",
            "Epoch :  3  Batch :  58  Loss :   0.0466958680290504  Accuracy :  0.9896551724137931 Time  1.04 s\n",
            "Epoch :  3  Batch :  59  Loss :   0.047265006097080974  Accuracy :  0.9889830508474576 Time  1.06 s\n",
            "Epoch :  3  Batch :  60  Loss :   0.04813442021647158  Accuracy :  0.9883333333333333 Time  1.06 s\n",
            "Epoch :  3  Batch :  61  Loss :   0.04961197752482639  Accuracy :  0.9877049180327869 Time  1.06 s\n",
            "Epoch :  3  Batch :  62  Loss :   0.04907448553187292  Accuracy :  0.9879032258064516 Time  1.05 s\n",
            "Epoch :  3  Batch :  63  Loss :   0.04866572634486984  Accuracy :  0.9880952380952381 Time  1.05 s\n",
            "Epoch :  3  Batch :  64  Loss :   0.04812005816529563  Accuracy :  0.98828125 Time  1.05 s\n",
            "Epoch :  3  Batch :  65  Loss :   0.04829920475335362  Accuracy :  0.9876923076923076 Time  1.05 s\n",
            "Epoch :  3  Batch :  66  Loss :   0.047651779289343256  Accuracy :  0.9878787878787879 Time  1.05 s\n",
            "Epoch :  3  Batch :  67  Loss :   0.047896560602960415  Accuracy :  0.9873134328358208 Time  1.04 s\n",
            "Epoch :  3  Batch :  68  Loss :   0.04722623747040737  Accuracy :  0.9875 Time  1.05 s\n",
            "Epoch :  3  Batch :  69  Loss :   0.046624019919165774  Accuracy :  0.9876811594202899 Time  1.05 s\n",
            "Epoch :  3  Batch :  70  Loss :   0.04606062700206946  Accuracy :  0.9878571428571429 Time  1.05 s\n",
            "Epoch :  3  Batch :  71  Loss :   0.04545607267793688  Accuracy :  0.9880281690140845 Time  1.06 s\n",
            "Epoch :  3  Batch :  72  Loss :   0.04550881846322833  Accuracy :  0.9881944444444445 Time  1.06 s\n",
            "Epoch :  3  Batch :  73  Loss :   0.04542578416972179  Accuracy :  0.9883561643835617 Time  1.04 s\n",
            "Epoch :  3  Batch :  74  Loss :   0.045282537540785865  Accuracy :  0.9885135135135135 Time  1.04 s\n",
            "Epoch :  3  Batch :  75  Loss :   0.04473800258866201  Accuracy :  0.9886666666666667 Time  1.05 s\n",
            "Epoch :  3  Batch :  76  Loss :   0.04431299180999447  Accuracy :  0.9888157894736842 Time  1.05 s\n",
            "Epoch :  3  Batch :  77  Loss :   0.044387936902038946  Accuracy :  0.9889610389610389 Time  1.04 s\n",
            "Epoch :  3  Batch :  78  Loss :   0.0439596401172308  Accuracy :  0.9891025641025641 Time  1.04 s\n",
            "Epoch :  3  Batch :  79  Loss :   0.043590561329104195  Accuracy :  0.989240506329114 Time  1.05 s\n",
            "Epoch :  3  Batch :  80  Loss :   0.04388998944341438  Accuracy :  0.98875 Time  1.05 s\n",
            "Epoch :  3  Batch :  81  Loss :   0.043407453214002335  Accuracy :  0.9888888888888889 Time  1.06 s\n",
            "Epoch :  3  Batch :  82  Loss :   0.04328132376625634  Accuracy :  0.9890243902439024 Time  1.06 s\n",
            "Epoch :  3  Batch :  83  Loss :   0.043381410023949324  Accuracy :  0.9885542168674699 Time  1.06 s\n",
            "Epoch :  3  Batch :  84  Loss :   0.04306850100207763  Accuracy :  0.9886904761904762 Time  1.05 s\n",
            "Epoch :  3  Batch :  85  Loss :   0.042705898678532854  Accuracy :  0.9888235294117647 Time  1.04 s\n",
            "Epoch :  3  Batch :  86  Loss :   0.042342767759691924  Accuracy :  0.9889534883720931 Time  1.05 s\n",
            "Epoch :  3  Batch :  87  Loss :   0.04503141149969493  Accuracy :  0.9879310344827587 Time  1.04 s\n",
            "Epoch :  3  Batch :  88  Loss :   0.04466389740991872  Accuracy :  0.9880681818181818 Time  1.04 s\n",
            "Epoch :  3  Batch :  89  Loss :   0.04431320842109579  Accuracy :  0.9882022471910112 Time  1.05 s\n",
            "Epoch :  3  Batch :  90  Loss :   0.044182619598642406  Accuracy :  0.9883333333333333 Time  1.05 s\n",
            "Epoch :  3  Batch :  91  Loss :   0.043975830263621944  Accuracy :  0.9884615384615385 Time  1.04 s\n",
            "Epoch :  3  Batch :  92  Loss :   0.043577535033641056  Accuracy :  0.9885869565217391 Time  1.05 s\n",
            "Epoch :  3  Batch :  93  Loss :   0.043188355448755926  Accuracy :  0.9887096774193549 Time  1.06 s\n",
            "Epoch :  3  Batch :  94  Loss :   0.043089828777196325  Accuracy :  0.9888297872340426 Time  1.07 s\n",
            "Epoch :  3  Batch :  95  Loss :   0.04344965339711818  Accuracy :  0.988421052631579 Time  1.05 s\n",
            "Epoch :  3  Batch :  96  Loss :   0.04303920788152027  Accuracy :  0.9885416666666667 Time  1.05 s\n",
            "Epoch :  3  Batch :  97  Loss :   0.043100845789988095  Accuracy :  0.9881443298969073 Time  1.05 s\n",
            "Epoch :  3  Batch :  98  Loss :   0.04300935348918738  Accuracy :  0.988265306122449 Time  1.04 s\n",
            "Epoch :  3  Batch :  99  Loss :   0.04280224674959865  Accuracy :  0.9883838383838384 Time  1.05 s\n",
            "Epoch :  3  Batch :  100  Loss :   0.04240080999792553  Accuracy :  0.9885 Time  1.05 s\n",
            "Epoch :  3  Batch :  101  Loss :   0.04301488505201506  Accuracy :  0.9881188118811881 Time  1.04 s\n",
            "Epoch :  3  Batch :  102  Loss :   0.042653272052834215  Accuracy :  0.9882352941176471 Time  1.06 s\n",
            "Epoch :  3  Batch :  103  Loss :   0.04282049474518891  Accuracy :  0.9878640776699029 Time  1.05 s\n",
            "Epoch :  3  Batch :  104  Loss :   0.04279905426553272  Accuracy :  0.9879807692307693 Time  1.06 s\n",
            "Epoch :  3  Batch :  105  Loss :   0.04247247077901626  Accuracy :  0.9880952380952381 Time  1.07 s\n",
            "Epoch :  3  Batch :  106  Loss :   0.04462314085740561  Accuracy :  0.9872641509433963 Time  1.04 s\n",
            "Epoch :  3  Batch :  107  Loss :   0.04660865843457074  Accuracy :  0.9864485981308411 Time  1.05 s\n",
            "Epoch :  3  Batch :  108  Loss :   0.04632718201955194  Accuracy :  0.986574074074074 Time  1.05 s\n",
            "Epoch :  3  Batch :  109  Loss :   0.04646789275404544  Accuracy :  0.9862385321100917 Time  1.04 s\n",
            "Epoch :  3  Batch :  110  Loss :   0.0461668662017804  Accuracy :  0.9863636363636363 Time  1.05 s\n",
            "Epoch :  3  Batch :  111  Loss :   0.04580484234690163  Accuracy :  0.9864864864864865 Time  1.04 s\n",
            "Epoch :  3  Batch :  112  Loss :   0.04563453538763237  Accuracy :  0.9866071428571429 Time  1.06 s\n",
            "Epoch :  3  Batch :  113  Loss :   0.04530623749380651  Accuracy :  0.9867256637168141 Time  1.05 s\n",
            "Epoch :  3  Batch :  114  Loss :   0.04497141546769917  Accuracy :  0.9868421052631579 Time  1.06 s\n",
            "Epoch :  3  Batch :  115  Loss :   0.04473549675540594  Accuracy :  0.9869565217391304 Time  1.07 s\n",
            "Epoch :  3  Batch :  116  Loss :   0.04443046216680347  Accuracy :  0.9870689655172413 Time  1.06 s\n",
            "Epoch :  3  Batch :  117  Loss :   0.04409490989493286  Accuracy :  0.9871794871794872 Time  1.05 s\n",
            "Epoch :  3  Batch :  118  Loss :   0.043767294334445025  Accuracy :  0.9872881355932204 Time  1.05 s\n",
            "Epoch :  3  Batch :  119  Loss :   0.043419983258525675  Accuracy :  0.9873949579831933 Time  1.04 s\n",
            "Epoch :  3  Batch :  120  Loss :   0.0431885649964291  Accuracy :  0.9875 Time  1.04 s\n",
            "Epoch :  3  Batch :  121  Loss :   0.04479270990772465  Accuracy :  0.9867768595041322 Time  1.04 s\n",
            "Epoch :  3  Batch :  122  Loss :   0.044494497815918056  Accuracy :  0.9868852459016394 Time  1.06 s\n",
            "Epoch :  3  Batch :  123  Loss :   0.04423380518669066  Accuracy :  0.9869918699186991 Time  1.05 s\n",
            "Epoch :  3  Batch :  124  Loss :   0.044502873421503955  Accuracy :  0.9866935483870968 Time  1.05 s\n",
            "Epoch :  3  Batch :  125  Loss :   0.044190569267608226  Accuracy :  0.9868 Time  1.06 s\n",
            "Epoch :  3  Batch :  126  Loss :   0.04397461702400404  Accuracy :  0.986904761904762 Time  1.06 s\n",
            "Epoch :  3  Batch :  127  Loss :   0.04388659526722315  Accuracy :  0.9870078740157481 Time  1.07 s\n",
            "Epoch :  3  Batch :  128  Loss :   0.04360086330143531  Accuracy :  0.987109375 Time  1.05 s\n",
            "Epoch :  3  Batch :  129  Loss :   0.043284594161659765  Accuracy :  0.9872093023255814 Time  1.04 s\n",
            "Epoch :  3  Batch :  130  Loss :   0.04296620901888953  Accuracy :  0.9873076923076923 Time  1.04 s\n",
            "Epoch :  3  Batch :  131  Loss :   0.04289791782436366  Accuracy :  0.9874045801526717 Time  1.05 s\n",
            "Epoch :  3  Batch :  132  Loss :   0.042691131457545314  Accuracy :  0.9875 Time  1.06 s\n",
            "Epoch :  3  Batch :  133  Loss :   0.04278079798108989  Accuracy :  0.987218045112782 Time  1.05 s\n",
            "Epoch :  3  Batch :  134  Loss :   0.042488305044096356  Accuracy :  0.9873134328358208 Time  1.05 s\n",
            "Epoch :  3  Batch :  135  Loss :   0.042338738431809124  Accuracy :  0.9874074074074074 Time  1.04 s\n",
            "Epoch :  3  Batch :  136  Loss :   0.04227441517115735  Accuracy :  0.9875 Time  1.06 s\n",
            "Epoch :  3  Batch :  137  Loss :   0.042035145209218465  Accuracy :  0.9875912408759124 Time  1.06 s\n",
            "Epoch :  3  Batch :  138  Loss :   0.04251439788419267  Accuracy :  0.9873188405797102 Time  1.06 s\n",
            "Epoch :  3  Batch :  139  Loss :   0.04223044102395074  Accuracy :  0.987410071942446 Time  1.05 s\n",
            "Epoch :  3  Batch :  140  Loss :   0.04200403936806002  Accuracy :  0.9875 Time  1.04 s\n",
            "Epoch :  3  Batch :  141  Loss :   0.04237831993266306  Accuracy :  0.9872340425531915 Time  1.05 s\n",
            "Epoch :  3  Batch :  142  Loss :   0.042147820175979546  Accuracy :  0.9873239436619718 Time  1.05 s\n",
            "Epoch :  3  Batch :  143  Loss :   0.04186440811824653  Accuracy :  0.9874125874125874 Time  1.04 s\n",
            "Epoch :  3  Batch :  144  Loss :   0.04225491898250766  Accuracy :  0.9871527777777778 Time  1.05 s\n",
            "Epoch :  3  Batch :  145  Loss :   0.04200690365878158  Accuracy :  0.9872413793103448 Time  1.05 s\n",
            "Epoch :  3  Batch :  146  Loss :   0.041829908422947135  Accuracy :  0.9873287671232877 Time  1.05 s\n",
            "Epoch :  3  Batch :  147  Loss :   0.041782115975736965  Accuracy :  0.9874149659863946 Time  1.06 s\n",
            "Epoch :  3  Batch :  148  Loss :   0.04152684956411454  Accuracy :  0.9875 Time  1.07 s\n",
            "Epoch :  3  Batch :  149  Loss :   0.04127856957469911  Accuracy :  0.9875838926174496 Time  1.06 s\n",
            "Epoch :  3  Batch :  150  Loss :   0.041128628014897306  Accuracy :  0.9876666666666667 Time  1.05 s\n",
            "Epoch :  3  Batch :  151  Loss :   0.04090108365646952  Accuracy :  0.9877483443708609 Time  1.05 s\n",
            "Epoch :  3  Batch :  152  Loss :   0.04065234052770967  Accuracy :  0.9878289473684211 Time  1.04 s\n",
            "Epoch :  3  Batch :  153  Loss :   0.04044186306076453  Accuracy :  0.9879084967320262 Time  1.05 s\n",
            "Epoch :  3  Batch :  154  Loss :   0.04026489504522269  Accuracy :  0.987987012987013 Time  1.05 s\n",
            "Epoch :  3  Batch :  155  Loss :   0.04049505911497099  Accuracy :  0.987741935483871 Time  1.05 s\n",
            "Epoch :  3  Batch :  156  Loss :   0.04028122620925737  Accuracy :  0.9878205128205129 Time  1.05 s\n",
            "Epoch :  3  Batch :  157  Loss :   0.04034840279543523  Accuracy :  0.9875796178343949 Time  1.05 s\n",
            "Epoch :  3  Batch :  158  Loss :   0.04013344440815522  Accuracy :  0.9876582278481013 Time  1.05 s\n",
            "Epoch :  3  Batch :  159  Loss :   0.04040344249685558  Accuracy :  0.9874213836477987 Time  1.05 s\n",
            "Epoch :  3  Batch :  160  Loss :   0.0401744569229777  Accuracy :  0.9875 Time  1.06 s\n",
            "Epoch :  3  Batch :  161  Loss :   0.039947221715072666  Accuracy :  0.9875776397515528 Time  1.05 s\n",
            "Epoch :  3  Batch :  162  Loss :   0.03982649461860642  Accuracy :  0.9876543209876543 Time  1.06 s\n",
            "Epoch :  3  Batch :  163  Loss :   0.03993419286983511  Accuracy :  0.9877300613496932 Time  1.05 s\n",
            "Epoch :  3  Batch :  164  Loss :   0.03971121592799247  Accuracy :  0.9878048780487805 Time  1.04 s\n",
            "Epoch :  3  Batch :  165  Loss :   0.03959247265293291  Accuracy :  0.9878787878787879 Time  1.05 s\n",
            "Epoch :  3  Batch :  166  Loss :   0.03941427075012351  Accuracy :  0.9879518072289156 Time  1.05 s\n",
            "Epoch :  3  Batch :  167  Loss :   0.03923320790061247  Accuracy :  0.9880239520958084 Time  1.05 s\n",
            "Epoch :  3  Batch :  168  Loss :   0.03903327895871674  Accuracy :  0.9880952380952381 Time  1.05 s\n",
            "Epoch :  3  Batch :  169  Loss :   0.03882419313001033  Accuracy :  0.9881656804733728 Time  1.05 s\n",
            "Epoch :  3  Batch :  170  Loss :   0.03867648402776788  Accuracy :  0.9882352941176471 Time  1.06 s\n",
            "Epoch :  3  Batch :  171  Loss :   0.03947182114056328  Accuracy :  0.987719298245614 Time  1.06 s\n",
            "Epoch :  3  Batch :  172  Loss :   0.039337415551376896  Accuracy :  0.9877906976744186 Time  1.05 s\n",
            "Epoch :  3  Batch :  173  Loss :   0.03914176305013068  Accuracy :  0.9878612716763006 Time  1.05 s\n",
            "Epoch :  3  Batch :  174  Loss :   0.038993016297490087  Accuracy :  0.9879310344827587 Time  1.05 s\n",
            "Epoch :  3  Batch :  175  Loss :   0.03888910006731749  Accuracy :  0.988 Time  1.05 s\n",
            "Epoch :  3  Batch :  176  Loss :   0.038706746825482696  Accuracy :  0.9880681818181818 Time  1.04 s\n",
            "Epoch :  3  Batch :  177  Loss :   0.03952380057718963  Accuracy :  0.9878531073446327 Time  1.04 s\n",
            "Epoch :  3  Batch :  178  Loss :   0.03939318104929636  Accuracy :  0.9879213483146068 Time  1.05 s\n",
            "Epoch :  3  Batch :  179  Loss :   0.03922076422367349  Accuracy :  0.9879888268156425 Time  1.04 s\n",
            "Epoch :  3  Batch :  180  Loss :   0.039152168968899385  Accuracy :  0.9880555555555556 Time  1.05 s\n",
            "Epoch :  3  Batch :  181  Loss :   0.038963193557539036  Accuracy :  0.9881215469613259 Time  1.06 s\n",
            "Epoch :  3  Batch :  182  Loss :   0.038791850672338854  Accuracy :  0.9881868131868132 Time  1.06 s\n",
            "Epoch :  3  Batch :  183  Loss :   0.03862910211605371  Accuracy :  0.9882513661202186 Time  1.05 s\n",
            "Epoch :  3  Batch :  184  Loss :   0.038626600246694266  Accuracy :  0.9883152173913043 Time  1.05 s\n",
            "Epoch :  3  Batch :  185  Loss :   0.0384458737624054  Accuracy :  0.9883783783783784 Time  1.05 s\n",
            "Epoch :  3  Batch :  186  Loss :   0.03830560465263183  Accuracy :  0.9884408602150537 Time  1.05 s\n",
            "Epoch :  3  Batch :  187  Loss :   0.03816058730487438  Accuracy :  0.9885026737967915 Time  1.04 s\n",
            "Epoch :  3  Batch :  188  Loss :   0.038154061792041866  Accuracy :  0.988563829787234 Time  1.04 s\n",
            "Epoch :  3  Batch :  189  Loss :   0.037984069636357684  Accuracy :  0.9886243386243386 Time  1.05 s\n",
            "Epoch :  3  Batch :  190  Loss :   0.03779686010981861  Accuracy :  0.9886842105263158 Time  1.05 s\n",
            "Epoch :  3  Batch :  191  Loss :   0.037644109498533904  Accuracy :  0.9887434554973822 Time  1.06 s\n",
            "Epoch :  3  Batch :  192  Loss :   0.03751512399079123  Accuracy :  0.9888020833333333 Time  1.07 s\n",
            "Epoch :  3  Batch :  193  Loss :   0.03823146323909877  Accuracy :  0.9886010362694301 Time  1.07 s\n",
            "Epoch :  3  Batch :  194  Loss :   0.03811134392702856  Accuracy :  0.988659793814433 Time  1.07 s\n",
            "Epoch :  3  Batch :  195  Loss :   0.0380258200976711  Accuracy :  0.9887179487179487 Time  1.06 s\n",
            "Epoch :  3  Batch :  196  Loss :   0.037848566311924736  Accuracy :  0.9887755102040816 Time  1.04 s\n",
            "Epoch :  3  Batch :  197  Loss :   0.037695133377830994  Accuracy :  0.9888324873096447 Time  1.04 s\n",
            "Epoch :  3  Batch :  198  Loss :   0.03758574334547074  Accuracy :  0.9888888888888889 Time  1.05 s\n",
            "Epoch :  3  Batch :  199  Loss :   0.03741274965955085  Accuracy :  0.9889447236180905 Time  1.04 s\n",
            "Epoch :  3  Batch :  200  Loss :   0.037235635046381506  Accuracy :  0.989 Time  1.05 s\n",
            "Epoch :  3  Batch :  201  Loss :   0.03731538882868281  Accuracy :  0.9888059701492538 Time  1.05 s\n",
            "Epoch :  3  Batch :  202  Loss :   0.03719245614991108  Accuracy :  0.9888613861386139 Time  1.05 s\n",
            "Epoch :  3  Batch :  203  Loss :   0.037113671390605826  Accuracy :  0.9889162561576355 Time  1.05 s\n",
            "Epoch :  3  Batch :  204  Loss :   0.03701799678132303  Accuracy :  0.9889705882352942 Time  1.06 s\n",
            "Epoch :  3  Batch :  205  Loss :   0.0377539937929591  Accuracy :  0.988780487804878 Time  1.06 s\n",
            "Epoch :  3  Batch :  206  Loss :   0.037835128996430815  Accuracy :  0.9885922330097088 Time  1.05 s\n",
            "Epoch :  3  Batch :  207  Loss :   0.03765618578240653  Accuracy :  0.9886473429951691 Time  1.05 s\n",
            "Epoch :  3  Batch :  208  Loss :   0.03749045431714666  Accuracy :  0.9887019230769231 Time  1.05 s\n",
            "Epoch :  3  Batch :  209  Loss :   0.03733431332261172  Accuracy :  0.988755980861244 Time  1.04 s\n",
            "Epoch :  3  Batch :  210  Loss :   0.03718672488383683  Accuracy :  0.9888095238095238 Time  1.05 s\n",
            "Epoch :  3  Batch :  211  Loss :   0.03709615373783149  Accuracy :  0.9888625592417062 Time  1.05 s\n",
            "Epoch :  3  Batch :  212  Loss :   0.036940606493823545  Accuracy :  0.9889150943396227 Time  1.05 s\n",
            "Epoch :  3  Batch :  213  Loss :   0.03708958710454177  Accuracy :  0.9887323943661972 Time  1.05 s\n",
            "Epoch :  3  Batch :  214  Loss :   0.03692094756401292  Accuracy :  0.9887850467289719 Time  1.05 s\n",
            "Epoch :  3  Batch :  215  Loss :   0.036795721070429444  Accuracy :  0.9888372093023255 Time  1.06 s\n",
            "Epoch :  3  Batch :  216  Loss :   0.036787954941111686  Accuracy :  0.9888888888888889 Time  1.07 s\n",
            "Epoch :  3  Batch :  217  Loss :   0.036629710530471656  Accuracy :  0.9889400921658986 Time  1.06 s\n",
            "Epoch :  3  Batch :  218  Loss :   0.03652459802907046  Accuracy :  0.9889908256880734 Time  1.04 s\n",
            "Epoch :  3  Batch :  219  Loss :   0.03726972370317044  Accuracy :  0.9888127853881279 Time  1.05 s\n",
            "Epoch :  3  Batch :  220  Loss :   0.03710364329586314  Accuracy :  0.9888636363636364 Time  1.04 s\n",
            "Epoch :  3  Batch :  221  Loss :   0.03695178964086787  Accuracy :  0.9889140271493213 Time  1.05 s\n",
            "Epoch :  3  Batch :  222  Loss :   0.036801423808257724  Accuracy :  0.9889639639639639 Time  1.04 s\n",
            "Epoch :  3  Batch :  223  Loss :   0.03674112071362512  Accuracy :  0.9890134529147983 Time  1.04 s\n",
            "Epoch :  3  Batch :  224  Loss :   0.036585365000064485  Accuracy :  0.9890625 Time  1.04 s\n",
            "Epoch :  3  Batch :  225  Loss :   0.03684832020973166  Accuracy :  0.9888888888888889 Time  1.05 s\n",
            "Epoch :  3  Batch :  226  Loss :   0.03670271697445913  Accuracy :  0.9889380530973452 Time  1.06 s\n",
            "Epoch :  3  Batch :  227  Loss :   0.03654503503261495  Accuracy :  0.9889867841409692 Time  1.06 s\n",
            "Epoch :  3  Batch :  228  Loss :   0.036393307295184696  Accuracy :  0.9890350877192983 Time  1.06 s\n",
            "Epoch :  3  Batch :  229  Loss :   0.03791295728767209  Accuracy :  0.9888646288209607 Time  1.05 s\n",
            "Epoch :  3  Batch :  230  Loss :   0.037772560634640164  Accuracy :  0.9889130434782609 Time  1.05 s\n",
            "Epoch :  3  Batch :  231  Loss :   0.03775397462047661  Accuracy :  0.9889610389610389 Time  1.05 s\n",
            "Epoch :  3  Batch :  232  Loss :   0.0376113649216332  Accuracy :  0.9890086206896552 Time  1.05 s\n",
            "Epoch :  3  Batch :  233  Loss :   0.03746207583422848  Accuracy :  0.9890557939914163 Time  1.05 s\n",
            "Epoch :  3  Batch :  234  Loss :   0.03730773626957049  Accuracy :  0.9891025641025641 Time  1.04 s\n",
            "Epoch :  3  Batch :  235  Loss :   0.03770337783746382  Accuracy :  0.9889361702127659 Time  1.05 s\n",
            "Epoch :  3  Batch :  236  Loss :   0.03767210018295219  Accuracy :  0.9889830508474576 Time  1.05 s\n",
            "Epoch :  3  Batch :  237  Loss :   0.03754838855955789  Accuracy :  0.9890295358649789 Time  1.06 s\n",
            "Epoch :  3  Batch :  238  Loss :   0.037397511925366354  Accuracy :  0.9890756302521009 Time  1.06 s\n",
            "Epoch :  3  Batch :  239  Loss :   0.0373146740062587  Accuracy :  0.9891213389121339 Time  1.06 s\n",
            "Epoch :  3  Batch :  240  Loss :   0.03748746325315248  Accuracy :  0.9891666666666666 Time  1.04 s\n",
            "Epoch :  3  Batch :  241  Loss :   0.037476114561361996  Accuracy :  0.9892116182572614 Time  1.05 s\n",
            "Epoch :  3  Batch :  242  Loss :   0.03746916769694899  Accuracy :  0.9892561983471074 Time  1.05 s\n",
            "Epoch :  3  Batch :  243  Loss :   0.03732777132203327  Accuracy :  0.9893004115226337 Time  1.05 s\n",
            "Epoch :  3  Batch :  244  Loss :   0.03718660358739368  Accuracy :  0.989344262295082 Time  1.05 s\n",
            "Epoch :  3  Batch :  245  Loss :   0.037055519347468736  Accuracy :  0.9893877551020408 Time  1.04 s\n",
            "Epoch :  3  Batch :  246  Loss :   0.036917142174159354  Accuracy :  0.989430894308943 Time  1.05 s\n",
            "Epoch :  3  Batch :  247  Loss :   0.03677901134780054  Accuracy :  0.9894736842105263 Time  1.05 s\n",
            "Epoch :  3  Batch :  248  Loss :   0.0366828252245473  Accuracy :  0.989516129032258 Time  1.06 s\n",
            "Epoch :  3  Batch :  249  Loss :   0.036589503797644515  Accuracy :  0.989558232931727 Time  1.06 s\n",
            "Epoch :  3  Batch :  250  Loss :   0.03648361079837196  Accuracy :  0.9896 Time  1.06 s\n",
            "Epoch :  3  Batch :  251  Loss :   0.03691950795566292  Accuracy :  0.9894422310756972 Time  1.04 s\n",
            "Epoch :  3  Batch :  252  Loss :   0.03718143002761136  Accuracy :  0.9892857142857143 Time  1.05 s\n",
            "Epoch :  3  Batch :  253  Loss :   0.037068792748340434  Accuracy :  0.9893280632411067 Time  1.05 s\n",
            "Epoch :  3  Batch :  254  Loss :   0.036937167350794956  Accuracy :  0.9893700787401575 Time  1.04 s\n",
            "Epoch :  3  Batch :  255  Loss :   0.03723682883519716  Accuracy :  0.9892156862745098 Time  1.05 s\n",
            "Epoch :  3  Batch :  256  Loss :   0.037341115264780456  Accuracy :  0.9890625 Time  1.05 s\n",
            "Epoch :  3  Batch :  257  Loss :   0.037210609740079365  Accuracy :  0.9891050583657588 Time  1.05 s\n",
            "Epoch :  3  Batch :  258  Loss :   0.03717742933031684  Accuracy :  0.9891472868217054 Time  1.04 s\n",
            "Epoch :  3  Batch :  259  Loss :   0.037043881467516575  Accuracy :  0.9891891891891892 Time  1.06 s\n",
            "Epoch :  3  Batch :  260  Loss :   0.036972519774840645  Accuracy :  0.9892307692307692 Time  1.06 s\n",
            "Epoch :  3  Batch :  261  Loss :   0.03688327762386601  Accuracy :  0.989272030651341 Time  1.07 s\n",
            "Epoch :  3  Batch :  262  Loss :   0.036770268999002806  Accuracy :  0.9893129770992366 Time  1.05 s\n",
            "Epoch :  3  Batch :  263  Loss :   0.03701810667113341  Accuracy :  0.9891634980988593 Time  1.04 s\n",
            "Epoch :  3  Batch :  264  Loss :   0.0368949203594556  Accuracy :  0.9892045454545455 Time  1.05 s\n",
            "Epoch :  3  Batch :  265  Loss :   0.036900048850341434  Accuracy :  0.9892452830188679 Time  1.05 s\n",
            "Epoch :  3  Batch :  266  Loss :   0.03691672404183782  Accuracy :  0.9892857142857143 Time  1.05 s\n",
            "Epoch :  3  Batch :  267  Loss :   0.03693157061215051  Accuracy :  0.9893258426966293 Time  1.04 s\n",
            "Epoch :  3  Batch :  268  Loss :   0.03817419524963966  Accuracy :  0.9889925373134328 Time  1.04 s\n",
            "Epoch :  3  Batch :  269  Loss :   0.03806305321458845  Accuracy :  0.9890334572490707 Time  1.04 s\n",
            "Epoch :  3  Batch :  270  Loss :   0.03793563079498536  Accuracy :  0.9890740740740741 Time  1.05 s\n",
            "Epoch :  3  Batch :  271  Loss :   0.03784145228629176  Accuracy :  0.9891143911439114 Time  1.06 s\n",
            "Epoch :  3  Batch :  272  Loss :   0.03771474368177758  Accuracy :  0.9891544117647059 Time  1.06 s\n",
            "Epoch :  3  Batch :  273  Loss :   0.03785777118909019  Accuracy :  0.989010989010989 Time  1.05 s\n",
            "Epoch :  3  Batch :  274  Loss :   0.03775143742635268  Accuracy :  0.9890510948905109 Time  1.05 s\n",
            "Epoch :  3  Batch :  275  Loss :   0.03767155151793056  Accuracy :  0.9890909090909091 Time  1.05 s\n",
            "Epoch :  3  Batch :  276  Loss :   0.03793410174325075  Accuracy :  0.9889492753623188 Time  1.04 s\n",
            "Epoch :  3  Batch :  277  Loss :   0.03785347407292877  Accuracy :  0.9889891696750902 Time  1.04 s\n",
            "Epoch :  3  Batch :  278  Loss :   0.03774997560125642  Accuracy :  0.9890287769784173 Time  1.05 s\n",
            "Epoch :  3  Batch :  279  Loss :   0.0376545797370451  Accuracy :  0.989068100358423 Time  1.05 s\n",
            "Epoch :  3  Batch :  280  Loss :   0.03755624340493731  Accuracy :  0.9891071428571429 Time  1.05 s\n",
            "Epoch :  3  Batch :  281  Loss :   0.038234461864309045  Accuracy :  0.9889679715302491 Time  1.06 s\n",
            "Epoch :  3  Batch :  282  Loss :   0.03810602270627348  Accuracy :  0.9890070921985815 Time  1.06 s\n",
            "Epoch :  3  Batch :  283  Loss :   0.03797916376896617  Accuracy :  0.9890459363957598 Time  1.06 s\n",
            "Epoch :  3  Batch :  284  Loss :   0.03785072023218701  Accuracy :  0.9890845070422535 Time  1.05 s\n",
            "Epoch :  3  Batch :  285  Loss :   0.037727433696936555  Accuracy :  0.9891228070175438 Time  1.05 s\n",
            "Epoch :  3  Batch :  286  Loss :   0.037612250611028096  Accuracy :  0.9891608391608392 Time  1.05 s\n",
            "Epoch :  3  Batch :  287  Loss :   0.03751179969440399  Accuracy :  0.989198606271777 Time  1.05 s\n",
            "Epoch :  3  Batch :  288  Loss :   0.03781576277651766  Accuracy :  0.9888888888888889 Time  1.05 s\n",
            "Epoch :  3  Batch :  289  Loss :   0.038981535947909034  Accuracy :  0.9885813148788928 Time  1.05 s\n",
            "Epoch :  3  Batch :  290  Loss :   0.03910480469429127  Accuracy :  0.9886206896551725 Time  1.04 s\n",
            "Epoch :  3  Batch :  291  Loss :   0.038984502503077126  Accuracy :  0.988659793814433 Time  1.05 s\n",
            "Epoch :  3  Batch :  292  Loss :   0.03889456646551206  Accuracy :  0.9886986301369863 Time  1.06 s\n",
            "Epoch :  3  Batch :  293  Loss :   0.03878262111620736  Accuracy :  0.9887372013651877 Time  1.06 s\n",
            "Epoch :  3  Batch :  294  Loss :   0.03865297001625766  Accuracy :  0.9887755102040816 Time  1.06 s\n",
            "Epoch :  3  Batch :  295  Loss :   0.038539251032624774  Accuracy :  0.9888135593220339 Time  1.04 s\n",
            "Epoch :  3  Batch :  296  Loss :   0.03849485604429926  Accuracy :  0.9888513513513514 Time  1.06 s\n",
            "Epoch :  3  Batch :  297  Loss :   0.038441182874019264  Accuracy :  0.9888888888888889 Time  1.05 s\n",
            "Epoch :  3  Batch :  298  Loss :   0.03835825268714376  Accuracy :  0.9889261744966443 Time  1.04 s\n",
            "Epoch :  3  Batch :  299  Loss :   0.03829055567085899  Accuracy :  0.9889632107023412 Time  1.04 s\n",
            "Epoch :  3  Batch :  300  Loss :   0.03846929317747708  Accuracy :  0.9888333333333333 Time  1.05 s\n",
            "Epoch :  3  Batch :  301  Loss :   0.038605343201777824  Accuracy :  0.988704318936877 Time  1.04 s\n",
            "Epoch :  3  Batch :  302  Loss :   0.03853078367013565  Accuracy :  0.9887417218543046 Time  1.05 s\n",
            "Epoch :  3  Batch :  303  Loss :   0.038422932577274874  Accuracy :  0.9887788778877887 Time  1.05 s\n",
            "Epoch :  3  Batch :  304  Loss :   0.03830946943554935  Accuracy :  0.9888157894736842 Time  1.06 s\n",
            "Epoch :  3  Batch :  305  Loss :   0.038560104232136405  Accuracy :  0.988688524590164 Time  1.07 s\n",
            "Epoch :  3  Batch :  306  Loss :   0.038453845614057604  Accuracy :  0.9887254901960785 Time  1.05 s\n",
            "Epoch :  3  Batch :  307  Loss :   0.03842349513907877  Accuracy :  0.9887438825448613 Time  0.54 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 98 %\n",
            "Accuracy of     2 : 97 %\n",
            "[3 epoch] Accuracy of the network on the Training images: 98 %\n",
            "Epoch :  4  Batch :  1  Loss :   0.007860247977077961  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  4  Batch :  2  Loss :   0.0046791956992819905  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  4  Batch :  3  Loss :   0.009678434037292996  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  4  Batch :  4  Loss :   0.008720425481442362  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  4  Batch :  5  Loss :   0.01122174602933228  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  4  Batch :  6  Loss :   0.014604512951336801  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  4  Batch :  7  Loss :   0.012853576635409678  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  4  Batch :  8  Loss :   0.013395702670095488  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  4  Batch :  9  Loss :   0.012454948051729135  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  4  Batch :  10  Loss :   0.01203383372630924  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  4  Batch :  11  Loss :   0.011064579752697186  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  4  Batch :  12  Loss :   0.010509883519262075  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  4  Batch :  13  Loss :   0.012152561344779454  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  4  Batch :  14  Loss :   0.011489927569138152  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  4  Batch :  15  Loss :   0.011562452216943105  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  4  Batch :  16  Loss :   0.010923913549049757  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  4  Batch :  17  Loss :   0.010973228018402177  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  4  Batch :  18  Loss :   0.01045475598786854  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  4  Batch :  19  Loss :   0.01384768114229174  Accuracy :  0.9973684210526316 Time  1.07 s\n",
            "Epoch :  4  Batch :  20  Loss :   0.013714687002357095  Accuracy :  0.9975 Time  1.06 s\n",
            "Epoch :  4  Batch :  21  Loss :   0.013483356806405243  Accuracy :  0.9976190476190476 Time  1.04 s\n",
            "Epoch :  4  Batch :  22  Loss :   0.013147151870229705  Accuracy :  0.9977272727272727 Time  1.04 s\n",
            "Epoch :  4  Batch :  23  Loss :   0.012752023697628276  Accuracy :  0.9978260869565218 Time  1.05 s\n",
            "Epoch :  4  Batch :  24  Loss :   0.016149927212002996  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  4  Batch :  25  Loss :   0.016650184290483594  Accuracy :  0.996 Time  1.05 s\n",
            "Epoch :  4  Batch :  26  Loss :   0.01609742139072086  Accuracy :  0.9961538461538462 Time  1.04 s\n",
            "Epoch :  4  Batch :  27  Loss :   0.015559593991686901  Accuracy :  0.9962962962962963 Time  1.05 s\n",
            "Epoch :  4  Batch :  28  Loss :   0.017017835501714478  Accuracy :  0.9946428571428572 Time  1.04 s\n",
            "Epoch :  4  Batch :  29  Loss :   0.016560527929586583  Accuracy :  0.9948275862068966 Time  1.04 s\n",
            "Epoch :  4  Batch :  30  Loss :   0.0165009419588993  Accuracy :  0.995 Time  1.07 s\n",
            "Epoch :  4  Batch :  31  Loss :   0.016211206724326455  Accuracy :  0.9951612903225806 Time  1.06 s\n",
            "Epoch :  4  Batch :  32  Loss :   0.015926980355288833  Accuracy :  0.9953125 Time  1.04 s\n",
            "Epoch :  4  Batch :  33  Loss :   0.01560234415316672  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  4  Batch :  34  Loss :   0.015184019702514085  Accuracy :  0.9955882352941177 Time  1.04 s\n",
            "Epoch :  4  Batch :  35  Loss :   0.01630353699770889  Accuracy :  0.9942857142857143 Time  1.05 s\n",
            "Epoch :  4  Batch :  36  Loss :   0.01742865948148796  Accuracy :  0.9930555555555556 Time  1.04 s\n",
            "Epoch :  4  Batch :  37  Loss :   0.016985384624677937  Accuracy :  0.9932432432432432 Time  1.05 s\n",
            "Epoch :  4  Batch :  38  Loss :   0.016940151736458863  Accuracy :  0.993421052631579 Time  1.05 s\n",
            "Epoch :  4  Batch :  39  Loss :   0.016710529703861818  Accuracy :  0.9935897435897436 Time  1.05 s\n",
            "Epoch :  4  Batch :  40  Loss :   0.017978030050289816  Accuracy :  0.9925 Time  1.05 s\n",
            "Epoch :  4  Batch :  41  Loss :   0.01758407267298913  Accuracy :  0.9926829268292683 Time  1.05 s\n",
            "Epoch :  4  Batch :  42  Loss :   0.017200194509877337  Accuracy :  0.9928571428571429 Time  1.06 s\n",
            "Epoch :  4  Batch :  43  Loss :   0.017267133380004834  Accuracy :  0.9930232558139535 Time  1.07 s\n",
            "Epoch :  4  Batch :  44  Loss :   0.01696984542914751  Accuracy :  0.9931818181818182 Time  1.05 s\n",
            "Epoch :  4  Batch :  45  Loss :   0.01691007116395566  Accuracy :  0.9933333333333333 Time  1.05 s\n",
            "Epoch :  4  Batch :  46  Loss :   0.016571387792836227  Accuracy :  0.9934782608695653 Time  1.05 s\n",
            "Epoch :  4  Batch :  47  Loss :   0.016309287005223015  Accuracy :  0.9936170212765958 Time  1.05 s\n",
            "Epoch :  4  Batch :  48  Loss :   0.015998703621638317  Accuracy :  0.99375 Time  1.05 s\n",
            "Epoch :  4  Batch :  49  Loss :   0.016040461061864485  Accuracy :  0.9938775510204082 Time  1.05 s\n",
            "Epoch :  4  Batch :  50  Loss :   0.01666951272636652  Accuracy :  0.993 Time  1.05 s\n",
            "Epoch :  4  Batch :  51  Loss :   0.01678214547242604  Accuracy :  0.9931372549019608 Time  1.05 s\n",
            "Epoch :  4  Batch :  52  Loss :   0.017124467421896182  Accuracy :  0.9932692307692308 Time  1.06 s\n",
            "Epoch :  4  Batch :  53  Loss :   0.016873196004626324  Accuracy :  0.9933962264150943 Time  1.06 s\n",
            "Epoch :  4  Batch :  54  Loss :   0.01669527454888103  Accuracy :  0.9935185185185185 Time  1.06 s\n",
            "Epoch :  4  Batch :  55  Loss :   0.01666235143996098  Accuracy :  0.9936363636363637 Time  1.04 s\n",
            "Epoch :  4  Batch :  56  Loss :   0.016542274396800036  Accuracy :  0.99375 Time  1.04 s\n",
            "Epoch :  4  Batch :  57  Loss :   0.01635652539532697  Accuracy :  0.993859649122807 Time  1.05 s\n",
            "Epoch :  4  Batch :  58  Loss :   0.01615729447100954  Accuracy :  0.9939655172413793 Time  1.06 s\n",
            "Epoch :  4  Batch :  59  Loss :   0.015904763825948083  Accuracy :  0.9940677966101695 Time  1.05 s\n",
            "Epoch :  4  Batch :  60  Loss :   0.015673929586773737  Accuracy :  0.9941666666666666 Time  1.04 s\n",
            "Epoch :  4  Batch :  61  Loss :   0.015431761838014802  Accuracy :  0.9942622950819672 Time  1.05 s\n",
            "Epoch :  4  Batch :  62  Loss :   0.01535193380738248  Accuracy :  0.9943548387096774 Time  1.04 s\n",
            "Epoch :  4  Batch :  63  Loss :   0.015124823172791077  Accuracy :  0.9944444444444445 Time  1.06 s\n",
            "Epoch :  4  Batch :  64  Loss :   0.01490295617168158  Accuracy :  0.99453125 Time  1.06 s\n",
            "Epoch :  4  Batch :  65  Loss :   0.016879852774302262  Accuracy :  0.9938461538461538 Time  1.06 s\n",
            "Epoch :  4  Batch :  66  Loss :   0.016674463135352584  Accuracy :  0.9939393939393939 Time  1.05 s\n",
            "Epoch :  4  Batch :  67  Loss :   0.016528914737299696  Accuracy :  0.9940298507462687 Time  1.04 s\n",
            "Epoch :  4  Batch :  68  Loss :   0.016299995170865098  Accuracy :  0.9941176470588236 Time  1.04 s\n",
            "Epoch :  4  Batch :  69  Loss :   0.016095979566064973  Accuracy :  0.9942028985507246 Time  1.05 s\n",
            "Epoch :  4  Batch :  70  Loss :   0.016456802051314817  Accuracy :  0.9942857142857143 Time  1.05 s\n",
            "Epoch :  4  Batch :  71  Loss :   0.016259933889082725  Accuracy :  0.9943661971830986 Time  1.05 s\n",
            "Epoch :  4  Batch :  72  Loss :   0.016082478911913414  Accuracy :  0.9944444444444445 Time  1.05 s\n",
            "Epoch :  4  Batch :  73  Loss :   0.01592502769637751  Accuracy :  0.9945205479452055 Time  1.05 s\n",
            "Epoch :  4  Batch :  74  Loss :   0.01590721937041176  Accuracy :  0.9945945945945946 Time  1.05 s\n",
            "Epoch :  4  Batch :  75  Loss :   0.015927261646526556  Accuracy :  0.9946666666666667 Time  1.07 s\n",
            "Epoch :  4  Batch :  76  Loss :   0.015732586203069474  Accuracy :  0.9947368421052631 Time  1.06 s\n",
            "Epoch :  4  Batch :  77  Loss :   0.015580250243595863  Accuracy :  0.9948051948051948 Time  1.05 s\n",
            "Epoch :  4  Batch :  78  Loss :   0.015464782676038643  Accuracy :  0.9948717948717949 Time  1.04 s\n",
            "Epoch :  4  Batch :  79  Loss :   0.015565668064747242  Accuracy :  0.9949367088607595 Time  1.05 s\n",
            "Epoch :  4  Batch :  80  Loss :   0.015383721150283237  Accuracy :  0.995 Time  1.05 s\n",
            "Epoch :  4  Batch :  81  Loss :   0.0152494893366195  Accuracy :  0.9950617283950617 Time  1.04 s\n",
            "Epoch :  4  Batch :  82  Loss :   0.01510796167393692  Accuracy :  0.9951219512195122 Time  1.05 s\n",
            "Epoch :  4  Batch :  83  Loss :   0.01494164265557585  Accuracy :  0.9951807228915662 Time  1.05 s\n",
            "Epoch :  4  Batch :  84  Loss :   0.014961333490819448  Accuracy :  0.9952380952380953 Time  1.05 s\n",
            "Epoch :  4  Batch :  85  Loss :   0.014823522347518627  Accuracy :  0.9952941176470588 Time  1.06 s\n",
            "Epoch :  4  Batch :  86  Loss :   0.014693123448185276  Accuracy :  0.9953488372093023 Time  1.06 s\n",
            "Epoch :  4  Batch :  87  Loss :   0.014564024913392362  Accuracy :  0.9954022988505747 Time  1.07 s\n",
            "Epoch :  4  Batch :  88  Loss :   0.014408971097013406  Accuracy :  0.9954545454545455 Time  1.07 s\n",
            "Epoch :  4  Batch :  89  Loss :   0.014461601821423163  Accuracy :  0.9955056179775281 Time  1.06 s\n",
            "Epoch :  4  Batch :  90  Loss :   0.01602847619473727  Accuracy :  0.9944444444444445 Time  1.04 s\n",
            "Epoch :  4  Batch :  91  Loss :   0.015916969083444022  Accuracy :  0.9945054945054945 Time  1.05 s\n",
            "Epoch :  4  Batch :  92  Loss :   0.01590099583415355  Accuracy :  0.9945652173913043 Time  1.05 s\n",
            "Epoch :  4  Batch :  93  Loss :   0.015753636368229906  Accuracy :  0.9946236559139785 Time  1.04 s\n",
            "Epoch :  4  Batch :  94  Loss :   0.015600204194073585  Accuracy :  0.9946808510638298 Time  1.05 s\n",
            "Epoch :  4  Batch :  95  Loss :   0.01545181149298227  Accuracy :  0.9947368421052631 Time  1.05 s\n",
            "Epoch :  4  Batch :  96  Loss :   0.015358729019984215  Accuracy :  0.9947916666666666 Time  1.05 s\n",
            "Epoch :  4  Batch :  97  Loss :   0.01521339485560995  Accuracy :  0.9948453608247423 Time  1.05 s\n",
            "Epoch :  4  Batch :  98  Loss :   0.015067217320355833  Accuracy :  0.9948979591836735 Time  1.06 s\n",
            "Epoch :  4  Batch :  99  Loss :   0.015088152516670901  Accuracy :  0.9949494949494949 Time  1.06 s\n",
            "Epoch :  4  Batch :  100  Loss :   0.016564247272908686  Accuracy :  0.9945 Time  1.06 s\n",
            "Epoch :  4  Batch :  101  Loss :   0.01641233238889513  Accuracy :  0.9945544554455445 Time  1.04 s\n",
            "Epoch :  4  Batch :  102  Loss :   0.016282881694553674  Accuracy :  0.9946078431372549 Time  1.05 s\n",
            "Epoch :  4  Batch :  103  Loss :   0.016149606842782747  Accuracy :  0.9946601941747573 Time  1.05 s\n",
            "Epoch :  4  Batch :  104  Loss :   0.016014134764331035  Accuracy :  0.9947115384615385 Time  1.05 s\n",
            "Epoch :  4  Batch :  105  Loss :   0.015877318391132923  Accuracy :  0.9947619047619047 Time  1.05 s\n",
            "Epoch :  4  Batch :  106  Loss :   0.015794824773691735  Accuracy :  0.994811320754717 Time  1.04 s\n",
            "Epoch :  4  Batch :  107  Loss :   0.01566116729964476  Accuracy :  0.9948598130841122 Time  1.05 s\n",
            "Epoch :  4  Batch :  108  Loss :   0.015545519143213622  Accuracy :  0.9949074074074075 Time  1.04 s\n",
            "Epoch :  4  Batch :  109  Loss :   0.01541761267255274  Accuracy :  0.994954128440367 Time  1.06 s\n",
            "Epoch :  4  Batch :  110  Loss :   0.015400559338741005  Accuracy :  0.995 Time  1.06 s\n",
            "Epoch :  4  Batch :  111  Loss :   0.015536683420692612  Accuracy :  0.9950450450450451 Time  1.07 s\n",
            "Epoch :  4  Batch :  112  Loss :   0.015452897336217575  Accuracy :  0.9950892857142857 Time  1.06 s\n",
            "Epoch :  4  Batch :  113  Loss :   0.015362028121140547  Accuracy :  0.9951327433628319 Time  1.05 s\n",
            "Epoch :  4  Batch :  114  Loss :   0.015241314197498324  Accuracy :  0.9951754385964913 Time  1.04 s\n",
            "Epoch :  4  Batch :  115  Loss :   0.015120340369479813  Accuracy :  0.9952173913043478 Time  1.05 s\n",
            "Epoch :  4  Batch :  116  Loss :   0.015125064151201012  Accuracy :  0.9952586206896552 Time  1.05 s\n",
            "Epoch :  4  Batch :  117  Loss :   0.01505494588572118  Accuracy :  0.9952991452991453 Time  1.05 s\n",
            "Epoch :  4  Batch :  118  Loss :   0.014978397357419638  Accuracy :  0.9953389830508474 Time  1.04 s\n",
            "Epoch :  4  Batch :  119  Loss :   0.014883747353695896  Accuracy :  0.9953781512605042 Time  1.05 s\n",
            "Epoch :  4  Batch :  120  Loss :   0.01490739532552349  Accuracy :  0.9954166666666666 Time  1.05 s\n",
            "Epoch :  4  Batch :  121  Loss :   0.01537781958237352  Accuracy :  0.9950413223140496 Time  1.06 s\n",
            "Epoch :  4  Batch :  122  Loss :   0.015267049537811305  Accuracy :  0.9950819672131147 Time  1.07 s\n",
            "Epoch :  4  Batch :  123  Loss :   0.016421544459284988  Accuracy :  0.9943089430894309 Time  1.05 s\n",
            "Epoch :  4  Batch :  124  Loss :   0.016614897817846447  Accuracy :  0.9943548387096774 Time  1.05 s\n",
            "Epoch :  4  Batch :  125  Loss :   0.016494523557834328  Accuracy :  0.9944 Time  1.05 s\n",
            "Epoch :  4  Batch :  126  Loss :   0.016371572946405247  Accuracy :  0.9944444444444445 Time  1.05 s\n",
            "Epoch :  4  Batch :  127  Loss :   0.016270768220030415  Accuracy :  0.9944881889763779 Time  1.05 s\n",
            "Epoch :  4  Batch :  128  Loss :   0.017417314928025007  Accuracy :  0.994140625 Time  1.05 s\n",
            "Epoch :  4  Batch :  129  Loss :   0.017665117834658586  Accuracy :  0.993798449612403 Time  1.04 s\n",
            "Epoch :  4  Batch :  130  Loss :   0.01773078557676994  Accuracy :  0.9938461538461538 Time  1.04 s\n",
            "Epoch :  4  Batch :  131  Loss :   0.01811440125505888  Accuracy :  0.9935114503816794 Time  1.06 s\n",
            "Epoch :  4  Batch :  132  Loss :   0.018035484019271804  Accuracy :  0.993560606060606 Time  1.06 s\n",
            "Epoch :  4  Batch :  133  Loss :   0.01851611320433536  Accuracy :  0.9932330827067669 Time  1.06 s\n",
            "Epoch :  4  Batch :  134  Loss :   0.018399108343743788  Accuracy :  0.9932835820895523 Time  1.05 s\n",
            "Epoch :  4  Batch :  135  Loss :   0.018349377724721477  Accuracy :  0.9933333333333333 Time  1.04 s\n",
            "Epoch :  4  Batch :  136  Loss :   0.01824899189352222  Accuracy :  0.9933823529411765 Time  1.04 s\n",
            "Epoch :  4  Batch :  137  Loss :   0.018192692717578073  Accuracy :  0.9934306569343065 Time  1.05 s\n",
            "Epoch :  4  Batch :  138  Loss :   0.018089112596235413  Accuracy :  0.9934782608695653 Time  1.05 s\n",
            "Epoch :  4  Batch :  139  Loss :   0.017993733092552894  Accuracy :  0.9935251798561151 Time  1.05 s\n",
            "Epoch :  4  Batch :  140  Loss :   0.0179256646029119  Accuracy :  0.9935714285714285 Time  1.05 s\n",
            "Epoch :  4  Batch :  141  Loss :   0.019190163298754406  Accuracy :  0.9932624113475177 Time  1.04 s\n",
            "Epoch :  4  Batch :  142  Loss :   0.019485080306431358  Accuracy :  0.9929577464788732 Time  1.05 s\n",
            "Epoch :  4  Batch :  143  Loss :   0.019377872282704274  Accuracy :  0.993006993006993 Time  1.06 s\n",
            "Epoch :  4  Batch :  144  Loss :   0.01946189659388943  Accuracy :  0.9930555555555556 Time  1.07 s\n",
            "Epoch :  4  Batch :  145  Loss :   0.019439363132776884  Accuracy :  0.993103448275862 Time  1.05 s\n",
            "Epoch :  4  Batch :  146  Loss :   0.01937580532840874  Accuracy :  0.9931506849315068 Time  1.04 s\n",
            "Epoch :  4  Batch :  147  Loss :   0.01928961568125555  Accuracy :  0.9931972789115646 Time  1.04 s\n",
            "Epoch :  4  Batch :  148  Loss :   0.01919900477750579  Accuracy :  0.9932432432432432 Time  1.04 s\n",
            "Epoch :  4  Batch :  149  Loss :   0.019428205698253525  Accuracy :  0.9932885906040269 Time  1.05 s\n",
            "Epoch :  4  Batch :  150  Loss :   0.021657273697977264  Accuracy :  0.992 Time  1.05 s\n",
            "Epoch :  4  Batch :  151  Loss :   0.02155512138670742  Accuracy :  0.9920529801324504 Time  1.05 s\n",
            "Epoch :  4  Batch :  152  Loss :   0.021536444660006582  Accuracy :  0.9921052631578947 Time  1.06 s\n",
            "Epoch :  4  Batch :  153  Loss :   0.021474638889890676  Accuracy :  0.9921568627450981 Time  1.06 s\n",
            "Epoch :  4  Batch :  154  Loss :   0.021375791765386603  Accuracy :  0.9922077922077922 Time  1.06 s\n",
            "Epoch :  4  Batch :  155  Loss :   0.021304592517234624  Accuracy :  0.9922580645161291 Time  1.06 s\n",
            "Epoch :  4  Batch :  156  Loss :   0.02121638989326759  Accuracy :  0.9923076923076923 Time  1.05 s\n",
            "Epoch :  4  Batch :  157  Loss :   0.021150209854956074  Accuracy :  0.9923566878980892 Time  1.04 s\n",
            "Epoch :  4  Batch :  158  Loss :   0.021250436875285415  Accuracy :  0.9924050632911392 Time  1.04 s\n",
            "Epoch :  4  Batch :  159  Loss :   0.021741205524746916  Accuracy :  0.9921383647798742 Time  1.04 s\n",
            "Epoch :  4  Batch :  160  Loss :   0.02165853744081687  Accuracy :  0.9921875 Time  1.05 s\n",
            "Epoch :  4  Batch :  161  Loss :   0.022684072482729366  Accuracy :  0.9919254658385093 Time  1.04 s\n",
            "Epoch :  4  Batch :  162  Loss :   0.023202730684805616  Accuracy :  0.9916666666666667 Time  1.06 s\n",
            "Epoch :  4  Batch :  163  Loss :   0.023761327278463203  Accuracy :  0.9914110429447853 Time  1.04 s\n",
            "Epoch :  4  Batch :  164  Loss :   0.023838191108619112  Accuracy :  0.9914634146341463 Time  1.05 s\n",
            "Epoch :  4  Batch :  165  Loss :   0.02396755719444517  Accuracy :  0.9915151515151515 Time  1.06 s\n",
            "Epoch :  4  Batch :  166  Loss :   0.023831907406600798  Accuracy :  0.9915662650602409 Time  1.06 s\n",
            "Epoch :  4  Batch :  167  Loss :   0.024730693498029636  Accuracy :  0.9913173652694611 Time  1.05 s\n",
            "Epoch :  4  Batch :  168  Loss :   0.024611524915339293  Accuracy :  0.9913690476190476 Time  1.05 s\n",
            "Epoch :  4  Batch :  169  Loss :   0.024553251314285547  Accuracy :  0.9914201183431952 Time  1.04 s\n",
            "Epoch :  4  Batch :  170  Loss :   0.02458782699986306  Accuracy :  0.9914705882352941 Time  1.05 s\n",
            "Epoch :  4  Batch :  171  Loss :   0.024494639989288185  Accuracy :  0.9915204678362574 Time  1.05 s\n",
            "Epoch :  4  Batch :  172  Loss :   0.024386901242609765  Accuracy :  0.9915697674418604 Time  1.05 s\n",
            "Epoch :  4  Batch :  173  Loss :   0.024304099996692658  Accuracy :  0.9916184971098266 Time  1.04 s\n",
            "Epoch :  4  Batch :  174  Loss :   0.02417363231297818  Accuracy :  0.9916666666666667 Time  1.05 s\n",
            "Epoch :  4  Batch :  175  Loss :   0.02406520807383848  Accuracy :  0.9917142857142857 Time  1.05 s\n",
            "Epoch :  4  Batch :  176  Loss :   0.024476460668665823  Accuracy :  0.9914772727272727 Time  1.06 s\n",
            "Epoch :  4  Batch :  177  Loss :   0.02445866519093282  Accuracy :  0.9915254237288136 Time  1.07 s\n",
            "Epoch :  4  Batch :  178  Loss :   0.02435888574796132  Accuracy :  0.9915730337078652 Time  1.04 s\n",
            "Epoch :  4  Batch :  179  Loss :   0.024235101336712153  Accuracy :  0.9916201117318436 Time  1.04 s\n",
            "Epoch :  4  Batch :  180  Loss :   0.0241711653128732  Accuracy :  0.9916666666666667 Time  1.04 s\n",
            "Epoch :  4  Batch :  181  Loss :   0.024050918913919746  Accuracy :  0.9917127071823204 Time  1.04 s\n",
            "Epoch :  4  Batch :  182  Loss :   0.02430352459822853  Accuracy :  0.9914835164835165 Time  1.05 s\n",
            "Epoch :  4  Batch :  183  Loss :   0.024181034260103734  Accuracy :  0.9915300546448087 Time  1.05 s\n",
            "Epoch :  4  Batch :  184  Loss :   0.024218811123991265  Accuracy :  0.9915760869565218 Time  1.04 s\n",
            "Epoch :  4  Batch :  185  Loss :   0.024241071422836064  Accuracy :  0.9916216216216216 Time  1.04 s\n",
            "Epoch :  4  Batch :  186  Loss :   0.02412778415945008  Accuracy :  0.9916666666666667 Time  1.06 s\n",
            "Epoch :  4  Batch :  187  Loss :   0.024048722298681376  Accuracy :  0.991711229946524 Time  1.06 s\n",
            "Epoch :  4  Batch :  188  Loss :   0.02396478060511575  Accuracy :  0.9917553191489362 Time  1.06 s\n",
            "Epoch :  4  Batch :  189  Loss :   0.02385021982992945  Accuracy :  0.9917989417989418 Time  1.05 s\n",
            "Epoch :  4  Batch :  190  Loss :   0.02374000080040117  Accuracy :  0.9918421052631579 Time  1.05 s\n",
            "Epoch :  4  Batch :  191  Loss :   0.023760209984575148  Accuracy :  0.9918848167539267 Time  1.05 s\n",
            "Epoch :  4  Batch :  192  Loss :   0.02373683809916353  Accuracy :  0.9919270833333333 Time  1.04 s\n",
            "Epoch :  4  Batch :  193  Loss :   0.0237013539956844  Accuracy :  0.9919689119170985 Time  1.04 s\n",
            "Epoch :  4  Batch :  194  Loss :   0.0245119087312841  Accuracy :  0.9917525773195877 Time  1.05 s\n",
            "Epoch :  4  Batch :  195  Loss :   0.0248271341346061  Accuracy :  0.9915384615384616 Time  1.04 s\n",
            "Epoch :  4  Batch :  196  Loss :   0.024849715978216037  Accuracy :  0.9915816326530612 Time  1.05 s\n",
            "Epoch :  4  Batch :  197  Loss :   0.024983500610909374  Accuracy :  0.9913705583756345 Time  1.05 s\n",
            "Epoch :  4  Batch :  198  Loss :   0.024874136741437732  Accuracy :  0.9914141414141414 Time  1.06 s\n",
            "Epoch :  4  Batch :  199  Loss :   0.024754412225377394  Accuracy :  0.9914572864321608 Time  1.06 s\n",
            "Epoch :  4  Batch :  200  Loss :   0.024647182766348124  Accuracy :  0.9915 Time  1.05 s\n",
            "Epoch :  4  Batch :  201  Loss :   0.024561566733917344  Accuracy :  0.9915422885572139 Time  1.05 s\n",
            "Epoch :  4  Batch :  202  Loss :   0.024516453747499254  Accuracy :  0.9915841584158416 Time  1.05 s\n",
            "Epoch :  4  Batch :  203  Loss :   0.024399697393513848  Accuracy :  0.9916256157635468 Time  1.05 s\n",
            "Epoch :  4  Batch :  204  Loss :   0.024580411026916264  Accuracy :  0.991421568627451 Time  1.04 s\n",
            "Epoch :  4  Batch :  205  Loss :   0.024478354136368667  Accuracy :  0.9914634146341463 Time  1.05 s\n",
            "Epoch :  4  Batch :  206  Loss :   0.024576089909476528  Accuracy :  0.991504854368932 Time  1.05 s\n",
            "Epoch :  4  Batch :  207  Loss :   0.02455503078220313  Accuracy :  0.9915458937198067 Time  1.05 s\n",
            "Epoch :  4  Batch :  208  Loss :   0.024445017821562942  Accuracy :  0.9915865384615384 Time  1.06 s\n",
            "Epoch :  4  Batch :  209  Loss :   0.024352646017843704  Accuracy :  0.9916267942583732 Time  1.06 s\n",
            "Epoch :  4  Batch :  210  Loss :   0.024458433057381105  Accuracy :  0.9914285714285714 Time  1.06 s\n",
            "Epoch :  4  Batch :  211  Loss :   0.024399546345241226  Accuracy :  0.9914691943127962 Time  1.05 s\n",
            "Epoch :  4  Batch :  212  Loss :   0.024368363128917925  Accuracy :  0.9915094339622641 Time  1.05 s\n",
            "Epoch :  4  Batch :  213  Loss :   0.02470301601876168  Accuracy :  0.9913145539906103 Time  1.05 s\n",
            "Epoch :  4  Batch :  214  Loss :   0.024598855975140582  Accuracy :  0.9913551401869158 Time  1.05 s\n",
            "Epoch :  4  Batch :  215  Loss :   0.02463093200471079  Accuracy :  0.9913953488372093 Time  1.05 s\n",
            "Epoch :  4  Batch :  216  Loss :   0.024522063111631992  Accuracy :  0.9914351851851851 Time  1.04 s\n",
            "Epoch :  4  Batch :  217  Loss :   0.024475425555293494  Accuracy :  0.9914746543778802 Time  1.06 s\n",
            "Epoch :  4  Batch :  218  Loss :   0.024720967743532954  Accuracy :  0.9912844036697248 Time  1.04 s\n",
            "Epoch :  4  Batch :  219  Loss :   0.024644731927225067  Accuracy :  0.991324200913242 Time  1.06 s\n",
            "Epoch :  4  Batch :  220  Loss :   0.02463353728843768  Accuracy :  0.9913636363636363 Time  1.06 s\n",
            "Epoch :  4  Batch :  221  Loss :   0.025169607526327725  Accuracy :  0.9911764705882353 Time  1.07 s\n",
            "Epoch :  4  Batch :  222  Loss :   0.02506403549970757  Accuracy :  0.9912162162162163 Time  1.05 s\n",
            "Epoch :  4  Batch :  223  Loss :   0.024970583373893705  Accuracy :  0.991255605381166 Time  1.05 s\n",
            "Epoch :  4  Batch :  224  Loss :   0.024887156259605296  Accuracy :  0.9912946428571429 Time  1.05 s\n",
            "Epoch :  4  Batch :  225  Loss :   0.024835621514357628  Accuracy :  0.9913333333333333 Time  1.05 s\n",
            "Epoch :  4  Batch :  226  Loss :   0.024727950372271636  Accuracy :  0.9913716814159292 Time  1.04 s\n",
            "Epoch :  4  Batch :  227  Loss :   0.024774311796561015  Accuracy :  0.991409691629956 Time  1.04 s\n",
            "Epoch :  4  Batch :  228  Loss :   0.02466966123795014  Accuracy :  0.9914473684210526 Time  1.04 s\n",
            "Epoch :  4  Batch :  229  Loss :   0.024587298503733243  Accuracy :  0.9914847161572052 Time  1.05 s\n",
            "Epoch :  4  Batch :  230  Loss :   0.024491240859851646  Accuracy :  0.9915217391304347 Time  1.05 s\n",
            "Epoch :  4  Batch :  231  Loss :   0.024387140379607822  Accuracy :  0.9915584415584415 Time  1.06 s\n",
            "Epoch :  4  Batch :  232  Loss :   0.024340090330073946  Accuracy :  0.9915948275862069 Time  1.07 s\n",
            "Epoch :  4  Batch :  233  Loss :   0.024250550508597073  Accuracy :  0.9916309012875536 Time  1.05 s\n",
            "Epoch :  4  Batch :  234  Loss :   0.02422612143852589  Accuracy :  0.9916666666666667 Time  1.05 s\n",
            "Epoch :  4  Batch :  235  Loss :   0.02420485612683672  Accuracy :  0.9917021276595744 Time  1.05 s\n",
            "Epoch :  4  Batch :  236  Loss :   0.024120813787380093  Accuracy :  0.9917372881355933 Time  1.05 s\n",
            "Epoch :  4  Batch :  237  Loss :   0.024060187475482702  Accuracy :  0.9917721518987341 Time  1.05 s\n",
            "Epoch :  4  Batch :  238  Loss :   0.024128703180624236  Accuracy :  0.9918067226890757 Time  1.05 s\n",
            "Epoch :  4  Batch :  239  Loss :   0.02403486724168444  Accuracy :  0.9918410041841004 Time  1.05 s\n",
            "Epoch :  4  Batch :  240  Loss :   0.02394059580619796  Accuracy :  0.991875 Time  1.05 s\n",
            "Epoch :  4  Batch :  241  Loss :   0.023870714747151786  Accuracy :  0.9919087136929461 Time  1.06 s\n",
            "Epoch :  4  Batch :  242  Loss :   0.02412531786229777  Accuracy :  0.9917355371900827 Time  1.07 s\n",
            "Epoch :  4  Batch :  243  Loss :   0.024074051199804745  Accuracy :  0.9917695473251029 Time  1.06 s\n",
            "Epoch :  4  Batch :  244  Loss :   0.023995121757997978  Accuracy :  0.9918032786885246 Time  1.05 s\n",
            "Epoch :  4  Batch :  245  Loss :   0.02393719560813577  Accuracy :  0.9918367346938776 Time  1.05 s\n",
            "Epoch :  4  Batch :  246  Loss :   0.024655507112961127  Accuracy :  0.9916666666666667 Time  1.04 s\n",
            "Epoch :  4  Batch :  247  Loss :   0.02457359190752187  Accuracy :  0.9917004048582996 Time  1.05 s\n",
            "Epoch :  4  Batch :  248  Loss :   0.024522491073801046  Accuracy :  0.9917338709677419 Time  1.04 s\n",
            "Epoch :  4  Batch :  249  Loss :   0.024461159381845284  Accuracy :  0.9917670682730924 Time  1.05 s\n",
            "Epoch :  4  Batch :  250  Loss :   0.024458998900139705  Accuracy :  0.9918 Time  1.04 s\n",
            "Epoch :  4  Batch :  251  Loss :   0.024372840316203678  Accuracy :  0.9918326693227092 Time  1.05 s\n",
            "Epoch :  4  Batch :  252  Loss :   0.024340026362638344  Accuracy :  0.9918650793650794 Time  1.06 s\n",
            "Epoch :  4  Batch :  253  Loss :   0.024311862401194914  Accuracy :  0.9918972332015811 Time  1.06 s\n",
            "Epoch :  4  Batch :  254  Loss :   0.02422638079376092  Accuracy :  0.9919291338582678 Time  1.06 s\n",
            "Epoch :  4  Batch :  255  Loss :   0.024346840559897543  Accuracy :  0.991764705882353 Time  1.05 s\n",
            "Epoch :  4  Batch :  256  Loss :   0.024285325034043126  Accuracy :  0.991796875 Time  1.05 s\n",
            "Epoch :  4  Batch :  257  Loss :   0.024217151394630293  Accuracy :  0.9918287937743191 Time  1.04 s\n",
            "Epoch :  4  Batch :  258  Loss :   0.024182844473195242  Accuracy :  0.9918604651162791 Time  1.05 s\n",
            "Epoch :  4  Batch :  259  Loss :   0.024096006312466885  Accuracy :  0.9918918918918919 Time  1.05 s\n",
            "Epoch :  4  Batch :  260  Loss :   0.024025067958147982  Accuracy :  0.9919230769230769 Time  1.04 s\n",
            "Epoch :  4  Batch :  261  Loss :   0.023964109347724638  Accuracy :  0.9919540229885058 Time  1.05 s\n",
            "Epoch :  4  Batch :  262  Loss :   0.023961467923239256  Accuracy :  0.9919847328244275 Time  1.05 s\n",
            "Epoch :  4  Batch :  263  Loss :   0.023876244722584  Accuracy :  0.9920152091254753 Time  1.06 s\n",
            "Epoch :  4  Batch :  264  Loss :   0.02379201753898418  Accuracy :  0.9920454545454546 Time  1.05 s\n",
            "Epoch :  4  Batch :  265  Loss :   0.023705008341535435  Accuracy :  0.9920754716981132 Time  1.06 s\n",
            "Epoch :  4  Batch :  266  Loss :   0.023667915195753602  Accuracy :  0.9921052631578947 Time  1.05 s\n",
            "Epoch :  4  Batch :  267  Loss :   0.02366300834511138  Accuracy :  0.9921348314606742 Time  1.04 s\n",
            "Epoch :  4  Batch :  268  Loss :   0.02362429843055703  Accuracy :  0.9921641791044776 Time  1.05 s\n",
            "Epoch :  4  Batch :  269  Loss :   0.024163384174237367  Accuracy :  0.9920074349442379 Time  1.04 s\n",
            "Epoch :  4  Batch :  270  Loss :   0.024086410838774303  Accuracy :  0.992037037037037 Time  1.04 s\n",
            "Epoch :  4  Batch :  271  Loss :   0.024012487816643245  Accuracy :  0.9920664206642067 Time  1.04 s\n",
            "Epoch :  4  Batch :  272  Loss :   0.024302937016995308  Accuracy :  0.9919117647058824 Time  1.06 s\n",
            "Epoch :  4  Batch :  273  Loss :   0.024754716835842697  Accuracy :  0.9917582417582418 Time  1.04 s\n",
            "Epoch :  4  Batch :  274  Loss :   0.024703501426507752  Accuracy :  0.9917883211678832 Time  1.06 s\n",
            "Epoch :  4  Batch :  275  Loss :   0.02461885148934512  Accuracy :  0.9918181818181818 Time  1.06 s\n",
            "Epoch :  4  Batch :  276  Loss :   0.024542234742050598  Accuracy :  0.9918478260869565 Time  1.06 s\n",
            "Epoch :  4  Batch :  277  Loss :   0.024459319871082054  Accuracy :  0.9918772563176895 Time  1.05 s\n",
            "Epoch :  4  Batch :  278  Loss :   0.024410146331496657  Accuracy :  0.9919064748201439 Time  1.05 s\n",
            "Epoch :  4  Batch :  279  Loss :   0.024335946631743023  Accuracy :  0.9919354838709677 Time  1.04 s\n",
            "Epoch :  4  Batch :  280  Loss :   0.024341824016508844  Accuracy :  0.9919642857142857 Time  1.04 s\n",
            "Epoch :  4  Batch :  281  Loss :   0.024258050479921196  Accuracy :  0.9919928825622776 Time  1.05 s\n",
            "Epoch :  4  Batch :  282  Loss :   0.02417321909367229  Accuracy :  0.9920212765957447 Time  1.05 s\n",
            "Epoch :  4  Batch :  283  Loss :   0.02409327726401043  Accuracy :  0.9920494699646644 Time  1.05 s\n",
            "Epoch :  4  Batch :  284  Loss :   0.02401330791941342  Accuracy :  0.9920774647887324 Time  1.06 s\n",
            "Epoch :  4  Batch :  285  Loss :   0.023940663391687465  Accuracy :  0.9921052631578947 Time  1.06 s\n",
            "Epoch :  4  Batch :  286  Loss :   0.0238723095181403  Accuracy :  0.9921328671328671 Time  1.07 s\n",
            "Epoch :  4  Batch :  287  Loss :   0.023793175290334638  Accuracy :  0.9921602787456446 Time  1.06 s\n",
            "Epoch :  4  Batch :  288  Loss :   0.02372032054164366  Accuracy :  0.9921875 Time  1.06 s\n",
            "Epoch :  4  Batch :  289  Loss :   0.023639731847110712  Accuracy :  0.9922145328719724 Time  1.06 s\n",
            "Epoch :  4  Batch :  290  Loss :   0.02367831580662008  Accuracy :  0.9922413793103448 Time  1.05 s\n",
            "Epoch :  4  Batch :  291  Loss :   0.02360505223213423  Accuracy :  0.9922680412371134 Time  1.05 s\n",
            "Epoch :  4  Batch :  292  Loss :   0.023661004829903017  Accuracy :  0.9921232876712329 Time  1.05 s\n",
            "Epoch :  4  Batch :  293  Loss :   0.02362714664384392  Accuracy :  0.9921501706484641 Time  1.05 s\n",
            "Epoch :  4  Batch :  294  Loss :   0.02355327731478313  Accuracy :  0.9921768707482993 Time  1.04 s\n",
            "Epoch :  4  Batch :  295  Loss :   0.02351761997274865  Accuracy :  0.9922033898305085 Time  1.05 s\n",
            "Epoch :  4  Batch :  296  Loss :   0.0236303253957492  Accuracy :  0.9920608108108108 Time  1.05 s\n",
            "Epoch :  4  Batch :  297  Loss :   0.02356503731986627  Accuracy :  0.9920875420875421 Time  1.04 s\n",
            "Epoch :  4  Batch :  298  Loss :   0.023491952655408783  Accuracy :  0.9921140939597315 Time  1.06 s\n",
            "Epoch :  4  Batch :  299  Loss :   0.023473379516391626  Accuracy :  0.9921404682274247 Time  1.06 s\n",
            "Epoch :  4  Batch :  300  Loss :   0.02341557593977389  Accuracy :  0.9921666666666666 Time  1.06 s\n",
            "Epoch :  4  Batch :  301  Loss :   0.02336463750459167  Accuracy :  0.9921926910299004 Time  1.05 s\n",
            "Epoch :  4  Batch :  302  Loss :   0.02343532507152093  Accuracy :  0.9920529801324504 Time  1.05 s\n",
            "Epoch :  4  Batch :  303  Loss :   0.02337025388692607  Accuracy :  0.9920792079207921 Time  1.05 s\n",
            "Epoch :  4  Batch :  304  Loss :   0.023355195757942152  Accuracy :  0.9921052631578947 Time  1.04 s\n",
            "Epoch :  4  Batch :  305  Loss :   0.023327152268793127  Accuracy :  0.9921311475409836 Time  1.05 s\n",
            "Epoch :  4  Batch :  306  Loss :   0.023256710866310943  Accuracy :  0.9921568627450981 Time  1.05 s\n",
            "Epoch :  4  Batch :  307  Loss :   0.02325900334217176  Accuracy :  0.9921696574225123 Time  0.54 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 99 %\n",
            "[4 epoch] Accuracy of the network on the Training images: 99 %\n",
            "Epoch :  5  Batch :  1  Loss :   0.06834378838539124  Accuracy :  0.95 Time  1.04 s\n",
            "Epoch :  5  Batch :  2  Loss :   0.035537197487428784  Accuracy :  0.975 Time  1.06 s\n",
            "Epoch :  5  Batch :  3  Loss :   0.024478094652295113  Accuracy :  0.9833333333333333 Time  1.06 s\n",
            "Epoch :  5  Batch :  4  Loss :   0.020128440228290856  Accuracy :  0.9875 Time  1.07 s\n",
            "Epoch :  5  Batch :  5  Loss :   0.016411786922253667  Accuracy :  0.99 Time  1.04 s\n",
            "Epoch :  5  Batch :  6  Loss :   0.01465946352497364  Accuracy :  0.9916666666666667 Time  1.05 s\n",
            "Epoch :  5  Batch :  7  Loss :   0.014962901843578688  Accuracy :  0.9928571428571429 Time  1.04 s\n",
            "Epoch :  5  Batch :  8  Loss :   0.013366731567657553  Accuracy :  0.99375 Time  1.04 s\n",
            "Epoch :  5  Batch :  9  Loss :   0.012808493739511404  Accuracy :  0.9944444444444445 Time  1.04 s\n",
            "Epoch :  5  Batch :  10  Loss :   0.013136610097717494  Accuracy :  0.995 Time  1.04 s\n",
            "Epoch :  5  Batch :  11  Loss :   0.012065870877863332  Accuracy :  0.9954545454545455 Time  1.04 s\n",
            "Epoch :  5  Batch :  12  Loss :   0.011188263820561891  Accuracy :  0.9958333333333333 Time  1.04 s\n",
            "Epoch :  5  Batch :  13  Loss :   0.01067090099856544  Accuracy :  0.9961538461538462 Time  1.05 s\n",
            "Epoch :  5  Batch :  14  Loss :   0.00997597596142441  Accuracy :  0.9964285714285714 Time  1.06 s\n",
            "Epoch :  5  Batch :  15  Loss :   0.00941539586832126  Accuracy :  0.9966666666666667 Time  1.06 s\n",
            "Epoch :  5  Batch :  16  Loss :   0.008875585386704188  Accuracy :  0.996875 Time  1.05 s\n",
            "Epoch :  5  Batch :  17  Loss :   0.008608577711343327  Accuracy :  0.9970588235294118 Time  1.05 s\n",
            "Epoch :  5  Batch :  18  Loss :   0.008194314193032268  Accuracy :  0.9972222222222222 Time  1.04 s\n",
            "Epoch :  5  Batch :  19  Loss :   0.007845854564373823  Accuracy :  0.9973684210526316 Time  1.05 s\n",
            "Epoch :  5  Batch :  20  Loss :   0.008115129714133218  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  5  Batch :  21  Loss :   0.008241982892199996  Accuracy :  0.9976190476190476 Time  1.05 s\n",
            "Epoch :  5  Batch :  22  Loss :   0.008026038206563417  Accuracy :  0.9977272727272727 Time  1.04 s\n",
            "Epoch :  5  Batch :  23  Loss :   0.007773325479913341  Accuracy :  0.9978260869565218 Time  1.06 s\n",
            "Epoch :  5  Batch :  24  Loss :   0.007631628138672871  Accuracy :  0.9979166666666667 Time  1.06 s\n",
            "Epoch :  5  Batch :  25  Loss :   0.007389289997518062  Accuracy :  0.998 Time  1.06 s\n",
            "Epoch :  5  Batch :  26  Loss :   0.007147191320725072  Accuracy :  0.9980769230769231 Time  1.06 s\n",
            "Epoch :  5  Batch :  27  Loss :   0.006967889951300566  Accuracy :  0.9981481481481481 Time  1.05 s\n",
            "Epoch :  5  Batch :  28  Loss :   0.0068087863328401  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  5  Batch :  29  Loss :   0.006621509220386888  Accuracy :  0.9982758620689656 Time  1.05 s\n",
            "Epoch :  5  Batch :  30  Loss :   0.006439311418216675  Accuracy :  0.9983333333333333 Time  1.04 s\n",
            "Epoch :  5  Batch :  31  Loss :   0.006321604878100897  Accuracy :  0.9983870967741936 Time  1.04 s\n",
            "Epoch :  5  Batch :  32  Loss :   0.006220263570867246  Accuracy :  0.9984375 Time  1.04 s\n",
            "Epoch :  5  Batch :  33  Loss :   0.00614519320273151  Accuracy :  0.9984848484848485 Time  1.04 s\n",
            "Epoch :  5  Batch :  34  Loss :   0.006009961841354037  Accuracy :  0.9985294117647059 Time  1.05 s\n",
            "Epoch :  5  Batch :  35  Loss :   0.006121975828760437  Accuracy :  0.9985714285714286 Time  1.06 s\n",
            "Epoch :  5  Batch :  36  Loss :   0.005967884360062372  Accuracy :  0.9986111111111111 Time  1.07 s\n",
            "Epoch :  5  Batch :  37  Loss :   0.005828287970035564  Accuracy :  0.9986486486486487 Time  1.06 s\n",
            "Epoch :  5  Batch :  38  Loss :   0.00576147993591516  Accuracy :  0.9986842105263158 Time  1.05 s\n",
            "Epoch :  5  Batch :  39  Loss :   0.005630452485862547  Accuracy :  0.9987179487179487 Time  1.04 s\n",
            "Epoch :  5  Batch :  40  Loss :   0.005517615175631363  Accuracy :  0.99875 Time  1.04 s\n",
            "Epoch :  5  Batch :  41  Loss :   0.005428260957492833  Accuracy :  0.998780487804878 Time  1.05 s\n",
            "Epoch :  5  Batch :  42  Loss :   0.005653816778379094  Accuracy :  0.9988095238095238 Time  1.05 s\n",
            "Epoch :  5  Batch :  43  Loss :   0.005918270901527776  Accuracy :  0.9988372093023256 Time  1.04 s\n",
            "Epoch :  5  Batch :  44  Loss :   0.005804520654650828  Accuracy :  0.9988636363636364 Time  1.05 s\n",
            "Epoch :  5  Batch :  45  Loss :   0.005693324816982365  Accuracy :  0.9988888888888889 Time  1.05 s\n",
            "Epoch :  5  Batch :  46  Loss :   0.005690507040328711  Accuracy :  0.9989130434782608 Time  1.06 s\n",
            "Epoch :  5  Batch :  47  Loss :   0.005593621235172403  Accuracy :  0.9989361702127659 Time  1.07 s\n",
            "Epoch :  5  Batch :  48  Loss :   0.005493497263766282  Accuracy :  0.9989583333333333 Time  1.06 s\n",
            "Epoch :  5  Batch :  49  Loss :   0.005462711792182633  Accuracy :  0.9989795918367347 Time  1.06 s\n",
            "Epoch :  5  Batch :  50  Loss :   0.005366854526801035  Accuracy :  0.999 Time  1.05 s\n",
            "Epoch :  5  Batch :  51  Loss :   0.00613361159590639  Accuracy :  0.9980392156862745 Time  1.04 s\n",
            "Epoch :  5  Batch :  52  Loss :   0.006841468632606288  Accuracy :  0.9971153846153846 Time  1.04 s\n",
            "Epoch :  5  Batch :  53  Loss :   0.0067273841123096645  Accuracy :  0.9971698113207547 Time  1.04 s\n",
            "Epoch :  5  Batch :  54  Loss :   0.006883904310935004  Accuracy :  0.9972222222222222 Time  1.05 s\n",
            "Epoch :  5  Batch :  55  Loss :   0.006777199274140664  Accuracy :  0.9972727272727273 Time  1.05 s\n",
            "Epoch :  5  Batch :  56  Loss :   0.006686035831600228  Accuracy :  0.9973214285714286 Time  1.05 s\n",
            "Epoch :  5  Batch :  57  Loss :   0.006591710011809738  Accuracy :  0.9973684210526316 Time  1.06 s\n",
            "Epoch :  5  Batch :  58  Loss :   0.006609253861323191  Accuracy :  0.9974137931034482 Time  1.06 s\n",
            "Epoch :  5  Batch :  59  Loss :   0.0067053957066845965  Accuracy :  0.997457627118644 Time  1.06 s\n",
            "Epoch :  5  Batch :  60  Loss :   0.007232380863085078  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  5  Batch :  61  Loss :   0.007165425448472321  Accuracy :  0.9975409836065574 Time  1.05 s\n",
            "Epoch :  5  Batch :  62  Loss :   0.007088664925709787  Accuracy :  0.9975806451612903 Time  1.05 s\n",
            "Epoch :  5  Batch :  63  Loss :   0.006989468721532455  Accuracy :  0.9976190476190476 Time  1.04 s\n",
            "Epoch :  5  Batch :  64  Loss :   0.010818692452630785  Accuracy :  0.996875 Time  1.04 s\n",
            "Epoch :  5  Batch :  65  Loss :   0.010700156597886234  Accuracy :  0.9969230769230769 Time  1.04 s\n",
            "Epoch :  5  Batch :  66  Loss :   0.010961435510332442  Accuracy :  0.996969696969697 Time  1.05 s\n",
            "Epoch :  5  Batch :  67  Loss :   0.011810397035129535  Accuracy :  0.996268656716418 Time  1.04 s\n",
            "Epoch :  5  Batch :  68  Loss :   0.011644126714707133  Accuracy :  0.9963235294117647 Time  1.06 s\n",
            "Epoch :  5  Batch :  69  Loss :   0.011485404545016101  Accuracy :  0.9963768115942029 Time  1.06 s\n",
            "Epoch :  5  Batch :  70  Loss :   0.011392255168175325  Accuracy :  0.9964285714285714 Time  1.07 s\n",
            "Epoch :  5  Batch :  71  Loss :   0.01124913907449194  Accuracy :  0.9964788732394366 Time  1.05 s\n",
            "Epoch :  5  Batch :  72  Loss :   0.011131525232889948  Accuracy :  0.9965277777777778 Time  1.05 s\n",
            "Epoch :  5  Batch :  73  Loss :   0.010989649710085958  Accuracy :  0.9965753424657534 Time  1.05 s\n",
            "Epoch :  5  Batch :  74  Loss :   0.010850026519226565  Accuracy :  0.9966216216216216 Time  1.04 s\n",
            "Epoch :  5  Batch :  75  Loss :   0.01083889565896243  Accuracy :  0.9966666666666667 Time  1.05 s\n",
            "Epoch :  5  Batch :  76  Loss :   0.010722373099078572  Accuracy :  0.9967105263157895 Time  1.04 s\n",
            "Epoch :  5  Batch :  77  Loss :   0.010794960810536785  Accuracy :  0.9967532467532467 Time  1.05 s\n",
            "Epoch :  5  Batch :  78  Loss :   0.010715796703245873  Accuracy :  0.9967948717948718 Time  1.04 s\n",
            "Epoch :  5  Batch :  79  Loss :   0.010593460886031861  Accuracy :  0.9968354430379747 Time  1.06 s\n",
            "Epoch :  5  Batch :  80  Loss :   0.010473985262797214  Accuracy :  0.996875 Time  1.06 s\n",
            "Epoch :  5  Batch :  81  Loss :   0.010452018651168472  Accuracy :  0.9969135802469136 Time  1.06 s\n",
            "Epoch :  5  Batch :  82  Loss :   0.01060758702877182  Accuracy :  0.9969512195121951 Time  1.04 s\n",
            "Epoch :  5  Batch :  83  Loss :   0.010513057968725103  Accuracy :  0.9969879518072289 Time  1.05 s\n",
            "Epoch :  5  Batch :  84  Loss :   0.010399896634875663  Accuracy :  0.9970238095238095 Time  1.04 s\n",
            "Epoch :  5  Batch :  85  Loss :   0.01031155358068645  Accuracy :  0.9970588235294118 Time  1.05 s\n",
            "Epoch :  5  Batch :  86  Loss :   0.010199233145740012  Accuracy :  0.997093023255814 Time  1.05 s\n",
            "Epoch :  5  Batch :  87  Loss :   0.010148188894769797  Accuracy :  0.9971264367816092 Time  1.05 s\n",
            "Epoch :  5  Batch :  88  Loss :   0.010052388977783266  Accuracy :  0.9971590909090909 Time  1.04 s\n",
            "Epoch :  5  Batch :  89  Loss :   0.009991772671026083  Accuracy :  0.9971910112359551 Time  1.05 s\n",
            "Epoch :  5  Batch :  90  Loss :   0.010078364020187615  Accuracy :  0.9972222222222222 Time  1.06 s\n",
            "Epoch :  5  Batch :  91  Loss :   0.00998361407495155  Accuracy :  0.9972527472527473 Time  1.06 s\n",
            "Epoch :  5  Batch :  92  Loss :   0.009900393977422145  Accuracy :  0.9972826086956522 Time  1.06 s\n",
            "Epoch :  5  Batch :  93  Loss :   0.009801919944459192  Accuracy :  0.9973118279569892 Time  1.05 s\n",
            "Epoch :  5  Batch :  94  Loss :   0.009707929043252853  Accuracy :  0.9973404255319149 Time  1.04 s\n",
            "Epoch :  5  Batch :  95  Loss :   0.009626210588153059  Accuracy :  0.9973684210526316 Time  1.04 s\n",
            "Epoch :  5  Batch :  96  Loss :   0.009964770316704138  Accuracy :  0.996875 Time  1.05 s\n",
            "Epoch :  5  Batch :  97  Loss :   0.010076281517988896  Accuracy :  0.9969072164948454 Time  1.04 s\n",
            "Epoch :  5  Batch :  98  Loss :   0.010473412622540847  Accuracy :  0.996938775510204 Time  1.04 s\n",
            "Epoch :  5  Batch :  99  Loss :   0.010401938608588858  Accuracy :  0.996969696969697 Time  1.05 s\n",
            "Epoch :  5  Batch :  100  Loss :   0.010311279198504053  Accuracy :  0.997 Time  1.05 s\n",
            "Epoch :  5  Batch :  101  Loss :   0.010237431959917604  Accuracy :  0.997029702970297 Time  1.06 s\n",
            "Epoch :  5  Batch :  102  Loss :   0.010326447398470277  Accuracy :  0.9970588235294118 Time  1.06 s\n",
            "Epoch :  5  Batch :  103  Loss :   0.010321780573011566  Accuracy :  0.9970873786407767 Time  1.07 s\n",
            "Epoch :  5  Batch :  104  Loss :   0.010254766325837968  Accuracy :  0.9971153846153846 Time  1.05 s\n",
            "Epoch :  5  Batch :  105  Loss :   0.010230608310528277  Accuracy :  0.9971428571428571 Time  1.05 s\n",
            "Epoch :  5  Batch :  106  Loss :   0.010828074866813555  Accuracy :  0.9966981132075472 Time  1.04 s\n",
            "Epoch :  5  Batch :  107  Loss :   0.010794441441452635  Accuracy :  0.9967289719626168 Time  1.04 s\n",
            "Epoch :  5  Batch :  108  Loss :   0.010723253422636643  Accuracy :  0.9967592592592592 Time  1.04 s\n",
            "Epoch :  5  Batch :  109  Loss :   0.010669656874881117  Accuracy :  0.9967889908256881 Time  1.05 s\n",
            "Epoch :  5  Batch :  110  Loss :   0.01057762738253752  Accuracy :  0.9968181818181818 Time  1.05 s\n",
            "Epoch :  5  Batch :  111  Loss :   0.010495277025174719  Accuracy :  0.9968468468468469 Time  1.04 s\n",
            "Epoch :  5  Batch :  112  Loss :   0.011021834685899583  Accuracy :  0.996875 Time  1.06 s\n",
            "Epoch :  5  Batch :  113  Loss :   0.010938049103463052  Accuracy :  0.9969026548672566 Time  1.06 s\n",
            "Epoch :  5  Batch :  114  Loss :   0.01085693583815571  Accuracy :  0.9969298245614036 Time  1.07 s\n",
            "Epoch :  5  Batch :  115  Loss :   0.010816411267075202  Accuracy :  0.9969565217391304 Time  1.05 s\n",
            "Epoch :  5  Batch :  116  Loss :   0.010990760977588722  Accuracy :  0.9969827586206896 Time  1.04 s\n",
            "Epoch :  5  Batch :  117  Loss :   0.011028423903382614  Accuracy :  0.997008547008547 Time  1.04 s\n",
            "Epoch :  5  Batch :  118  Loss :   0.010945549313673529  Accuracy :  0.9970338983050847 Time  1.05 s\n",
            "Epoch :  5  Batch :  119  Loss :   0.01127098555512288  Accuracy :  0.9966386554621849 Time  1.05 s\n",
            "Epoch :  5  Batch :  120  Loss :   0.011189106459884595  Accuracy :  0.9966666666666667 Time  1.05 s\n",
            "Epoch :  5  Batch :  121  Loss :   0.011112128565288034  Accuracy :  0.996694214876033 Time  1.05 s\n",
            "Epoch :  5  Batch :  122  Loss :   0.011130296667182788  Accuracy :  0.9967213114754099 Time  1.04 s\n",
            "Epoch :  5  Batch :  123  Loss :   0.011076398099189609  Accuracy :  0.9967479674796748 Time  1.06 s\n",
            "Epoch :  5  Batch :  124  Loss :   0.01104196559097017  Accuracy :  0.9967741935483871 Time  1.06 s\n",
            "Epoch :  5  Batch :  125  Loss :   0.010989153988659382  Accuracy :  0.9968 Time  1.07 s\n",
            "Epoch :  5  Batch :  126  Loss :   0.010943628315414702  Accuracy :  0.9968253968253968 Time  1.05 s\n",
            "Epoch :  5  Batch :  127  Loss :   0.010865597654772028  Accuracy :  0.9968503937007874 Time  1.05 s\n",
            "Epoch :  5  Batch :  128  Loss :   0.01079259458128945  Accuracy :  0.996875 Time  1.05 s\n",
            "Epoch :  5  Batch :  129  Loss :   0.010816377381859131  Accuracy :  0.9968992248062015 Time  1.05 s\n",
            "Epoch :  5  Batch :  130  Loss :   0.010749269672669471  Accuracy :  0.9969230769230769 Time  1.04 s\n",
            "Epoch :  5  Batch :  131  Loss :   0.01073334435221405  Accuracy :  0.9969465648854962 Time  1.05 s\n",
            "Epoch :  5  Batch :  132  Loss :   0.010729576011230662  Accuracy :  0.996969696969697 Time  1.05 s\n",
            "Epoch :  5  Batch :  133  Loss :   0.010654732050580722  Accuracy :  0.9969924812030075 Time  1.04 s\n",
            "Epoch :  5  Batch :  134  Loss :   0.010589711414563205  Accuracy :  0.9970149253731343 Time  1.05 s\n",
            "Epoch :  5  Batch :  135  Loss :   0.010517643966832784  Accuracy :  0.997037037037037 Time  1.07 s\n",
            "Epoch :  5  Batch :  136  Loss :   0.0104489797817865  Accuracy :  0.9970588235294118 Time  1.07 s\n",
            "Epoch :  5  Batch :  137  Loss :   0.010378920325727277  Accuracy :  0.997080291970803 Time  1.05 s\n",
            "Epoch :  5  Batch :  138  Loss :   0.01053518609800106  Accuracy :  0.9971014492753624 Time  1.05 s\n",
            "Epoch :  5  Batch :  139  Loss :   0.010466292678050215  Accuracy :  0.9971223021582734 Time  1.05 s\n",
            "Epoch :  5  Batch :  140  Loss :   0.010423265633705471  Accuracy :  0.9971428571428571 Time  1.06 s\n",
            "Epoch :  5  Batch :  141  Loss :   0.010377385154868482  Accuracy :  0.9971631205673759 Time  1.05 s\n",
            "Epoch :  5  Batch :  142  Loss :   0.010336222563227507  Accuracy :  0.9971830985915493 Time  1.05 s\n",
            "Epoch :  5  Batch :  143  Loss :   0.010319315160826579  Accuracy :  0.9972027972027973 Time  1.05 s\n",
            "Epoch :  5  Batch :  144  Loss :   0.010266014809733152  Accuracy :  0.9972222222222222 Time  1.05 s\n",
            "Epoch :  5  Batch :  145  Loss :   0.010237359465902736  Accuracy :  0.9972413793103448 Time  1.06 s\n",
            "Epoch :  5  Batch :  146  Loss :   0.010170558305841564  Accuracy :  0.9972602739726028 Time  1.06 s\n",
            "Epoch :  5  Batch :  147  Loss :   0.010143123979903567  Accuracy :  0.9972789115646259 Time  1.06 s\n",
            "Epoch :  5  Batch :  148  Loss :   0.010317886733753619  Accuracy :  0.9972972972972973 Time  1.05 s\n",
            "Epoch :  5  Batch :  149  Loss :   0.010260326663796296  Accuracy :  0.9973154362416108 Time  1.04 s\n",
            "Epoch :  5  Batch :  150  Loss :   0.010197037746547722  Accuracy :  0.9973333333333333 Time  1.04 s\n",
            "Epoch :  5  Batch :  151  Loss :   0.010139157725937528  Accuracy :  0.9973509933774835 Time  1.04 s\n",
            "Epoch :  5  Batch :  152  Loss :   0.010355237321690717  Accuracy :  0.9973684210526316 Time  1.04 s\n",
            "Epoch :  5  Batch :  153  Loss :   0.010368093965594882  Accuracy :  0.9973856209150327 Time  1.05 s\n",
            "Epoch :  5  Batch :  154  Loss :   0.010835058523180242  Accuracy :  0.9970779220779221 Time  1.05 s\n",
            "Epoch :  5  Batch :  155  Loss :   0.010771938225051628  Accuracy :  0.9970967741935484 Time  1.06 s\n",
            "Epoch :  5  Batch :  156  Loss :   0.010715156932583807  Accuracy :  0.9971153846153846 Time  1.06 s\n",
            "Epoch :  5  Batch :  157  Loss :   0.01067379991554323  Accuracy :  0.9971337579617834 Time  1.06 s\n",
            "Epoch :  5  Batch :  158  Loss :   0.01061100483804947  Accuracy :  0.9971518987341772 Time  1.06 s\n",
            "Epoch :  5  Batch :  159  Loss :   0.010664808785919182  Accuracy :  0.9971698113207547 Time  1.05 s\n",
            "Epoch :  5  Batch :  160  Loss :   0.01060854252518766  Accuracy :  0.9971875 Time  1.05 s\n",
            "Epoch :  5  Batch :  161  Loss :   0.010552260506448412  Accuracy :  0.9972049689440994 Time  1.05 s\n",
            "Epoch :  5  Batch :  162  Loss :   0.01050131472153158  Accuracy :  0.9972222222222222 Time  1.05 s\n",
            "Epoch :  5  Batch :  163  Loss :   0.010444950666085766  Accuracy :  0.997239263803681 Time  1.05 s\n",
            "Epoch :  5  Batch :  164  Loss :   0.010493891290424493  Accuracy :  0.9972560975609757 Time  1.05 s\n",
            "Epoch :  5  Batch :  165  Loss :   0.010539373235642966  Accuracy :  0.9972727272727273 Time  1.05 s\n",
            "Epoch :  5  Batch :  166  Loss :   0.010502241133377012  Accuracy :  0.9972891566265061 Time  1.05 s\n",
            "Epoch :  5  Batch :  167  Loss :   0.010457336830991615  Accuracy :  0.9973053892215569 Time  1.05 s\n",
            "Epoch :  5  Batch :  168  Loss :   0.010401924949595317  Accuracy :  0.9973214285714286 Time  1.06 s\n",
            "Epoch :  5  Batch :  169  Loss :   0.010343584191562475  Accuracy :  0.9973372781065089 Time  1.07 s\n",
            "Epoch :  5  Batch :  170  Loss :   0.010295433882000746  Accuracy :  0.9973529411764706 Time  1.05 s\n",
            "Epoch :  5  Batch :  171  Loss :   0.010446737299309717  Accuracy :  0.9973684210526316 Time  1.05 s\n",
            "Epoch :  5  Batch :  172  Loss :   0.010399257341368654  Accuracy :  0.9973837209302325 Time  1.05 s\n",
            "Epoch :  5  Batch :  173  Loss :   0.010358489088403233  Accuracy :  0.9973988439306358 Time  1.05 s\n",
            "Epoch :  5  Batch :  174  Loss :   0.010414032016154219  Accuracy :  0.9974137931034482 Time  1.05 s\n",
            "Epoch :  5  Batch :  175  Loss :   0.010620510500489868  Accuracy :  0.9971428571428571 Time  1.04 s\n",
            "Epoch :  5  Batch :  176  Loss :   0.010651090052223562  Accuracy :  0.9971590909090909 Time  1.05 s\n",
            "Epoch :  5  Batch :  177  Loss :   0.010594600751255455  Accuracy :  0.9971751412429378 Time  1.04 s\n",
            "Epoch :  5  Batch :  178  Loss :   0.010574637692627037  Accuracy :  0.9971910112359551 Time  1.06 s\n",
            "Epoch :  5  Batch :  179  Loss :   0.010593120019275476  Accuracy :  0.9972067039106145 Time  1.06 s\n",
            "Epoch :  5  Batch :  180  Loss :   0.01054767826458879  Accuracy :  0.9972222222222222 Time  1.07 s\n",
            "Epoch :  5  Batch :  181  Loss :   0.010496655674117427  Accuracy :  0.9972375690607734 Time  1.07 s\n",
            "Epoch :  5  Batch :  182  Loss :   0.010451404703557134  Accuracy :  0.9972527472527473 Time  1.06 s\n",
            "Epoch :  5  Batch :  183  Loss :   0.010406717465448707  Accuracy :  0.9972677595628415 Time  1.05 s\n",
            "Epoch :  5  Batch :  184  Loss :   0.010367606911566563  Accuracy :  0.9972826086956522 Time  1.05 s\n",
            "Epoch :  5  Batch :  185  Loss :   0.01031886560895883  Accuracy :  0.9972972972972973 Time  1.04 s\n",
            "Epoch :  5  Batch :  186  Loss :   0.0102727479053854  Accuracy :  0.9973118279569892 Time  1.05 s\n",
            "Epoch :  5  Batch :  187  Loss :   0.010238291574473975  Accuracy :  0.9973262032085561 Time  1.05 s\n",
            "Epoch :  5  Batch :  188  Loss :   0.010220828434691437  Accuracy :  0.9973404255319149 Time  1.04 s\n",
            "Epoch :  5  Batch :  189  Loss :   0.010183223065699663  Accuracy :  0.9973544973544973 Time  1.05 s\n",
            "Epoch :  5  Batch :  190  Loss :   0.010136066485153462  Accuracy :  0.9973684210526316 Time  1.06 s\n",
            "Epoch :  5  Batch :  191  Loss :   0.01011329815928405  Accuracy :  0.9973821989528796 Time  1.08 s\n",
            "Epoch :  5  Batch :  192  Loss :   0.010094009781369095  Accuracy :  0.9973958333333334 Time  1.06 s\n",
            "Epoch :  5  Batch :  193  Loss :   0.010046373926545855  Accuracy :  0.9974093264248705 Time  1.05 s\n",
            "Epoch :  5  Batch :  194  Loss :   0.010033195724767229  Accuracy :  0.9974226804123711 Time  1.04 s\n",
            "Epoch :  5  Batch :  195  Loss :   0.010520496068205923  Accuracy :  0.9971794871794872 Time  1.05 s\n",
            "Epoch :  5  Batch :  196  Loss :   0.010478270841656699  Accuracy :  0.9971938775510204 Time  1.04 s\n",
            "Epoch :  5  Batch :  197  Loss :   0.010432186100819871  Accuracy :  0.9972081218274111 Time  1.05 s\n",
            "Epoch :  5  Batch :  198  Loss :   0.010384342623927461  Accuracy :  0.9972222222222222 Time  1.04 s\n",
            "Epoch :  5  Batch :  199  Loss :   0.010341410966965617  Accuracy :  0.9972361809045226 Time  1.05 s\n",
            "Epoch :  5  Batch :  200  Loss :   0.010339857202052372  Accuracy :  0.99725 Time  1.05 s\n",
            "Epoch :  5  Batch :  201  Loss :   0.01043108754530109  Accuracy :  0.9972636815920398 Time  1.05 s\n",
            "Epoch :  5  Batch :  202  Loss :   0.010434627027676146  Accuracy :  0.9972772277227723 Time  1.06 s\n",
            "Epoch :  5  Batch :  203  Loss :   0.01200548512818122  Accuracy :  0.996551724137931 Time  1.06 s\n",
            "Epoch :  5  Batch :  204  Loss :   0.01195085112233167  Accuracy :  0.9965686274509804 Time  1.05 s\n",
            "Epoch :  5  Batch :  205  Loss :   0.011893940373840629  Accuracy :  0.9965853658536585 Time  1.05 s\n",
            "Epoch :  5  Batch :  206  Loss :   0.011857081844230653  Accuracy :  0.9966019417475728 Time  1.05 s\n",
            "Epoch :  5  Batch :  207  Loss :   0.011802938730081381  Accuracy :  0.9966183574879227 Time  1.05 s\n",
            "Epoch :  5  Batch :  208  Loss :   0.01177435541061501  Accuracy :  0.9966346153846154 Time  1.05 s\n",
            "Epoch :  5  Batch :  209  Loss :   0.011733022480996984  Accuracy :  0.9966507177033492 Time  1.05 s\n",
            "Epoch :  5  Batch :  210  Loss :   0.011700659926420831  Accuracy :  0.9966666666666667 Time  1.05 s\n",
            "Epoch :  5  Batch :  211  Loss :   0.012013399318468612  Accuracy :  0.9964454976303317 Time  1.04 s\n",
            "Epoch :  5  Batch :  212  Loss :   0.012019603889197207  Accuracy :  0.9964622641509434 Time  1.06 s\n",
            "Epoch :  5  Batch :  213  Loss :   0.0119748187415081  Accuracy :  0.9964788732394366 Time  1.06 s\n",
            "Epoch :  5  Batch :  214  Loss :   0.011941672889619117  Accuracy :  0.9964953271028038 Time  1.06 s\n",
            "Epoch :  5  Batch :  215  Loss :   0.01189964150422426  Accuracy :  0.9965116279069768 Time  1.05 s\n",
            "Epoch :  5  Batch :  216  Loss :   0.011861923388264754  Accuracy :  0.9965277777777778 Time  1.04 s\n",
            "Epoch :  5  Batch :  217  Loss :   0.011839647247742493  Accuracy :  0.9965437788018433 Time  1.04 s\n",
            "Epoch :  5  Batch :  218  Loss :   0.013465778208353745  Accuracy :  0.9963302752293578 Time  1.04 s\n",
            "Epoch :  5  Batch :  219  Loss :   0.013414276224113448  Accuracy :  0.9963470319634703 Time  1.05 s\n",
            "Epoch :  5  Batch :  220  Loss :   0.013368867700046394  Accuracy :  0.9963636363636363 Time  1.05 s\n",
            "Epoch :  5  Batch :  221  Loss :   0.013346369595691451  Accuracy :  0.9963800904977376 Time  1.05 s\n",
            "Epoch :  5  Batch :  222  Loss :   0.013298257761021346  Accuracy :  0.9963963963963964 Time  1.04 s\n",
            "Epoch :  5  Batch :  223  Loss :   0.013256473660300958  Accuracy :  0.9964125560538116 Time  1.04 s\n",
            "Epoch :  5  Batch :  224  Loss :   0.013234339186055877  Accuracy :  0.9964285714285714 Time  1.07 s\n",
            "Epoch :  5  Batch :  225  Loss :   0.013192240854906332  Accuracy :  0.9964444444444445 Time  1.06 s\n",
            "Epoch :  5  Batch :  226  Loss :   0.013357677845571776  Accuracy :  0.9962389380530974 Time  1.05 s\n",
            "Epoch :  5  Batch :  227  Loss :   0.013305409058455649  Accuracy :  0.9962555066079295 Time  1.05 s\n",
            "Epoch :  5  Batch :  228  Loss :   0.013375424736934068  Accuracy :  0.9962719298245614 Time  1.04 s\n",
            "Epoch :  5  Batch :  229  Loss :   0.013344134138891978  Accuracy :  0.9962882096069869 Time  1.06 s\n",
            "Epoch :  5  Batch :  230  Loss :   0.014153397370651162  Accuracy :  0.9960869565217392 Time  1.04 s\n",
            "Epoch :  5  Batch :  231  Loss :   0.014120627115898891  Accuracy :  0.9961038961038962 Time  1.05 s\n",
            "Epoch :  5  Batch :  232  Loss :   0.014269592740110339  Accuracy :  0.9959051724137931 Time  1.05 s\n",
            "Epoch :  5  Batch :  233  Loss :   0.01422007765524517  Accuracy :  0.9959227467811159 Time  1.04 s\n",
            "Epoch :  5  Batch :  234  Loss :   0.014234716167079452  Accuracy :  0.995940170940171 Time  1.04 s\n",
            "Epoch :  5  Batch :  235  Loss :   0.014207397745005746  Accuracy :  0.9959574468085106 Time  1.06 s\n",
            "Epoch :  5  Batch :  236  Loss :   0.014151809692074016  Accuracy :  0.9959745762711865 Time  1.06 s\n",
            "Epoch :  5  Batch :  237  Loss :   0.014098829437388138  Accuracy :  0.9959915611814346 Time  1.04 s\n",
            "Epoch :  5  Batch :  238  Loss :   0.014042479577332446  Accuracy :  0.9960084033613446 Time  1.04 s\n",
            "Epoch :  5  Batch :  239  Loss :   0.014218449363047569  Accuracy :  0.9960251046025105 Time  1.04 s\n",
            "Epoch :  5  Batch :  240  Loss :   0.014395671528351766  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  5  Batch :  241  Loss :   0.014364878231967719  Accuracy :  0.995850622406639 Time  1.05 s\n",
            "Epoch :  5  Batch :  242  Loss :   0.014314928175164733  Accuracy :  0.9958677685950413 Time  1.04 s\n",
            "Epoch :  5  Batch :  243  Loss :   0.014268178942516531  Accuracy :  0.9958847736625515 Time  1.04 s\n",
            "Epoch :  5  Batch :  244  Loss :   0.01422104769666416  Accuracy :  0.9959016393442623 Time  1.05 s\n",
            "Epoch :  5  Batch :  245  Loss :   0.014166658067937979  Accuracy :  0.9959183673469387 Time  1.05 s\n",
            "Epoch :  5  Batch :  246  Loss :   0.014113138474186132  Accuracy :  0.9959349593495935 Time  1.06 s\n",
            "Epoch :  5  Batch :  247  Loss :   0.014415266986645057  Accuracy :  0.995748987854251 Time  1.06 s\n",
            "Epoch :  5  Batch :  248  Loss :   0.014403263485174619  Accuracy :  0.995766129032258 Time  1.04 s\n",
            "Epoch :  5  Batch :  249  Loss :   0.014486120216035092  Accuracy :  0.9957831325301205 Time  1.05 s\n",
            "Epoch :  5  Batch :  250  Loss :   0.014439013356226497  Accuracy :  0.9958 Time  1.04 s\n",
            "Epoch :  5  Batch :  251  Loss :   0.014400578556518327  Accuracy :  0.9958167330677291 Time  1.05 s\n",
            "Epoch :  5  Batch :  252  Loss :   0.01442669076641323  Accuracy :  0.9958333333333333 Time  1.04 s\n",
            "Epoch :  5  Batch :  253  Loss :   0.01452212916535745  Accuracy :  0.9958498023715415 Time  1.04 s\n",
            "Epoch :  5  Batch :  254  Loss :   0.014826549000659496  Accuracy :  0.9956692913385827 Time  1.04 s\n",
            "Epoch :  5  Batch :  255  Loss :   0.01490914025789072  Accuracy :  0.995686274509804 Time  1.05 s\n",
            "Epoch :  5  Batch :  256  Loss :   0.01486170737177872  Accuracy :  0.995703125 Time  1.04 s\n",
            "Epoch :  5  Batch :  257  Loss :   0.014807333485409075  Accuracy :  0.9957198443579767 Time  1.06 s\n",
            "Epoch :  5  Batch :  258  Loss :   0.014790146698829602  Accuracy :  0.9957364341085271 Time  1.06 s\n",
            "Epoch :  5  Batch :  259  Loss :   0.014768195673522205  Accuracy :  0.9957528957528957 Time  1.06 s\n",
            "Epoch :  5  Batch :  260  Loss :   0.014714347767809298  Accuracy :  0.9957692307692307 Time  1.04 s\n",
            "Epoch :  5  Batch :  261  Loss :   0.014675485176838029  Accuracy :  0.9957854406130269 Time  1.04 s\n",
            "Epoch :  5  Batch :  262  Loss :   0.014632261468951576  Accuracy :  0.9958015267175573 Time  1.05 s\n",
            "Epoch :  5  Batch :  263  Loss :   0.014601225820091154  Accuracy :  0.9958174904942966 Time  1.05 s\n",
            "Epoch :  5  Batch :  264  Loss :   0.014683282215022095  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  5  Batch :  265  Loss :   0.014641662662132767  Accuracy :  0.9958490566037735 Time  1.05 s\n",
            "Epoch :  5  Batch :  266  Loss :   0.01459080620313893  Accuracy :  0.9958646616541353 Time  1.04 s\n",
            "Epoch :  5  Batch :  267  Loss :   0.014539021087704882  Accuracy :  0.9958801498127341 Time  1.05 s\n",
            "Epoch :  5  Batch :  268  Loss :   0.014490153044857495  Accuracy :  0.9958955223880597 Time  1.06 s\n",
            "Epoch :  5  Batch :  269  Loss :   0.014446834538830444  Accuracy :  0.995910780669145 Time  1.06 s\n",
            "Epoch :  5  Batch :  270  Loss :   0.01439589908210716  Accuracy :  0.9959259259259259 Time  1.07 s\n",
            "Epoch :  5  Batch :  271  Loss :   0.014402597033993488  Accuracy :  0.9959409594095942 Time  1.05 s\n",
            "Epoch :  5  Batch :  272  Loss :   0.014394594392048448  Accuracy :  0.9959558823529412 Time  1.05 s\n",
            "Epoch :  5  Batch :  273  Loss :   0.01448137375354279  Accuracy :  0.9957875457875458 Time  1.05 s\n",
            "Epoch :  5  Batch :  274  Loss :   0.014457261585481771  Accuracy :  0.9958029197080291 Time  1.05 s\n",
            "Epoch :  5  Batch :  275  Loss :   0.014409658880163493  Accuracy :  0.9958181818181818 Time  1.05 s\n",
            "Epoch :  5  Batch :  276  Loss :   0.01437827943585074  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  5  Batch :  277  Loss :   0.014345270085727647  Accuracy :  0.9958483754512636 Time  1.05 s\n",
            "Epoch :  5  Batch :  278  Loss :   0.014297329917530825  Accuracy :  0.995863309352518 Time  1.05 s\n",
            "Epoch :  5  Batch :  279  Loss :   0.014423250854637066  Accuracy :  0.9956989247311828 Time  1.06 s\n",
            "Epoch :  5  Batch :  280  Loss :   0.014891863903384157  Accuracy :  0.9955357142857143 Time  1.06 s\n",
            "Epoch :  5  Batch :  281  Loss :   0.014843447883037545  Accuracy :  0.9955516014234875 Time  1.07 s\n",
            "Epoch :  5  Batch :  282  Loss :   0.014811634941539292  Accuracy :  0.9955673758865248 Time  1.04 s\n",
            "Epoch :  5  Batch :  283  Loss :   0.014775718032574122  Accuracy :  0.9955830388692579 Time  1.05 s\n",
            "Epoch :  5  Batch :  284  Loss :   0.014769739333747617  Accuracy :  0.9955985915492958 Time  1.04 s\n",
            "Epoch :  5  Batch :  285  Loss :   0.014732532008865587  Accuracy :  0.9956140350877193 Time  1.05 s\n",
            "Epoch :  5  Batch :  286  Loss :   0.014716529158872903  Accuracy :  0.9956293706293706 Time  1.04 s\n",
            "Epoch :  5  Batch :  287  Loss :   0.014671670107957047  Accuracy :  0.9956445993031359 Time  1.05 s\n",
            "Epoch :  5  Batch :  288  Loss :   0.015117492304145545  Accuracy :  0.9954861111111111 Time  1.04 s\n",
            "Epoch :  5  Batch :  289  Loss :   0.015122138334870834  Accuracy :  0.9955017301038063 Time  1.05 s\n",
            "Epoch :  5  Batch :  290  Loss :   0.015105189447760872  Accuracy :  0.9955172413793103 Time  1.06 s\n",
            "Epoch :  5  Batch :  291  Loss :   0.015113243641659035  Accuracy :  0.99553264604811 Time  1.06 s\n",
            "Epoch :  5  Batch :  292  Loss :   0.015074472656423284  Accuracy :  0.9955479452054794 Time  1.06 s\n",
            "Epoch :  5  Batch :  293  Loss :   0.015025836809632867  Accuracy :  0.9955631399317406 Time  1.04 s\n",
            "Epoch :  5  Batch :  294  Loss :   0.014979947448552104  Accuracy :  0.995578231292517 Time  1.04 s\n",
            "Epoch :  5  Batch :  295  Loss :   0.014938326546097553  Accuracy :  0.995593220338983 Time  1.06 s\n",
            "Epoch :  5  Batch :  296  Loss :   0.014904639768517242  Accuracy :  0.9956081081081081 Time  1.05 s\n",
            "Epoch :  5  Batch :  297  Loss :   0.014952397898920602  Accuracy :  0.9956228956228956 Time  1.05 s\n",
            "Epoch :  5  Batch :  298  Loss :   0.014979903082946947  Accuracy :  0.9956375838926175 Time  1.05 s\n",
            "Epoch :  5  Batch :  299  Loss :   0.01493827439367656  Accuracy :  0.9956521739130435 Time  1.04 s\n",
            "Epoch :  5  Batch :  300  Loss :   0.0148967771952933  Accuracy :  0.9956666666666667 Time  1.04 s\n",
            "Epoch :  5  Batch :  301  Loss :   0.014874160686485388  Accuracy :  0.9956810631229236 Time  1.06 s\n",
            "Epoch :  5  Batch :  302  Loss :   0.014834265862896094  Accuracy :  0.9956953642384105 Time  1.06 s\n",
            "Epoch :  5  Batch :  303  Loss :   0.014786675570762436  Accuracy :  0.9957095709570957 Time  1.06 s\n",
            "Epoch :  5  Batch :  304  Loss :   0.014742123464250318  Accuracy :  0.9957236842105263 Time  1.05 s\n",
            "Epoch :  5  Batch :  305  Loss :   0.015347970617824372  Accuracy :  0.9954098360655738 Time  1.06 s\n",
            "Epoch :  5  Batch :  306  Loss :   0.015304069429939594  Accuracy :  0.9954248366013072 Time  1.05 s\n",
            "Epoch :  5  Batch :  307  Loss :   0.015635406082258666  Accuracy :  0.9952691680261011 Time  0.54 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 99 %\n",
            "[5 epoch] Accuracy of the network on the Training images: 99 %\n",
            "Epoch :  6  Batch :  1  Loss :   0.009879588149487972  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  6  Batch :  2  Loss :   0.005987788666971028  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  6  Batch :  3  Loss :   0.00415143864423347  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  6  Batch :  4  Loss :   0.006118244236859027  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  6  Batch :  5  Loss :   0.01646930927527137  Accuracy :  0.99 Time  1.06 s\n",
            "Epoch :  6  Batch :  6  Loss :   0.014406973171086671  Accuracy :  0.9916666666666667 Time  1.07 s\n",
            "Epoch :  6  Batch :  7  Loss :   0.012597187941927197  Accuracy :  0.9928571428571429 Time  1.06 s\n",
            "Epoch :  6  Batch :  8  Loss :   0.014129238774330588  Accuracy :  0.99375 Time  1.05 s\n",
            "Epoch :  6  Batch :  9  Loss :   0.02524430686672632  Accuracy :  0.9888888888888889 Time  1.05 s\n",
            "Epoch :  6  Batch :  10  Loss :   0.027170757661224343  Accuracy :  0.99 Time  1.05 s\n",
            "Epoch :  6  Batch :  11  Loss :   0.024826105665521358  Accuracy :  0.990909090909091 Time  1.04 s\n",
            "Epoch :  6  Batch :  12  Loss :   0.023897858474811073  Accuracy :  0.9916666666666667 Time  1.04 s\n",
            "Epoch :  6  Batch :  13  Loss :   0.022104836142926406  Accuracy :  0.9923076923076923 Time  1.05 s\n",
            "Epoch :  6  Batch :  14  Loss :   0.025656731269139397  Accuracy :  0.9892857142857143 Time  1.05 s\n",
            "Epoch :  6  Batch :  15  Loss :   0.024247398974451547  Accuracy :  0.99 Time  1.05 s\n",
            "Epoch :  6  Batch :  16  Loss :   0.02329750231183425  Accuracy :  0.990625 Time  1.07 s\n",
            "Epoch :  6  Batch :  17  Loss :   0.02237982363337377  Accuracy :  0.9911764705882353 Time  1.07 s\n",
            "Epoch :  6  Batch :  18  Loss :   0.026923193856621057  Accuracy :  0.9861111111111112 Time  1.06 s\n",
            "Epoch :  6  Batch :  19  Loss :   0.026021108670025377  Accuracy :  0.9868421052631579 Time  1.05 s\n",
            "Epoch :  6  Batch :  20  Loss :   0.025142634242365604  Accuracy :  0.9875 Time  1.04 s\n",
            "Epoch :  6  Batch :  21  Loss :   0.024119215508800976  Accuracy :  0.9880952380952381 Time  1.05 s\n",
            "Epoch :  6  Batch :  22  Loss :   0.024666000307082537  Accuracy :  0.9886363636363636 Time  1.04 s\n",
            "Epoch :  6  Batch :  23  Loss :   0.023835799394873902  Accuracy :  0.9891304347826086 Time  1.05 s\n",
            "Epoch :  6  Batch :  24  Loss :   0.022945919996952096  Accuracy :  0.9895833333333334 Time  1.04 s\n",
            "Epoch :  6  Batch :  25  Loss :   0.02391642942209728  Accuracy :  0.988 Time  1.05 s\n",
            "Epoch :  6  Batch :  26  Loss :   0.023280304598022036  Accuracy :  0.9884615384615385 Time  1.05 s\n",
            "Epoch :  6  Batch :  27  Loss :   0.02297682483374417  Accuracy :  0.9888888888888889 Time  1.07 s\n",
            "Epoch :  6  Batch :  28  Loss :   0.022205353786245854  Accuracy :  0.9892857142857143 Time  1.06 s\n",
            "Epoch :  6  Batch :  29  Loss :   0.0215123281864337  Accuracy :  0.9896551724137931 Time  1.06 s\n",
            "Epoch :  6  Batch :  30  Loss :   0.021269658507662825  Accuracy :  0.99 Time  1.05 s\n",
            "Epoch :  6  Batch :  31  Loss :   0.020700233031792806  Accuracy :  0.9903225806451613 Time  1.05 s\n",
            "Epoch :  6  Batch :  32  Loss :   0.020098746755138563  Accuracy :  0.990625 Time  1.05 s\n",
            "Epoch :  6  Batch :  33  Loss :   0.019665219963027277  Accuracy :  0.990909090909091 Time  1.06 s\n",
            "Epoch :  6  Batch :  34  Loss :   0.019499574460068662  Accuracy :  0.9911764705882353 Time  1.05 s\n",
            "Epoch :  6  Batch :  35  Loss :   0.018991623703290574  Accuracy :  0.9914285714285714 Time  1.06 s\n",
            "Epoch :  6  Batch :  36  Loss :   0.01853130112754621  Accuracy :  0.9916666666666667 Time  1.05 s\n",
            "Epoch :  6  Batch :  37  Loss :   0.01828492439518733  Accuracy :  0.9918918918918919 Time  1.05 s\n",
            "Epoch :  6  Batch :  38  Loss :   0.017867729923301868  Accuracy :  0.9921052631578947 Time  1.06 s\n",
            "Epoch :  6  Batch :  39  Loss :   0.017457508757405985  Accuracy :  0.9923076923076923 Time  1.06 s\n",
            "Epoch :  6  Batch :  40  Loss :   0.023013587168679805  Accuracy :  0.99 Time  1.06 s\n",
            "Epoch :  6  Batch :  41  Loss :   0.02267849203497285  Accuracy :  0.9902439024390244 Time  1.05 s\n",
            "Epoch :  6  Batch :  42  Loss :   0.022498489324962498  Accuracy :  0.9904761904761905 Time  1.05 s\n",
            "Epoch :  6  Batch :  43  Loss :   0.022195685926288757  Accuracy :  0.9906976744186047 Time  1.05 s\n",
            "Epoch :  6  Batch :  44  Loss :   0.02175252683611523  Accuracy :  0.990909090909091 Time  1.04 s\n",
            "Epoch :  6  Batch :  45  Loss :   0.021464522266372417  Accuracy :  0.9911111111111112 Time  1.05 s\n",
            "Epoch :  6  Batch :  46  Loss :   0.02107409216810281  Accuracy :  0.991304347826087 Time  1.05 s\n",
            "Epoch :  6  Batch :  47  Loss :   0.020810672330318337  Accuracy :  0.9914893617021276 Time  1.04 s\n",
            "Epoch :  6  Batch :  48  Loss :   0.02287743557887249  Accuracy :  0.990625 Time  1.04 s\n",
            "Epoch :  6  Batch :  49  Loss :   0.022446899559011455  Accuracy :  0.9908163265306122 Time  1.06 s\n",
            "Epoch :  6  Batch :  50  Loss :   0.022051720701274462  Accuracy :  0.991 Time  1.06 s\n",
            "Epoch :  6  Batch :  51  Loss :   0.02170202320306927  Accuracy :  0.9911764705882353 Time  1.06 s\n",
            "Epoch :  6  Batch :  52  Loss :   0.021297950529865026  Accuracy :  0.9913461538461539 Time  1.05 s\n",
            "Epoch :  6  Batch :  53  Loss :   0.02098708660374747  Accuracy :  0.9915094339622641 Time  1.05 s\n",
            "Epoch :  6  Batch :  54  Loss :   0.02060524139960762  Accuracy :  0.9916666666666667 Time  1.04 s\n",
            "Epoch :  6  Batch :  55  Loss :   0.020710606266088276  Accuracy :  0.9918181818181818 Time  1.04 s\n",
            "Epoch :  6  Batch :  56  Loss :   0.02037519964765774  Accuracy :  0.9919642857142857 Time  1.05 s\n",
            "Epoch :  6  Batch :  57  Loss :   0.020121318384649624  Accuracy :  0.9921052631578947 Time  1.04 s\n",
            "Epoch :  6  Batch :  58  Loss :   0.019895466131354875  Accuracy :  0.9922413793103448 Time  1.05 s\n",
            "Epoch :  6  Batch :  59  Loss :   0.019584293996666606  Accuracy :  0.9923728813559322 Time  1.05 s\n",
            "Epoch :  6  Batch :  60  Loss :   0.01970776201390739  Accuracy :  0.9925 Time  1.06 s\n",
            "Epoch :  6  Batch :  61  Loss :   0.019417687321604673  Accuracy :  0.9926229508196721 Time  1.07 s\n",
            "Epoch :  6  Batch :  62  Loss :   0.019118805868915404  Accuracy :  0.992741935483871 Time  1.06 s\n",
            "Epoch :  6  Batch :  63  Loss :   0.01886702507285268  Accuracy :  0.9928571428571429 Time  1.05 s\n",
            "Epoch :  6  Batch :  64  Loss :   0.018584839475352055  Accuracy :  0.99296875 Time  1.05 s\n",
            "Epoch :  6  Batch :  65  Loss :   0.018310003516783652  Accuracy :  0.9930769230769231 Time  1.05 s\n",
            "Epoch :  6  Batch :  66  Loss :   0.01806723323313759  Accuracy :  0.9931818181818182 Time  1.05 s\n",
            "Epoch :  6  Batch :  67  Loss :   0.024878059496925866  Accuracy :  0.9925373134328358 Time  1.05 s\n",
            "Epoch :  6  Batch :  68  Loss :   0.02456933383612469  Accuracy :  0.9926470588235294 Time  1.05 s\n",
            "Epoch :  6  Batch :  69  Loss :   0.024219553443127432  Accuracy :  0.9927536231884058 Time  1.05 s\n",
            "Epoch :  6  Batch :  70  Loss :   0.023889854059338436  Accuracy :  0.9928571428571429 Time  1.05 s\n",
            "Epoch :  6  Batch :  71  Loss :   0.023559785803409398  Accuracy :  0.9929577464788732 Time  1.06 s\n",
            "Epoch :  6  Batch :  72  Loss :   0.023326840789700834  Accuracy :  0.9930555555555556 Time  1.06 s\n",
            "Epoch :  6  Batch :  73  Loss :   0.023138074135353584  Accuracy :  0.9931506849315068 Time  1.06 s\n",
            "Epoch :  6  Batch :  74  Loss :   0.022855912921619852  Accuracy :  0.9932432432432432 Time  1.06 s\n",
            "Epoch :  6  Batch :  75  Loss :   0.02256729328810858  Accuracy :  0.9933333333333333 Time  1.06 s\n",
            "Epoch :  6  Batch :  76  Loss :   0.022363416487151927  Accuracy :  0.993421052631579 Time  1.06 s\n",
            "Epoch :  6  Batch :  77  Loss :   0.02208799366384152  Accuracy :  0.9935064935064936 Time  1.05 s\n",
            "Epoch :  6  Batch :  78  Loss :   0.023027209142291654  Accuracy :  0.992948717948718 Time  1.05 s\n",
            "Epoch :  6  Batch :  79  Loss :   0.023223501790382697  Accuracy :  0.9930379746835443 Time  1.05 s\n",
            "Epoch :  6  Batch :  80  Loss :   0.023004993541326256  Accuracy :  0.993125 Time  1.04 s\n",
            "Epoch :  6  Batch :  81  Loss :   0.02274150946170356  Accuracy :  0.9932098765432099 Time  1.05 s\n",
            "Epoch :  6  Batch :  82  Loss :   0.022537822350675082  Accuracy :  0.9932926829268293 Time  1.07 s\n",
            "Epoch :  6  Batch :  83  Loss :   0.02235664061869562  Accuracy :  0.9933734939759036 Time  1.06 s\n",
            "Epoch :  6  Batch :  84  Loss :   0.0221437254703709  Accuracy :  0.993452380952381 Time  1.05 s\n",
            "Epoch :  6  Batch :  85  Loss :   0.02190559178364792  Accuracy :  0.9935294117647059 Time  1.05 s\n",
            "Epoch :  6  Batch :  86  Loss :   0.02167082628672455  Accuracy :  0.9936046511627907 Time  1.04 s\n",
            "Epoch :  6  Batch :  87  Loss :   0.021437456005389384  Accuracy :  0.9936781609195402 Time  1.04 s\n",
            "Epoch :  6  Batch :  88  Loss :   0.021205645510649032  Accuracy :  0.99375 Time  1.05 s\n",
            "Epoch :  6  Batch :  89  Loss :   0.020984576600067462  Accuracy :  0.9938202247191011 Time  1.05 s\n",
            "Epoch :  6  Batch :  90  Loss :   0.021077169026183482  Accuracy :  0.9938888888888889 Time  1.05 s\n",
            "Epoch :  6  Batch :  91  Loss :   0.020885314754975428  Accuracy :  0.993956043956044 Time  1.05 s\n",
            "Epoch :  6  Batch :  92  Loss :   0.0207387067503473  Accuracy :  0.9940217391304348 Time  1.04 s\n",
            "Epoch :  6  Batch :  93  Loss :   0.021425229515717615  Accuracy :  0.9935483870967742 Time  1.06 s\n",
            "Epoch :  6  Batch :  94  Loss :   0.021207801618382947  Accuracy :  0.9936170212765958 Time  1.06 s\n",
            "Epoch :  6  Batch :  95  Loss :   0.021004101898410895  Accuracy :  0.9936842105263158 Time  1.06 s\n",
            "Epoch :  6  Batch :  96  Loss :   0.020818465248339635  Accuracy :  0.99375 Time  1.05 s\n",
            "Epoch :  6  Batch :  97  Loss :   0.020625197865997834  Accuracy :  0.9938144329896907 Time  1.04 s\n",
            "Epoch :  6  Batch :  98  Loss :   0.02052481851643854  Accuracy :  0.9938775510204082 Time  1.05 s\n",
            "Epoch :  6  Batch :  99  Loss :   0.02033325557736915  Accuracy :  0.9939393939393939 Time  1.05 s\n",
            "Epoch :  6  Batch :  100  Loss :   0.020475169887940865  Accuracy :  0.994 Time  1.04 s\n",
            "Epoch :  6  Batch :  101  Loss :   0.020279316422656308  Accuracy :  0.994059405940594 Time  1.05 s\n",
            "Epoch :  6  Batch :  102  Loss :   0.020260081139984357  Accuracy :  0.9941176470588236 Time  1.04 s\n",
            "Epoch :  6  Batch :  103  Loss :   0.020325491916630144  Accuracy :  0.9941747572815534 Time  1.05 s\n",
            "Epoch :  6  Batch :  104  Loss :   0.020151580954496658  Accuracy :  0.9942307692307693 Time  1.06 s\n",
            "Epoch :  6  Batch :  105  Loss :   0.020002444640323077  Accuracy :  0.9942857142857143 Time  1.06 s\n",
            "Epoch :  6  Batch :  106  Loss :   0.019819147901045036  Accuracy :  0.9943396226415094 Time  1.06 s\n",
            "Epoch :  6  Batch :  107  Loss :   0.01963716089289209  Accuracy :  0.994392523364486 Time  1.05 s\n",
            "Epoch :  6  Batch :  108  Loss :   0.019483700784567865  Accuracy :  0.9944444444444445 Time  1.05 s\n",
            "Epoch :  6  Batch :  109  Loss :   0.019323571732239968  Accuracy :  0.9944954128440368 Time  1.05 s\n",
            "Epoch :  6  Batch :  110  Loss :   0.01915770983402829  Accuracy :  0.9945454545454545 Time  1.04 s\n",
            "Epoch :  6  Batch :  111  Loss :   0.018998278729430912  Accuracy :  0.9945945945945946 Time  1.05 s\n",
            "Epoch :  6  Batch :  112  Loss :   0.018834913847708128  Accuracy :  0.9946428571428572 Time  1.05 s\n",
            "Epoch :  6  Batch :  113  Loss :   0.018677713564520835  Accuracy :  0.9946902654867257 Time  1.05 s\n",
            "Epoch :  6  Batch :  114  Loss :   0.018611163726809696  Accuracy :  0.9947368421052631 Time  1.04 s\n",
            "Epoch :  6  Batch :  115  Loss :   0.018480027514337763  Accuracy :  0.9947826086956522 Time  1.07 s\n",
            "Epoch :  6  Batch :  116  Loss :   0.018331415406056952  Accuracy :  0.9948275862068966 Time  1.07 s\n",
            "Epoch :  6  Batch :  117  Loss :   0.018181558354393557  Accuracy :  0.9948717948717949 Time  1.06 s\n",
            "Epoch :  6  Batch :  118  Loss :   0.018114508988219313  Accuracy :  0.9949152542372881 Time  1.04 s\n",
            "Epoch :  6  Batch :  119  Loss :   0.01796595424735535  Accuracy :  0.9949579831932773 Time  1.05 s\n",
            "Epoch :  6  Batch :  120  Loss :   0.01782295039568756  Accuracy :  0.995 Time  1.05 s\n",
            "Epoch :  6  Batch :  121  Loss :   0.017725410020942915  Accuracy :  0.9950413223140496 Time  1.04 s\n",
            "Epoch :  6  Batch :  122  Loss :   0.017586545131052844  Accuracy :  0.9950819672131147 Time  1.05 s\n",
            "Epoch :  6  Batch :  123  Loss :   0.01766253120196809  Accuracy :  0.9951219512195122 Time  1.04 s\n",
            "Epoch :  6  Batch :  124  Loss :   0.017526387423605492  Accuracy :  0.9951612903225806 Time  1.05 s\n",
            "Epoch :  6  Batch :  125  Loss :   0.017398834717227148  Accuracy :  0.9952 Time  1.04 s\n",
            "Epoch :  6  Batch :  126  Loss :   0.017286356147480315  Accuracy :  0.9952380952380953 Time  1.06 s\n",
            "Epoch :  6  Batch :  127  Loss :   0.0174474476469501  Accuracy :  0.9952755905511811 Time  1.06 s\n",
            "Epoch :  6  Batch :  128  Loss :   0.017334081531544143  Accuracy :  0.9953125 Time  1.07 s\n",
            "Epoch :  6  Batch :  129  Loss :   0.017207676502145004  Accuracy :  0.9953488372093023 Time  1.05 s\n",
            "Epoch :  6  Batch :  130  Loss :   0.017094962589684515  Accuracy :  0.9953846153846154 Time  1.05 s\n",
            "Epoch :  6  Batch :  131  Loss :   0.017102384844480283  Accuracy :  0.9954198473282443 Time  1.05 s\n",
            "Epoch :  6  Batch :  132  Loss :   0.016982343651761767  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  6  Batch :  133  Loss :   0.016887324621254942  Accuracy :  0.9954887218045113 Time  1.05 s\n",
            "Epoch :  6  Batch :  134  Loss :   0.016777104617108646  Accuracy :  0.9955223880597015 Time  1.06 s\n",
            "Epoch :  6  Batch :  135  Loss :   0.016669579145834886  Accuracy :  0.9955555555555555 Time  1.05 s\n",
            "Epoch :  6  Batch :  136  Loss :   0.016789561650084044  Accuracy :  0.9955882352941177 Time  1.04 s\n",
            "Epoch :  6  Batch :  137  Loss :   0.016675891641682067  Accuracy :  0.9956204379562044 Time  1.06 s\n",
            "Epoch :  6  Batch :  138  Loss :   0.01655798975316693  Accuracy :  0.9956521739130435 Time  1.06 s\n",
            "Epoch :  6  Batch :  139  Loss :   0.016455365222394668  Accuracy :  0.99568345323741 Time  1.06 s\n",
            "Epoch :  6  Batch :  140  Loss :   0.016340956510768074  Accuracy :  0.9957142857142857 Time  1.05 s\n",
            "Epoch :  6  Batch :  141  Loss :   0.01623402974144754  Accuracy :  0.9957446808510638 Time  1.04 s\n",
            "Epoch :  6  Batch :  142  Loss :   0.016183342938565545  Accuracy :  0.995774647887324 Time  1.05 s\n",
            "Epoch :  6  Batch :  143  Loss :   0.016087983293463705  Accuracy :  0.9958041958041958 Time  1.05 s\n",
            "Epoch :  6  Batch :  144  Loss :   0.01601222101352404  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  6  Batch :  145  Loss :   0.015905750956153496  Accuracy :  0.9958620689655172 Time  1.04 s\n",
            "Epoch :  6  Batch :  146  Loss :   0.015925401198790944  Accuracy :  0.9958904109589041 Time  1.04 s\n",
            "Epoch :  6  Batch :  147  Loss :   0.01587144520706307  Accuracy :  0.9959183673469387 Time  1.05 s\n",
            "Epoch :  6  Batch :  148  Loss :   0.015838390828788317  Accuracy :  0.995945945945946 Time  1.06 s\n",
            "Epoch :  6  Batch :  149  Loss :   0.015750176341065063  Accuracy :  0.9959731543624161 Time  1.07 s\n",
            "Epoch :  6  Batch :  150  Loss :   0.016224878168626066  Accuracy :  0.9956666666666667 Time  1.06 s\n",
            "Epoch :  6  Batch :  151  Loss :   0.016263691732152644  Accuracy :  0.9956953642384105 Time  1.05 s\n",
            "Epoch :  6  Batch :  152  Loss :   0.016170697319466854  Accuracy :  0.9957236842105263 Time  1.05 s\n",
            "Epoch :  6  Batch :  153  Loss :   0.016102761625779453  Accuracy :  0.9957516339869281 Time  1.05 s\n",
            "Epoch :  6  Batch :  154  Loss :   0.01610896885058655  Accuracy :  0.9957792207792208 Time  1.06 s\n",
            "Epoch :  6  Batch :  155  Loss :   0.0160344602981387  Accuracy :  0.9958064516129033 Time  1.04 s\n",
            "Epoch :  6  Batch :  156  Loss :   0.01606841282885468  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  6  Batch :  157  Loss :   0.017902667096417076  Accuracy :  0.9952229299363057 Time  1.05 s\n",
            "Epoch :  6  Batch :  158  Loss :   0.01779247751688626  Accuracy :  0.995253164556962 Time  1.04 s\n",
            "Epoch :  6  Batch :  159  Loss :   0.017684380382368248  Accuracy :  0.9952830188679245 Time  1.05 s\n",
            "Epoch :  6  Batch :  160  Loss :   0.017594537567310907  Accuracy :  0.9953125 Time  1.07 s\n",
            "Epoch :  6  Batch :  161  Loss :   0.017486954525276523  Accuracy :  0.9953416149068323 Time  1.06 s\n",
            "Epoch :  6  Batch :  162  Loss :   0.017460792889728002  Accuracy :  0.9953703703703703 Time  1.05 s\n",
            "Epoch :  6  Batch :  163  Loss :   0.017356720815685525  Accuracy :  0.995398773006135 Time  1.05 s\n",
            "Epoch :  6  Batch :  164  Loss :   0.017279651526911126  Accuracy :  0.9954268292682927 Time  1.06 s\n",
            "Epoch :  6  Batch :  165  Loss :   0.017178437285739083  Accuracy :  0.9954545454545455 Time  1.04 s\n",
            "Epoch :  6  Batch :  166  Loss :   0.017083019392490555  Accuracy :  0.9954819277108434 Time  1.05 s\n",
            "Epoch :  6  Batch :  167  Loss :   0.01699734079873909  Accuracy :  0.9955089820359282 Time  1.05 s\n",
            "Epoch :  6  Batch :  168  Loss :   0.01698455313158948  Accuracy :  0.9955357142857143 Time  1.05 s\n",
            "Epoch :  6  Batch :  169  Loss :   0.016926865043640743  Accuracy :  0.9955621301775148 Time  1.05 s\n",
            "Epoch :  6  Batch :  170  Loss :   0.01716663501043941  Accuracy :  0.9955882352941177 Time  1.06 s\n",
            "Epoch :  6  Batch :  171  Loss :   0.0175181508868625  Accuracy :  0.9953216374269006 Time  1.06 s\n",
            "Epoch :  6  Batch :  172  Loss :   0.017418375576685217  Accuracy :  0.9953488372093023 Time  1.07 s\n",
            "Epoch :  6  Batch :  173  Loss :   0.01732146614409493  Accuracy :  0.9953757225433526 Time  1.05 s\n",
            "Epoch :  6  Batch :  174  Loss :   0.017224409978103494  Accuracy :  0.9954022988505747 Time  1.06 s\n",
            "Epoch :  6  Batch :  175  Loss :   0.017129522127964134  Accuracy :  0.9954285714285714 Time  1.05 s\n",
            "Epoch :  6  Batch :  176  Loss :   0.01704930385610417  Accuracy :  0.9954545454545455 Time  1.04 s\n",
            "Epoch :  6  Batch :  177  Loss :   0.016969235691223505  Accuracy :  0.9954802259887006 Time  1.05 s\n",
            "Epoch :  6  Batch :  178  Loss :   0.016893575446395036  Accuracy :  0.9955056179775281 Time  1.04 s\n",
            "Epoch :  6  Batch :  179  Loss :   0.016810179120593044  Accuracy :  0.9955307262569832 Time  1.04 s\n",
            "Epoch :  6  Batch :  180  Loss :   0.016724370490798415  Accuracy :  0.9955555555555555 Time  1.04 s\n",
            "Epoch :  6  Batch :  181  Loss :   0.016633307860436777  Accuracy :  0.9955801104972376 Time  1.07 s\n",
            "Epoch :  6  Batch :  182  Loss :   0.016762191184830293  Accuracy :  0.9953296703296703 Time  1.06 s\n",
            "Epoch :  6  Batch :  183  Loss :   0.016678990447041936  Accuracy :  0.9953551912568306 Time  1.06 s\n",
            "Epoch :  6  Batch :  184  Loss :   0.01670989863590006  Accuracy :  0.9953804347826087 Time  1.05 s\n",
            "Epoch :  6  Batch :  185  Loss :   0.016631681865436728  Accuracy :  0.9954054054054055 Time  1.05 s\n",
            "Epoch :  6  Batch :  186  Loss :   0.016552262050828832  Accuracy :  0.9954301075268818 Time  1.05 s\n",
            "Epoch :  6  Batch :  187  Loss :   0.01647228002991779  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  6  Batch :  188  Loss :   0.016390287010893945  Accuracy :  0.9954787234042554 Time  1.05 s\n",
            "Epoch :  6  Batch :  189  Loss :   0.016314250587749976  Accuracy :  0.9955026455026456 Time  1.04 s\n",
            "Epoch :  6  Batch :  190  Loss :   0.016230119632229232  Accuracy :  0.9955263157894737 Time  1.05 s\n",
            "Epoch :  6  Batch :  191  Loss :   0.016544673206416904  Accuracy :  0.9952879581151832 Time  1.05 s\n",
            "Epoch :  6  Batch :  192  Loss :   0.016468560311371522  Accuracy :  0.9953125 Time  1.06 s\n",
            "Epoch :  6  Batch :  193  Loss :   0.016385727558768153  Accuracy :  0.9953367875647668 Time  1.06 s\n",
            "Epoch :  6  Batch :  194  Loss :   0.016419806653653515  Accuracy :  0.9953608247422681 Time  1.07 s\n",
            "Epoch :  6  Batch :  195  Loss :   0.016391814090923454  Accuracy :  0.9953846153846154 Time  1.05 s\n",
            "Epoch :  6  Batch :  196  Loss :   0.016316204961779532  Accuracy :  0.9954081632653061 Time  1.05 s\n",
            "Epoch :  6  Batch :  197  Loss :   0.01654131820481235  Accuracy :  0.9951776649746192 Time  1.05 s\n",
            "Epoch :  6  Batch :  198  Loss :   0.016459968607714565  Accuracy :  0.9952020202020202 Time  1.05 s\n",
            "Epoch :  6  Batch :  199  Loss :   0.01639609566961951  Accuracy :  0.9952261306532664 Time  1.05 s\n",
            "Epoch :  6  Batch :  200  Loss :   0.016381702453290927  Accuracy :  0.99525 Time  1.05 s\n",
            "Epoch :  6  Batch :  201  Loss :   0.016300788449398033  Accuracy :  0.995273631840796 Time  1.05 s\n",
            "Epoch :  6  Batch :  202  Loss :   0.016222748193817242  Accuracy :  0.9952970297029703 Time  1.06 s\n",
            "Epoch :  6  Batch :  203  Loss :   0.01614925804392554  Accuracy :  0.9953201970443349 Time  1.07 s\n",
            "Epoch :  6  Batch :  204  Loss :   0.016119685138298392  Accuracy :  0.995343137254902 Time  1.07 s\n",
            "Epoch :  6  Batch :  205  Loss :   0.01604928858478826  Accuracy :  0.9953658536585366 Time  1.06 s\n",
            "Epoch :  6  Batch :  206  Loss :   0.01598232428136946  Accuracy :  0.9953883495145631 Time  1.05 s\n",
            "Epoch :  6  Batch :  207  Loss :   0.015916265488927283  Accuracy :  0.9954106280193237 Time  1.05 s\n",
            "Epoch :  6  Batch :  208  Loss :   0.015842458231977965  Accuracy :  0.9954326923076923 Time  1.05 s\n",
            "Epoch :  6  Batch :  209  Loss :   0.015769524939116493  Accuracy :  0.9954545454545455 Time  1.04 s\n",
            "Epoch :  6  Batch :  210  Loss :   0.015714833422403207  Accuracy :  0.9954761904761905 Time  1.05 s\n",
            "Epoch :  6  Batch :  211  Loss :   0.015643484892529962  Accuracy :  0.9954976303317535 Time  1.04 s\n",
            "Epoch :  6  Batch :  212  Loss :   0.015581036862815107  Accuracy :  0.9955188679245283 Time  1.04 s\n",
            "Epoch :  6  Batch :  213  Loss :   0.01551316094028921  Accuracy :  0.9955399061032864 Time  1.05 s\n",
            "Epoch :  6  Batch :  214  Loss :   0.015444204213275034  Accuracy :  0.9955607476635514 Time  1.06 s\n",
            "Epoch :  6  Batch :  215  Loss :   0.015383171568337697  Accuracy :  0.9955813953488372 Time  1.06 s\n",
            "Epoch :  6  Batch :  216  Loss :   0.015313505518052908  Accuracy :  0.9956018518518519 Time  1.06 s\n",
            "Epoch :  6  Batch :  217  Loss :   0.015243461463365152  Accuracy :  0.9956221198156682 Time  1.05 s\n",
            "Epoch :  6  Batch :  218  Loss :   0.015183627092216214  Accuracy :  0.9956422018348624 Time  1.05 s\n",
            "Epoch :  6  Batch :  219  Loss :   0.015132519077863405  Accuracy :  0.995662100456621 Time  1.05 s\n",
            "Epoch :  6  Batch :  220  Loss :   0.015070483633495646  Accuracy :  0.9956818181818182 Time  1.05 s\n",
            "Epoch :  6  Batch :  221  Loss :   0.015013967074918175  Accuracy :  0.9957013574660634 Time  1.04 s\n",
            "Epoch :  6  Batch :  222  Loss :   0.015347352917357254  Accuracy :  0.9954954954954955 Time  1.04 s\n",
            "Epoch :  6  Batch :  223  Loss :   0.01528722465488169  Accuracy :  0.9955156950672646 Time  1.05 s\n",
            "Epoch :  6  Batch :  224  Loss :   0.01522300764681209  Accuracy :  0.9955357142857143 Time  1.06 s\n",
            "Epoch :  6  Batch :  225  Loss :   0.01516773251580566  Accuracy :  0.9955555555555555 Time  1.06 s\n",
            "Epoch :  6  Batch :  226  Loss :   0.015116555883828087  Accuracy :  0.995575221238938 Time  1.05 s\n",
            "Epoch :  6  Batch :  227  Loss :   0.015115840167404892  Accuracy :  0.9955947136563876 Time  1.06 s\n",
            "Epoch :  6  Batch :  228  Loss :   0.015056689856736053  Accuracy :  0.9956140350877193 Time  1.05 s\n",
            "Epoch :  6  Batch :  229  Loss :   0.01499228664583249  Accuracy :  0.9956331877729258 Time  1.05 s\n",
            "Epoch :  6  Batch :  230  Loss :   0.01493680677930293  Accuracy :  0.9956521739130435 Time  1.05 s\n",
            "Epoch :  6  Batch :  231  Loss :   0.014874992210505424  Accuracy :  0.9956709956709957 Time  1.05 s\n",
            "Epoch :  6  Batch :  232  Loss :   0.014816374396261756  Accuracy :  0.9956896551724138 Time  1.04 s\n",
            "Epoch :  6  Batch :  233  Loss :   0.0147551367480731  Accuracy :  0.9957081545064378 Time  1.05 s\n",
            "Epoch :  6  Batch :  234  Loss :   0.014693984747499887  Accuracy :  0.9957264957264957 Time  1.05 s\n",
            "Epoch :  6  Batch :  235  Loss :   0.014632675970967691  Accuracy :  0.9957446808510638 Time  1.05 s\n",
            "Epoch :  6  Batch :  236  Loss :   0.014579501071932748  Accuracy :  0.9957627118644068 Time  1.05 s\n",
            "Epoch :  6  Batch :  237  Loss :   0.014703055951622255  Accuracy :  0.9955696202531645 Time  1.06 s\n",
            "Epoch :  6  Batch :  238  Loss :   0.01464613814218368  Accuracy :  0.9955882352941177 Time  1.06 s\n",
            "Epoch :  6  Batch :  239  Loss :   0.014587316581550509  Accuracy :  0.9956066945606694 Time  1.05 s\n",
            "Epoch :  6  Batch :  240  Loss :   0.014555425824892155  Accuracy :  0.995625 Time  1.05 s\n",
            "Epoch :  6  Batch :  241  Loss :   0.014553214016961488  Accuracy :  0.995643153526971 Time  1.04 s\n",
            "Epoch :  6  Batch :  242  Loss :   0.014536443200439344  Accuracy :  0.9956611570247934 Time  1.05 s\n",
            "Epoch :  6  Batch :  243  Loss :   0.014483722528856421  Accuracy :  0.995679012345679 Time  1.05 s\n",
            "Epoch :  6  Batch :  244  Loss :   0.014435401194308814  Accuracy :  0.9956967213114755 Time  1.04 s\n",
            "Epoch :  6  Batch :  245  Loss :   0.014395923506110972  Accuracy :  0.9957142857142857 Time  1.04 s\n",
            "Epoch :  6  Batch :  246  Loss :   0.014341655090621368  Accuracy :  0.9957317073170732 Time  1.05 s\n",
            "Epoch :  6  Batch :  247  Loss :   0.014313801199140976  Accuracy :  0.995748987854251 Time  1.06 s\n",
            "Epoch :  6  Batch :  248  Loss :   0.014282610423100367  Accuracy :  0.995766129032258 Time  1.06 s\n",
            "Epoch :  6  Batch :  249  Loss :   0.014227916024961691  Accuracy :  0.9957831325301205 Time  1.06 s\n",
            "Epoch :  6  Batch :  250  Loss :   0.01417141959926812  Accuracy :  0.9958 Time  1.05 s\n",
            "Epoch :  6  Batch :  251  Loss :   0.014121383105938516  Accuracy :  0.9958167330677291 Time  1.04 s\n",
            "Epoch :  6  Batch :  252  Loss :   0.014073705246350045  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  6  Batch :  253  Loss :   0.014022979910819968  Accuracy :  0.9958498023715415 Time  1.04 s\n",
            "Epoch :  6  Batch :  254  Loss :   0.013969116014376646  Accuracy :  0.9958661417322835 Time  1.04 s\n",
            "Epoch :  6  Batch :  255  Loss :   0.01399651400478986  Accuracy :  0.9958823529411764 Time  1.05 s\n",
            "Epoch :  6  Batch :  256  Loss :   0.01394331274804017  Accuracy :  0.9958984375 Time  1.04 s\n",
            "Epoch :  6  Batch :  257  Loss :   0.013920118680922768  Accuracy :  0.9959143968871595 Time  1.04 s\n",
            "Epoch :  6  Batch :  258  Loss :   0.01387029883690682  Accuracy :  0.9959302325581395 Time  1.05 s\n",
            "Epoch :  6  Batch :  259  Loss :   0.013834270429031107  Accuracy :  0.995945945945946 Time  1.08 s\n",
            "Epoch :  6  Batch :  260  Loss :   0.013786088080814806  Accuracy :  0.9959615384615385 Time  1.07 s\n",
            "Epoch :  6  Batch :  261  Loss :   0.013742120459421995  Accuracy :  0.9959770114942529 Time  1.05 s\n",
            "Epoch :  6  Batch :  262  Loss :   0.013690351443518512  Accuracy :  0.9959923664122138 Time  1.04 s\n",
            "Epoch :  6  Batch :  263  Loss :   0.013654803542764202  Accuracy :  0.9960076045627376 Time  1.05 s\n",
            "Epoch :  6  Batch :  264  Loss :   0.013604444855059917  Accuracy :  0.9960227272727272 Time  1.05 s\n",
            "Epoch :  6  Batch :  265  Loss :   0.013555333758254079  Accuracy :  0.9960377358490566 Time  1.04 s\n",
            "Epoch :  6  Batch :  266  Loss :   0.013519839404833143  Accuracy :  0.9960526315789474 Time  1.04 s\n",
            "Epoch :  6  Batch :  267  Loss :   0.013471304885430474  Accuracy :  0.9960674157303371 Time  1.05 s\n",
            "Epoch :  6  Batch :  268  Loss :   0.013437695319904623  Accuracy :  0.9960820895522388 Time  1.04 s\n",
            "Epoch :  6  Batch :  269  Loss :   0.013389163312091506  Accuracy :  0.996096654275093 Time  1.07 s\n",
            "Epoch :  6  Batch :  270  Loss :   0.013358549671021238  Accuracy :  0.9961111111111111 Time  1.06 s\n",
            "Epoch :  6  Batch :  271  Loss :   0.013310738728539348  Accuracy :  0.9961254612546125 Time  1.07 s\n",
            "Epoch :  6  Batch :  272  Loss :   0.013269710197164386  Accuracy :  0.9961397058823529 Time  1.05 s\n",
            "Epoch :  6  Batch :  273  Loss :   0.013227421453641813  Accuracy :  0.9961538461538462 Time  1.04 s\n",
            "Epoch :  6  Batch :  274  Loss :   0.013185753991109758  Accuracy :  0.9961678832116788 Time  1.05 s\n",
            "Epoch :  6  Batch :  275  Loss :   0.013140599517395128  Accuracy :  0.9961818181818182 Time  1.05 s\n",
            "Epoch :  6  Batch :  276  Loss :   0.01311397443640854  Accuracy :  0.996195652173913 Time  1.06 s\n",
            "Epoch :  6  Batch :  277  Loss :   0.0130672783477695  Accuracy :  0.9962093862815884 Time  1.06 s\n",
            "Epoch :  6  Batch :  278  Loss :   0.013024102468007972  Accuracy :  0.9962230215827338 Time  1.06 s\n",
            "Epoch :  6  Batch :  279  Loss :   0.012978000226975117  Accuracy :  0.9962365591397849 Time  1.05 s\n",
            "Epoch :  6  Batch :  280  Loss :   0.013231689071575861  Accuracy :  0.9960714285714286 Time  1.06 s\n",
            "Epoch :  6  Batch :  281  Loss :   0.013195865528479473  Accuracy :  0.9960854092526691 Time  1.06 s\n",
            "Epoch :  6  Batch :  282  Loss :   0.013149544828109433  Accuracy :  0.9960992907801418 Time  1.06 s\n",
            "Epoch :  6  Batch :  283  Loss :   0.013105032326130759  Accuracy :  0.996113074204947 Time  1.04 s\n",
            "Epoch :  6  Batch :  284  Loss :   0.013062562317840351  Accuracy :  0.9961267605633802 Time  1.04 s\n",
            "Epoch :  6  Batch :  285  Loss :   0.01304879591115056  Accuracy :  0.996140350877193 Time  1.05 s\n",
            "Epoch :  6  Batch :  286  Loss :   0.013032169647726891  Accuracy :  0.9961538461538462 Time  1.05 s\n",
            "Epoch :  6  Batch :  287  Loss :   0.012989583851184245  Accuracy :  0.9961672473867595 Time  1.04 s\n",
            "Epoch :  6  Batch :  288  Loss :   0.013382857304097544  Accuracy :  0.9960069444444445 Time  1.05 s\n",
            "Epoch :  6  Batch :  289  Loss :   0.01335872437851866  Accuracy :  0.9960207612456747 Time  1.04 s\n",
            "Epoch :  6  Batch :  290  Loss :   0.013319227567137267  Accuracy :  0.9960344827586207 Time  1.05 s\n",
            "Epoch :  6  Batch :  291  Loss :   0.013280755582257955  Accuracy :  0.9960481099656358 Time  1.06 s\n",
            "Epoch :  6  Batch :  292  Loss :   0.013236037015004724  Accuracy :  0.9960616438356165 Time  1.07 s\n",
            "Epoch :  6  Batch :  293  Loss :   0.01319305309708947  Accuracy :  0.9960750853242321 Time  1.06 s\n",
            "Epoch :  6  Batch :  294  Loss :   0.01320464273845994  Accuracy :  0.9960884353741497 Time  1.05 s\n",
            "Epoch :  6  Batch :  295  Loss :   0.013656717976722946  Accuracy :  0.9959322033898305 Time  1.04 s\n",
            "Epoch :  6  Batch :  296  Loss :   0.013611724362694862  Accuracy :  0.995945945945946 Time  1.05 s\n",
            "Epoch :  6  Batch :  297  Loss :   0.01356881786838859  Accuracy :  0.9959595959595959 Time  1.04 s\n",
            "Epoch :  6  Batch :  298  Loss :   0.013545122769578819  Accuracy :  0.9959731543624161 Time  1.04 s\n",
            "Epoch :  6  Batch :  299  Loss :   0.01351281370813639  Accuracy :  0.9959866220735786 Time  1.04 s\n",
            "Epoch :  6  Batch :  300  Loss :   0.013471569758888411  Accuracy :  0.996 Time  1.05 s\n",
            "Epoch :  6  Batch :  301  Loss :   0.013427482548305544  Accuracy :  0.9960132890365448 Time  1.04 s\n",
            "Epoch :  6  Batch :  302  Loss :   0.013446147320936635  Accuracy :  0.9960264900662251 Time  1.05 s\n",
            "Epoch :  6  Batch :  303  Loss :   0.013404463523437528  Accuracy :  0.996039603960396 Time  1.07 s\n",
            "Epoch :  6  Batch :  304  Loss :   0.01337316680708586  Accuracy :  0.9960526315789474 Time  1.06 s\n",
            "Epoch :  6  Batch :  305  Loss :   0.013331269093771985  Accuracy :  0.9960655737704918 Time  1.05 s\n",
            "Epoch :  6  Batch :  306  Loss :   0.013288437355405977  Accuracy :  0.996078431372549 Time  1.04 s\n",
            "Epoch :  6  Batch :  307  Loss :   0.013248628188208647  Accuracy :  0.9960848287112561 Time  0.54 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 98 %\n",
            "[6 epoch] Accuracy of the network on the Training images: 99 %\n",
            "Epoch :  7  Batch :  1  Loss :   0.0006639615166932344  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  7  Batch :  2  Loss :   0.004156426643021405  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  7  Batch :  3  Loss :   0.005120122882847984  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  7  Batch :  4  Loss :   0.004612700606230646  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  7  Batch :  5  Loss :   0.02369946804828942  Accuracy :  0.98 Time  1.05 s\n",
            "Epoch :  7  Batch :  6  Loss :   0.019926049242106576  Accuracy :  0.9833333333333333 Time  1.06 s\n",
            "Epoch :  7  Batch :  7  Loss :   0.017306691451397325  Accuracy :  0.9857142857142858 Time  1.07 s\n",
            "Epoch :  7  Batch :  8  Loss :   0.015186104865279049  Accuracy :  0.9875 Time  1.06 s\n",
            "Epoch :  7  Batch :  9  Loss :   0.01376419235020876  Accuracy :  0.9888888888888889 Time  1.05 s\n",
            "Epoch :  7  Batch :  10  Loss :   0.0126549071399495  Accuracy :  0.99 Time  1.04 s\n",
            "Epoch :  7  Batch :  11  Loss :   0.011724676661701365  Accuracy :  0.990909090909091 Time  1.05 s\n",
            "Epoch :  7  Batch :  12  Loss :   0.012623916593535492  Accuracy :  0.9916666666666667 Time  1.04 s\n",
            "Epoch :  7  Batch :  13  Loss :   0.01178889898941494  Accuracy :  0.9923076923076923 Time  1.05 s\n",
            "Epoch :  7  Batch :  14  Loss :   0.010977124672665792  Accuracy :  0.9928571428571429 Time  1.05 s\n",
            "Epoch :  7  Batch :  15  Loss :   0.010645353959019607  Accuracy :  0.9933333333333333 Time  1.05 s\n",
            "Epoch :  7  Batch :  16  Loss :   0.010745959569248953  Accuracy :  0.99375 Time  1.05 s\n",
            "Epoch :  7  Batch :  17  Loss :   0.010853106684996472  Accuracy :  0.9941176470588236 Time  1.06 s\n",
            "Epoch :  7  Batch :  18  Loss :   0.010407159164767817  Accuracy :  0.9944444444444445 Time  1.06 s\n",
            "Epoch :  7  Batch :  19  Loss :   0.00988574185267728  Accuracy :  0.9947368421052631 Time  1.06 s\n",
            "Epoch :  7  Batch :  20  Loss :   0.009405048050393817  Accuracy :  0.995 Time  1.05 s\n",
            "Epoch :  7  Batch :  21  Loss :   0.009004017591775795  Accuracy :  0.9952380952380953 Time  1.04 s\n",
            "Epoch :  7  Batch :  22  Loss :   0.008659471348933452  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  7  Batch :  23  Loss :   0.008361104920091431  Accuracy :  0.9956521739130435 Time  1.05 s\n",
            "Epoch :  7  Batch :  24  Loss :   0.008026603101219129  Accuracy :  0.9958333333333333 Time  1.04 s\n",
            "Epoch :  7  Batch :  25  Loss :   0.00787803999730386  Accuracy :  0.996 Time  1.05 s\n",
            "Epoch :  7  Batch :  26  Loss :   0.008378651347951606  Accuracy :  0.9961538461538462 Time  1.05 s\n",
            "Epoch :  7  Batch :  27  Loss :   0.008081837370776123  Accuracy :  0.9962962962962963 Time  1.05 s\n",
            "Epoch :  7  Batch :  28  Loss :   0.007892345189507719  Accuracy :  0.9964285714285714 Time  1.06 s\n",
            "Epoch :  7  Batch :  29  Loss :   0.007644662688735166  Accuracy :  0.996551724137931 Time  1.06 s\n",
            "Epoch :  7  Batch :  30  Loss :   0.007441630166916487  Accuracy :  0.9966666666666667 Time  1.06 s\n",
            "Epoch :  7  Batch :  31  Loss :   0.007218856928727379  Accuracy :  0.9967741935483871 Time  1.05 s\n",
            "Epoch :  7  Batch :  32  Loss :   0.007021389411420387  Accuracy :  0.996875 Time  1.05 s\n",
            "Epoch :  7  Batch :  33  Loss :   0.008445277560509347  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  7  Batch :  34  Loss :   0.011016741152750054  Accuracy :  0.9941176470588236 Time  1.04 s\n",
            "Epoch :  7  Batch :  35  Loss :   0.010716542120124878  Accuracy :  0.9942857142857143 Time  1.04 s\n",
            "Epoch :  7  Batch :  36  Loss :   0.010431900060211774  Accuracy :  0.9944444444444445 Time  1.05 s\n",
            "Epoch :  7  Batch :  37  Loss :   0.010333151867254512  Accuracy :  0.9945945945945946 Time  1.05 s\n",
            "Epoch :  7  Batch :  38  Loss :   0.010066076632938348  Accuracy :  0.9947368421052631 Time  1.04 s\n",
            "Epoch :  7  Batch :  39  Loss :   0.009820620219659012  Accuracy :  0.9948717948717949 Time  1.06 s\n",
            "Epoch :  7  Batch :  40  Loss :   0.009663969168468612  Accuracy :  0.995 Time  1.06 s\n",
            "Epoch :  7  Batch :  41  Loss :   0.00944521678852985  Accuracy :  0.9951219512195122 Time  1.06 s\n",
            "Epoch :  7  Batch :  42  Loss :   0.00923234433104794  Accuracy :  0.9952380952380953 Time  1.05 s\n",
            "Epoch :  7  Batch :  43  Loss :   0.009042297856107917  Accuracy :  0.9953488372093023 Time  1.05 s\n",
            "Epoch :  7  Batch :  44  Loss :   0.008857985949858134  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  7  Batch :  45  Loss :   0.008687436499813985  Accuracy :  0.9955555555555555 Time  1.05 s\n",
            "Epoch :  7  Batch :  46  Loss :   0.008861268386523158  Accuracy :  0.9956521739130435 Time  1.05 s\n",
            "Epoch :  7  Batch :  47  Loss :   0.008679816612671625  Accuracy :  0.9957446808510638 Time  1.05 s\n",
            "Epoch :  7  Batch :  48  Loss :   0.008506559148978946  Accuracy :  0.9958333333333333 Time  1.04 s\n",
            "Epoch :  7  Batch :  49  Loss :   0.00836627442705711  Accuracy :  0.9959183673469387 Time  1.05 s\n",
            "Epoch :  7  Batch :  50  Loss :   0.00837496735912282  Accuracy :  0.996 Time  1.07 s\n",
            "Epoch :  7  Batch :  51  Loss :   0.008220635314001794  Accuracy :  0.996078431372549 Time  1.06 s\n",
            "Epoch :  7  Batch :  52  Loss :   0.00807413670106773  Accuracy :  0.9961538461538462 Time  1.06 s\n",
            "Epoch :  7  Batch :  53  Loss :   0.00794301071272337  Accuracy :  0.9962264150943396 Time  1.05 s\n",
            "Epoch :  7  Batch :  54  Loss :   0.007825075592134252  Accuracy :  0.9962962962962963 Time  1.05 s\n",
            "Epoch :  7  Batch :  55  Loss :   0.007703749970427122  Accuracy :  0.9963636363636363 Time  1.05 s\n",
            "Epoch :  7  Batch :  56  Loss :   0.008257538079695743  Accuracy :  0.9964285714285714 Time  1.04 s\n",
            "Epoch :  7  Batch :  57  Loss :   0.008117502509297705  Accuracy :  0.9964912280701754 Time  1.05 s\n",
            "Epoch :  7  Batch :  58  Loss :   0.008100363300462928  Accuracy :  0.996551724137931 Time  1.05 s\n",
            "Epoch :  7  Batch :  59  Loss :   0.008078853413265325  Accuracy :  0.9966101694915255 Time  1.05 s\n",
            "Epoch :  7  Batch :  60  Loss :   0.007961202681569073  Accuracy :  0.9966666666666667 Time  1.05 s\n",
            "Epoch :  7  Batch :  61  Loss :   0.007837698685165618  Accuracy :  0.9967213114754099 Time  1.06 s\n",
            "Epoch :  7  Batch :  62  Loss :   0.007732073942597415  Accuracy :  0.9967741935483871 Time  1.06 s\n",
            "Epoch :  7  Batch :  63  Loss :   0.007628247484616522  Accuracy :  0.9968253968253968 Time  1.06 s\n",
            "Epoch :  7  Batch :  64  Loss :   0.007620203296937689  Accuracy :  0.996875 Time  1.05 s\n",
            "Epoch :  7  Batch :  65  Loss :   0.007520146006180977  Accuracy :  0.9969230769230769 Time  1.04 s\n",
            "Epoch :  7  Batch :  66  Loss :   0.007460211598575397  Accuracy :  0.996969696969697 Time  1.05 s\n",
            "Epoch :  7  Batch :  67  Loss :   0.0073558560836671  Accuracy :  0.9970149253731343 Time  1.05 s\n",
            "Epoch :  7  Batch :  68  Loss :   0.007521431126138743  Accuracy :  0.9970588235294118 Time  1.04 s\n",
            "Epoch :  7  Batch :  69  Loss :   0.007426220239511273  Accuracy :  0.9971014492753624 Time  1.05 s\n",
            "Epoch :  7  Batch :  70  Loss :   0.007359646461139034  Accuracy :  0.9971428571428571 Time  1.06 s\n",
            "Epoch :  7  Batch :  71  Loss :   0.00739841622343077  Accuracy :  0.9971830985915493 Time  1.04 s\n",
            "Epoch :  7  Batch :  72  Loss :   0.00735646097665368  Accuracy :  0.9972222222222222 Time  1.05 s\n",
            "Epoch :  7  Batch :  73  Loss :   0.007262944031749176  Accuracy :  0.9972602739726028 Time  1.06 s\n",
            "Epoch :  7  Batch :  74  Loss :   0.0071958529155676226  Accuracy :  0.9972972972972973 Time  1.07 s\n",
            "Epoch :  7  Batch :  75  Loss :   0.0071440925464654964  Accuracy :  0.9973333333333333 Time  1.05 s\n",
            "Epoch :  7  Batch :  76  Loss :   0.007056439418719444  Accuracy :  0.9973684210526316 Time  1.05 s\n",
            "Epoch :  7  Batch :  77  Loss :   0.00698479600840357  Accuracy :  0.9974025974025974 Time  1.04 s\n",
            "Epoch :  7  Batch :  78  Loss :   0.006897890632666158  Accuracy :  0.9974358974358974 Time  1.05 s\n",
            "Epoch :  7  Batch :  79  Loss :   0.006830665058731927  Accuracy :  0.9974683544303797 Time  1.05 s\n",
            "Epoch :  7  Batch :  80  Loss :   0.006761071320033807  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  7  Batch :  81  Loss :   0.006683970321319066  Accuracy :  0.9975308641975309 Time  1.05 s\n",
            "Epoch :  7  Batch :  82  Loss :   0.006607575177906434  Accuracy :  0.9975609756097561 Time  1.04 s\n",
            "Epoch :  7  Batch :  83  Loss :   0.006533016181931023  Accuracy :  0.9975903614457832 Time  1.06 s\n",
            "Epoch :  7  Batch :  84  Loss :   0.006509082508793134  Accuracy :  0.9976190476190476 Time  1.06 s\n",
            "Epoch :  7  Batch :  85  Loss :   0.006465049778789227  Accuracy :  0.9976470588235294 Time  1.07 s\n",
            "Epoch :  7  Batch :  86  Loss :   0.006419074381018046  Accuracy :  0.9976744186046511 Time  1.05 s\n",
            "Epoch :  7  Batch :  87  Loss :   0.006347976235576488  Accuracy :  0.9977011494252873 Time  1.04 s\n",
            "Epoch :  7  Batch :  88  Loss :   0.007037936578853987  Accuracy :  0.9971590909090909 Time  1.04 s\n",
            "Epoch :  7  Batch :  89  Loss :   0.006961567137224255  Accuracy :  0.9971910112359551 Time  1.05 s\n",
            "Epoch :  7  Batch :  90  Loss :   0.0069032433114544905  Accuracy :  0.9972222222222222 Time  1.05 s\n",
            "Epoch :  7  Batch :  91  Loss :   0.006832013743165829  Accuracy :  0.9972527472527473 Time  1.04 s\n",
            "Epoch :  7  Batch :  92  Loss :   0.006803654997107198  Accuracy :  0.9972826086956522 Time  1.05 s\n",
            "Epoch :  7  Batch :  93  Loss :   0.0067930875409592285  Accuracy :  0.9973118279569892 Time  1.05 s\n",
            "Epoch :  7  Batch :  94  Loss :   0.0067316420917198595  Accuracy :  0.9973404255319149 Time  1.07 s\n",
            "Epoch :  7  Batch :  95  Loss :   0.006973346889993225  Accuracy :  0.9973684210526316 Time  1.06 s\n",
            "Epoch :  7  Batch :  96  Loss :   0.006921390162309156  Accuracy :  0.9973958333333334 Time  1.06 s\n",
            "Epoch :  7  Batch :  97  Loss :   0.00687227566044216  Accuracy :  0.9974226804123711 Time  1.04 s\n",
            "Epoch :  7  Batch :  98  Loss :   0.006809410438201704  Accuracy :  0.9974489795918368 Time  1.04 s\n",
            "Epoch :  7  Batch :  99  Loss :   0.006844722062456328  Accuracy :  0.9974747474747475 Time  1.04 s\n",
            "Epoch :  7  Batch :  100  Loss :   0.006794688334775856  Accuracy :  0.9975 Time  1.04 s\n",
            "Epoch :  7  Batch :  101  Loss :   0.006733346281378383  Accuracy :  0.9975247524752475 Time  1.04 s\n",
            "Epoch :  7  Batch :  102  Loss :   0.00667142541808026  Accuracy :  0.9975490196078431 Time  1.05 s\n",
            "Epoch :  7  Batch :  103  Loss :   0.006614083107882993  Accuracy :  0.9975728155339806 Time  1.05 s\n",
            "Epoch :  7  Batch :  104  Loss :   0.006552806277306025  Accuracy :  0.9975961538461539 Time  1.05 s\n",
            "Epoch :  7  Batch :  105  Loss :   0.0064951002662945985  Accuracy :  0.9976190476190476 Time  1.06 s\n",
            "Epoch :  7  Batch :  106  Loss :   0.006848374142103942  Accuracy :  0.9971698113207547 Time  1.06 s\n",
            "Epoch :  7  Batch :  107  Loss :   0.006892641229314564  Accuracy :  0.997196261682243 Time  1.06 s\n",
            "Epoch :  7  Batch :  108  Loss :   0.006861777482333343  Accuracy :  0.9972222222222222 Time  1.04 s\n",
            "Epoch :  7  Batch :  109  Loss :   0.006802165966375236  Accuracy :  0.9972477064220183 Time  1.05 s\n",
            "Epoch :  7  Batch :  110  Loss :   0.006747242796832738  Accuracy :  0.9972727272727273 Time  1.05 s\n",
            "Epoch :  7  Batch :  111  Loss :   0.006693548930998732  Accuracy :  0.9972972972972973 Time  1.05 s\n",
            "Epoch :  7  Batch :  112  Loss :   0.006637332248569042  Accuracy :  0.9973214285714286 Time  1.05 s\n",
            "Epoch :  7  Batch :  113  Loss :   0.006582064817188744  Accuracy :  0.9973451327433628 Time  1.05 s\n",
            "Epoch :  7  Batch :  114  Loss :   0.006535254180275125  Accuracy :  0.9973684210526316 Time  1.05 s\n",
            "Epoch :  7  Batch :  115  Loss :   0.006485228662495501  Accuracy :  0.9973913043478261 Time  1.04 s\n",
            "Epoch :  7  Batch :  116  Loss :   0.006492411409547865  Accuracy :  0.9974137931034482 Time  1.05 s\n",
            "Epoch :  7  Batch :  117  Loss :   0.006504967439453054  Accuracy :  0.9974358974358974 Time  1.06 s\n",
            "Epoch :  7  Batch :  118  Loss :   0.006459271502597218  Accuracy :  0.997457627118644 Time  1.06 s\n",
            "Epoch :  7  Batch :  119  Loss :   0.006410398462688464  Accuracy :  0.9974789915966387 Time  1.05 s\n",
            "Epoch :  7  Batch :  120  Loss :   0.006359264604422303  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  7  Batch :  121  Loss :   0.006309517318539683  Accuracy :  0.9975206611570248 Time  1.05 s\n",
            "Epoch :  7  Batch :  122  Loss :   0.006271960599167601  Accuracy :  0.9975409836065574 Time  1.04 s\n",
            "Epoch :  7  Batch :  123  Loss :   0.00634572915796747  Accuracy :  0.9975609756097561 Time  1.04 s\n",
            "Epoch :  7  Batch :  124  Loss :   0.006297458438629908  Accuracy :  0.9975806451612903 Time  1.05 s\n",
            "Epoch :  7  Batch :  125  Loss :   0.00627808439207729  Accuracy :  0.9976 Time  1.05 s\n",
            "Epoch :  7  Batch :  126  Loss :   0.006235877400156044  Accuracy :  0.9976190476190476 Time  1.05 s\n",
            "Epoch :  7  Batch :  127  Loss :   0.006278576542816755  Accuracy :  0.9976377952755906 Time  1.04 s\n",
            "Epoch :  7  Batch :  128  Loss :   0.00623783805065159  Accuracy :  0.99765625 Time  1.06 s\n",
            "Epoch :  7  Batch :  129  Loss :   0.006212217282831431  Accuracy :  0.9976744186046511 Time  1.06 s\n",
            "Epoch :  7  Batch :  130  Loss :   0.006329468720874642  Accuracy :  0.9976923076923077 Time  1.05 s\n",
            "Epoch :  7  Batch :  131  Loss :   0.006283923468924881  Accuracy :  0.9977099236641221 Time  1.05 s\n",
            "Epoch :  7  Batch :  132  Loss :   0.006257111022216232  Accuracy :  0.9977272727272727 Time  1.05 s\n",
            "Epoch :  7  Batch :  133  Loss :   0.006229355184539025  Accuracy :  0.9977443609022556 Time  1.05 s\n",
            "Epoch :  7  Batch :  134  Loss :   0.006191446231187272  Accuracy :  0.9977611940298508 Time  1.04 s\n",
            "Epoch :  7  Batch :  135  Loss :   0.006146485112064208  Accuracy :  0.9977777777777778 Time  1.05 s\n",
            "Epoch :  7  Batch :  136  Loss :   0.006144377966419971  Accuracy :  0.9977941176470588 Time  1.05 s\n",
            "Epoch :  7  Batch :  137  Loss :   0.0061097086348186795  Accuracy :  0.9978102189781022 Time  1.05 s\n",
            "Epoch :  7  Batch :  138  Loss :   0.0062924355162607935  Accuracy :  0.9978260869565218 Time  1.05 s\n",
            "Epoch :  7  Batch :  139  Loss :   0.006247737560305651  Accuracy :  0.9978417266187051 Time  1.08 s\n",
            "Epoch :  7  Batch :  140  Loss :   0.006204370255311785  Accuracy :  0.9978571428571429 Time  1.07 s\n",
            "Epoch :  7  Batch :  141  Loss :   0.006170318936594722  Accuracy :  0.997872340425532 Time  1.04 s\n",
            "Epoch :  7  Batch :  142  Loss :   0.006129741026264858  Accuracy :  0.997887323943662 Time  1.04 s\n",
            "Epoch :  7  Batch :  143  Loss :   0.00614173829559989  Accuracy :  0.9979020979020979 Time  1.05 s\n",
            "Epoch :  7  Batch :  144  Loss :   0.0061113346911143805  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  7  Batch :  145  Loss :   0.006084146795710067  Accuracy :  0.9979310344827587 Time  1.04 s\n",
            "Epoch :  7  Batch :  146  Loss :   0.006047600094760825  Accuracy :  0.9979452054794521 Time  1.04 s\n",
            "Epoch :  7  Batch :  147  Loss :   0.006015082342026685  Accuracy :  0.9979591836734694 Time  1.05 s\n",
            "Epoch :  7  Batch :  148  Loss :   0.005975024361170064  Accuracy :  0.9979729729729729 Time  1.04 s\n",
            "Epoch :  7  Batch :  149  Loss :   0.005935956204562814  Accuracy :  0.9979865771812081 Time  1.04 s\n",
            "Epoch :  7  Batch :  150  Loss :   0.0059028292305204864  Accuracy :  0.998 Time  1.06 s\n",
            "Epoch :  7  Batch :  151  Loss :   0.005866534457823789  Accuracy :  0.9980132450331126 Time  1.07 s\n",
            "Epoch :  7  Batch :  152  Loss :   0.005841391620953418  Accuracy :  0.9980263157894737 Time  1.05 s\n",
            "Epoch :  7  Batch :  153  Loss :   0.005823649833116945  Accuracy :  0.9980392156862745 Time  1.05 s\n",
            "Epoch :  7  Batch :  154  Loss :   0.0060433419560692305  Accuracy :  0.9977272727272727 Time  1.04 s\n",
            "Epoch :  7  Batch :  155  Loss :   0.006007431589598362  Accuracy :  0.997741935483871 Time  1.05 s\n",
            "Epoch :  7  Batch :  156  Loss :   0.005971569280751656  Accuracy :  0.9977564102564103 Time  1.04 s\n",
            "Epoch :  7  Batch :  157  Loss :   0.005935246690210166  Accuracy :  0.9977707006369426 Time  1.05 s\n",
            "Epoch :  7  Batch :  158  Loss :   0.005975475725214972  Accuracy :  0.9977848101265823 Time  1.04 s\n",
            "Epoch :  7  Batch :  159  Loss :   0.005947821945237192  Accuracy :  0.9977987421383647 Time  1.04 s\n",
            "Epoch :  7  Batch :  160  Loss :   0.005911509768293399  Accuracy :  0.9978125 Time  1.04 s\n",
            "Epoch :  7  Batch :  161  Loss :   0.005875307887690272  Accuracy :  0.9978260869565218 Time  1.06 s\n",
            "Epoch :  7  Batch :  162  Loss :   0.00584392224221647  Accuracy :  0.9978395061728395 Time  1.05 s\n",
            "Epoch :  7  Batch :  163  Loss :   0.005811061817115998  Accuracy :  0.9978527607361963 Time  1.07 s\n",
            "Epoch :  7  Batch :  164  Loss :   0.005782929245425493  Accuracy :  0.9978658536585366 Time  1.05 s\n",
            "Epoch :  7  Batch :  165  Loss :   0.005749038801921856  Accuracy :  0.9978787878787879 Time  1.06 s\n",
            "Epoch :  7  Batch :  166  Loss :   0.005717756588133655  Accuracy :  0.9978915662650603 Time  1.05 s\n",
            "Epoch :  7  Batch :  167  Loss :   0.00568473212980187  Accuracy :  0.9979041916167665 Time  1.05 s\n",
            "Epoch :  7  Batch :  168  Loss :   0.005654150855748628  Accuracy :  0.9979166666666667 Time  1.04 s\n",
            "Epoch :  7  Batch :  169  Loss :   0.005631016132729362  Accuracy :  0.9979289940828402 Time  1.04 s\n",
            "Epoch :  7  Batch :  170  Loss :   0.005609418490862213  Accuracy :  0.9979411764705882 Time  1.05 s\n",
            "Epoch :  7  Batch :  171  Loss :   0.005578694591812356  Accuracy :  0.997953216374269 Time  1.05 s\n",
            "Epoch :  7  Batch :  172  Loss :   0.00555369346900904  Accuracy :  0.9979651162790698 Time  1.06 s\n",
            "Epoch :  7  Batch :  173  Loss :   0.005539301290646844  Accuracy :  0.9979768786127168 Time  1.05 s\n",
            "Epoch :  7  Batch :  174  Loss :   0.005510031122584819  Accuracy :  0.9979885057471264 Time  1.08 s\n",
            "Epoch :  7  Batch :  175  Loss :   0.0055971886164791484  Accuracy :  0.998 Time  1.06 s\n",
            "Epoch :  7  Batch :  176  Loss :   0.005566050425097108  Accuracy :  0.9980113636363637 Time  1.07 s\n",
            "Epoch :  7  Batch :  177  Loss :   0.005541523611603837  Accuracy :  0.9980225988700565 Time  1.05 s\n",
            "Epoch :  7  Batch :  178  Loss :   0.005516613878324769  Accuracy :  0.9980337078651685 Time  1.05 s\n",
            "Epoch :  7  Batch :  179  Loss :   0.005487479597694948  Accuracy :  0.9980446927374301 Time  1.05 s\n",
            "Epoch :  7  Batch :  180  Loss :   0.005471328985246752  Accuracy :  0.9980555555555556 Time  1.04 s\n",
            "Epoch :  7  Batch :  181  Loss :   0.005443894066117853  Accuracy :  0.9980662983425415 Time  1.04 s\n",
            "Epoch :  7  Batch :  182  Loss :   0.005450671068971572  Accuracy :  0.9980769230769231 Time  1.05 s\n",
            "Epoch :  7  Batch :  183  Loss :   0.005456610713577116  Accuracy :  0.9980874316939891 Time  1.04 s\n",
            "Epoch :  7  Batch :  184  Loss :   0.0055661454589426285  Accuracy :  0.9980978260869565 Time  1.07 s\n",
            "Epoch :  7  Batch :  185  Loss :   0.005539434340017898  Accuracy :  0.9981081081081081 Time  1.06 s\n",
            "Epoch :  7  Batch :  186  Loss :   0.005553590546638119  Accuracy :  0.9981182795698925 Time  1.07 s\n",
            "Epoch :  7  Batch :  187  Loss :   0.0055251547883552335  Accuracy :  0.9981283422459893 Time  1.05 s\n",
            "Epoch :  7  Batch :  188  Loss :   0.005507884151658335  Accuracy :  0.9981382978723404 Time  1.04 s\n",
            "Epoch :  7  Batch :  189  Loss :   0.005501306264291064  Accuracy :  0.9981481481481481 Time  1.04 s\n",
            "Epoch :  7  Batch :  190  Loss :   0.005476063294075797  Accuracy :  0.9981578947368421 Time  1.04 s\n",
            "Epoch :  7  Batch :  191  Loss :   0.005451136737307948  Accuracy :  0.9981675392670157 Time  1.04 s\n",
            "Epoch :  7  Batch :  192  Loss :   0.005427564028574731  Accuracy :  0.9981770833333333 Time  1.04 s\n",
            "Epoch :  7  Batch :  193  Loss :   0.005404474336091173  Accuracy :  0.9981865284974093 Time  1.05 s\n",
            "Epoch :  7  Batch :  194  Loss :   0.005378011556551671  Accuracy :  0.9981958762886598 Time  1.05 s\n",
            "Epoch :  7  Batch :  195  Loss :   0.005353701860682728  Accuracy :  0.9982051282051282 Time  1.05 s\n",
            "Epoch :  7  Batch :  196  Loss :   0.005330461198039357  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  7  Batch :  197  Loss :   0.0053036216092647555  Accuracy :  0.9982233502538072 Time  1.06 s\n",
            "Epoch :  7  Batch :  198  Loss :   0.005280237972806619  Accuracy :  0.9982323232323232 Time  1.05 s\n",
            "Epoch :  7  Batch :  199  Loss :   0.005328612864049954  Accuracy :  0.9982412060301508 Time  1.06 s\n",
            "Epoch :  7  Batch :  200  Loss :   0.005306253224080138  Accuracy :  0.99825 Time  1.04 s\n",
            "Epoch :  7  Batch :  201  Loss :   0.005398898450731571  Accuracy :  0.9982587064676617 Time  1.05 s\n",
            "Epoch :  7  Batch :  202  Loss :   0.005374136983639143  Accuracy :  0.9982673267326733 Time  1.05 s\n",
            "Epoch :  7  Batch :  203  Loss :   0.006174008691133736  Accuracy :  0.9980295566502463 Time  1.05 s\n",
            "Epoch :  7  Batch :  204  Loss :   0.006173503337456221  Accuracy :  0.9980392156862745 Time  1.04 s\n",
            "Epoch :  7  Batch :  205  Loss :   0.0061456218149964475  Accuracy :  0.9980487804878049 Time  1.05 s\n",
            "Epoch :  7  Batch :  206  Loss :   0.006117077990743338  Accuracy :  0.9980582524271845 Time  1.06 s\n",
            "Epoch :  7  Batch :  207  Loss :   0.006095006185715089  Accuracy :  0.9980676328502416 Time  1.07 s\n",
            "Epoch :  7  Batch :  208  Loss :   0.0060668714903001205  Accuracy :  0.9980769230769231 Time  1.07 s\n",
            "Epoch :  7  Batch :  209  Loss :   0.0060500850574529514  Accuracy :  0.9980861244019139 Time  1.05 s\n",
            "Epoch :  7  Batch :  210  Loss :   0.006027871036141213  Accuracy :  0.9980952380952381 Time  1.05 s\n",
            "Epoch :  7  Batch :  211  Loss :   0.0060287354547037715  Accuracy :  0.9981042654028436 Time  1.04 s\n",
            "Epoch :  7  Batch :  212  Loss :   0.006011799350813337  Accuracy :  0.9981132075471698 Time  1.05 s\n",
            "Epoch :  7  Batch :  213  Loss :   0.006016025144355142  Accuracy :  0.9981220657276996 Time  1.05 s\n",
            "Epoch :  7  Batch :  214  Loss :   0.005992730072079059  Accuracy :  0.9981308411214953 Time  1.05 s\n",
            "Epoch :  7  Batch :  215  Loss :   0.0060722192382199015  Accuracy :  0.998139534883721 Time  1.05 s\n",
            "Epoch :  7  Batch :  216  Loss :   0.0062307190276071735  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  7  Batch :  217  Loss :   0.006211287470866791  Accuracy :  0.997926267281106 Time  1.06 s\n",
            "Epoch :  7  Batch :  218  Loss :   0.00618343179977532  Accuracy :  0.9979357798165137 Time  1.06 s\n",
            "Epoch :  7  Batch :  219  Loss :   0.0061603932337849785  Accuracy :  0.9979452054794521 Time  1.07 s\n",
            "Epoch :  7  Batch :  220  Loss :   0.006134672253491854  Accuracy :  0.9979545454545454 Time  1.05 s\n",
            "Epoch :  7  Batch :  221  Loss :   0.0061089719002168285  Accuracy :  0.9979638009049774 Time  1.05 s\n",
            "Epoch :  7  Batch :  222  Loss :   0.006101466742309328  Accuracy :  0.9979729729729729 Time  1.04 s\n",
            "Epoch :  7  Batch :  223  Loss :   0.006080986793807367  Accuracy :  0.997982062780269 Time  1.05 s\n",
            "Epoch :  7  Batch :  224  Loss :   0.006054286828007857  Accuracy :  0.9979910714285715 Time  1.06 s\n",
            "Epoch :  7  Batch :  225  Loss :   0.0060340298491064455  Accuracy :  0.998 Time  1.04 s\n",
            "Epoch :  7  Batch :  226  Loss :   0.006013260533197668  Accuracy :  0.9980088495575221 Time  1.04 s\n",
            "Epoch :  7  Batch :  227  Loss :   0.00600410161844028  Accuracy :  0.9980176211453744 Time  1.04 s\n",
            "Epoch :  7  Batch :  228  Loss :   0.005987056270737013  Accuracy :  0.9980263157894737 Time  1.05 s\n",
            "Epoch :  7  Batch :  229  Loss :   0.005995442363963255  Accuracy :  0.9980349344978166 Time  1.06 s\n",
            "Epoch :  7  Batch :  230  Loss :   0.006236566130562848  Accuracy :  0.9978260869565218 Time  1.07 s\n",
            "Epoch :  7  Batch :  231  Loss :   0.0062120824345958  Accuracy :  0.9978354978354979 Time  1.05 s\n",
            "Epoch :  7  Batch :  232  Loss :   0.006188061596260703  Accuracy :  0.9978448275862069 Time  1.05 s\n",
            "Epoch :  7  Batch :  233  Loss :   0.006167702540348568  Accuracy :  0.9978540772532188 Time  1.04 s\n",
            "Epoch :  7  Batch :  234  Loss :   0.006157349187778269  Accuracy :  0.9978632478632479 Time  1.05 s\n",
            "Epoch :  7  Batch :  235  Loss :   0.006166438264598911  Accuracy :  0.997872340425532 Time  1.05 s\n",
            "Epoch :  7  Batch :  236  Loss :   0.006148935964166273  Accuracy :  0.9978813559322034 Time  1.05 s\n",
            "Epoch :  7  Batch :  237  Loss :   0.006129144468047078  Accuracy :  0.9978902953586498 Time  1.05 s\n",
            "Epoch :  7  Batch :  238  Loss :   0.006111346202938692  Accuracy :  0.9978991596638656 Time  1.05 s\n",
            "Epoch :  7  Batch :  239  Loss :   0.006086530000707875  Accuracy :  0.997907949790795 Time  1.06 s\n",
            "Epoch :  7  Batch :  240  Loss :   0.006062132260315896  Accuracy :  0.9979166666666667 Time  1.06 s\n",
            "Epoch :  7  Batch :  241  Loss :   0.006042166984508672  Accuracy :  0.9979253112033195 Time  1.06 s\n",
            "Epoch :  7  Batch :  242  Loss :   0.006017885687229443  Accuracy :  0.9979338842975206 Time  1.05 s\n",
            "Epoch :  7  Batch :  243  Loss :   0.005994577342086211  Accuracy :  0.9979423868312757 Time  1.04 s\n",
            "Epoch :  7  Batch :  244  Loss :   0.0059704839781350855  Accuracy :  0.9979508196721312 Time  1.04 s\n",
            "Epoch :  7  Batch :  245  Loss :   0.0059477304690814465  Accuracy :  0.9979591836734694 Time  1.04 s\n",
            "Epoch :  7  Batch :  246  Loss :   0.00593507679573159  Accuracy :  0.9979674796747967 Time  1.05 s\n",
            "Epoch :  7  Batch :  247  Loss :   0.00594097726273389  Accuracy :  0.9979757085020243 Time  1.04 s\n",
            "Epoch :  7  Batch :  248  Loss :   0.005917758490619646  Accuracy :  0.9979838709677419 Time  1.05 s\n",
            "Epoch :  7  Batch :  249  Loss :   0.005918025208704209  Accuracy :  0.9979919678714859 Time  1.04 s\n",
            "Epoch :  7  Batch :  250  Loss :   0.0058952051807718816  Accuracy :  0.998 Time  1.06 s\n",
            "Epoch :  7  Batch :  251  Loss :   0.005873210671381233  Accuracy :  0.99800796812749 Time  1.07 s\n",
            "Epoch :  7  Batch :  252  Loss :   0.005853030991414608  Accuracy :  0.998015873015873 Time  1.07 s\n",
            "Epoch :  7  Batch :  253  Loss :   0.00583275298456493  Accuracy :  0.9980237154150198 Time  1.05 s\n",
            "Epoch :  7  Batch :  254  Loss :   0.005810139515806369  Accuracy :  0.9980314960629921 Time  1.04 s\n",
            "Epoch :  7  Batch :  255  Loss :   0.005792275547505329  Accuracy :  0.9980392156862745 Time  1.04 s\n",
            "Epoch :  7  Batch :  256  Loss :   0.005781239197602872  Accuracy :  0.998046875 Time  1.04 s\n",
            "Epoch :  7  Batch :  257  Loss :   0.005762294208618916  Accuracy :  0.9980544747081712 Time  1.05 s\n",
            "Epoch :  7  Batch :  258  Loss :   0.005927493924097033  Accuracy :  0.9978682170542635 Time  1.05 s\n",
            "Epoch :  7  Batch :  259  Loss :   0.005909573636663867  Accuracy :  0.9978764478764479 Time  1.05 s\n",
            "Epoch :  7  Batch :  260  Loss :   0.005888085585404322  Accuracy :  0.9978846153846154 Time  1.04 s\n",
            "Epoch :  7  Batch :  261  Loss :   0.005894192639497239  Accuracy :  0.9978927203065134 Time  1.06 s\n",
            "Epoch :  7  Batch :  262  Loss :   0.005873520817679401  Accuracy :  0.9979007633587786 Time  1.06 s\n",
            "Epoch :  7  Batch :  263  Loss :   0.005851764953291325  Accuracy :  0.9979087452471483 Time  1.06 s\n",
            "Epoch :  7  Batch :  264  Loss :   0.005830162719824081  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  7  Batch :  265  Loss :   0.005808810123878329  Accuracy :  0.9979245283018868 Time  1.04 s\n",
            "Epoch :  7  Batch :  266  Loss :   0.005798435483176631  Accuracy :  0.9979323308270677 Time  1.05 s\n",
            "Epoch :  7  Batch :  267  Loss :   0.005777422351559253  Accuracy :  0.997940074906367 Time  1.05 s\n",
            "Epoch :  7  Batch :  268  Loss :   0.005764972688490134  Accuracy :  0.9979477611940298 Time  1.05 s\n",
            "Epoch :  7  Batch :  269  Loss :   0.005745606517459683  Accuracy :  0.9979553903345725 Time  1.05 s\n",
            "Epoch :  7  Batch :  270  Loss :   0.005729052194807014  Accuracy :  0.9979629629629629 Time  1.05 s\n",
            "Epoch :  7  Batch :  271  Loss :   0.005742018961370622  Accuracy :  0.997970479704797 Time  1.05 s\n",
            "Epoch :  7  Batch :  272  Loss :   0.00572167775416186  Accuracy :  0.9979779411764705 Time  1.06 s\n",
            "Epoch :  7  Batch :  273  Loss :   0.0057106910843196185  Accuracy :  0.997985347985348 Time  1.06 s\n",
            "Epoch :  7  Batch :  274  Loss :   0.0056902001805790026  Accuracy :  0.997992700729927 Time  1.06 s\n",
            "Epoch :  7  Batch :  275  Loss :   0.0056717954508018345  Accuracy :  0.998 Time  1.04 s\n",
            "Epoch :  7  Batch :  276  Loss :   0.005653341628282721  Accuracy :  0.9980072463768116 Time  1.05 s\n",
            "Epoch :  7  Batch :  277  Loss :   0.005633952956480043  Accuracy :  0.998014440433213 Time  1.05 s\n",
            "Epoch :  7  Batch :  278  Loss :   0.005614495473787592  Accuracy :  0.9980215827338129 Time  1.05 s\n",
            "Epoch :  7  Batch :  279  Loss :   0.005595182670046881  Accuracy :  0.9980286738351255 Time  1.04 s\n",
            "Epoch :  7  Batch :  280  Loss :   0.005578412863880138  Accuracy :  0.9980357142857142 Time  1.04 s\n",
            "Epoch :  7  Batch :  281  Loss :   0.005559358843818706  Accuracy :  0.9980427046263345 Time  1.05 s\n",
            "Epoch :  7  Batch :  282  Loss :   0.005605070401175676  Accuracy :  0.9980496453900709 Time  1.04 s\n",
            "Epoch :  7  Batch :  283  Loss :   0.005586667040671681  Accuracy :  0.9980565371024736 Time  1.06 s\n",
            "Epoch :  7  Batch :  284  Loss :   0.005570492455121958  Accuracy :  0.9980633802816902 Time  1.06 s\n",
            "Epoch :  7  Batch :  285  Loss :   0.005551896926057484  Accuracy :  0.9980701754385964 Time  1.06 s\n",
            "Epoch :  7  Batch :  286  Loss :   0.005548703168801535  Accuracy :  0.9980769230769231 Time  1.05 s\n",
            "Epoch :  7  Batch :  287  Loss :   0.0055317166965393565  Accuracy :  0.9980836236933798 Time  1.05 s\n",
            "Epoch :  7  Batch :  288  Loss :   0.005513067019440617  Accuracy :  0.9980902777777778 Time  1.05 s\n",
            "Epoch :  7  Batch :  289  Loss :   0.005494541837103709  Accuracy :  0.9980968858131488 Time  1.04 s\n",
            "Epoch :  7  Batch :  290  Loss :   0.005475882968889421  Accuracy :  0.9981034482758621 Time  1.04 s\n",
            "Epoch :  7  Batch :  291  Loss :   0.005457866413603197  Accuracy :  0.9981099656357388 Time  1.05 s\n",
            "Epoch :  7  Batch :  292  Loss :   0.0057446474658643465  Accuracy :  0.9979452054794521 Time  1.05 s\n",
            "Epoch :  7  Batch :  293  Loss :   0.00572584956296745  Accuracy :  0.9979522184300341 Time  1.05 s\n",
            "Epoch :  7  Batch :  294  Loss :   0.0057125904350454415  Accuracy :  0.9979591836734694 Time  1.07 s\n",
            "Epoch :  7  Batch :  295  Loss :   0.005697617339368878  Accuracy :  0.9979661016949153 Time  1.06 s\n",
            "Epoch :  7  Batch :  296  Loss :   0.005687485016096616  Accuracy :  0.9979729729729729 Time  1.06 s\n",
            "Epoch :  7  Batch :  297  Loss :   0.005672318345544897  Accuracy :  0.997979797979798 Time  1.05 s\n",
            "Epoch :  7  Batch :  298  Loss :   0.005789137331807154  Accuracy :  0.9979865771812081 Time  1.05 s\n",
            "Epoch :  7  Batch :  299  Loss :   0.005769990213970883  Accuracy :  0.9979933110367893 Time  1.05 s\n",
            "Epoch :  7  Batch :  300  Loss :   0.005752003297990692  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  7  Batch :  301  Loss :   0.005749643235271093  Accuracy :  0.9980066445182725 Time  1.05 s\n",
            "Epoch :  7  Batch :  302  Loss :   0.005735195587958334  Accuracy :  0.9980132450331126 Time  1.05 s\n",
            "Epoch :  7  Batch :  303  Loss :   0.005721605296340647  Accuracy :  0.998019801980198 Time  1.05 s\n",
            "Epoch :  7  Batch :  304  Loss :   0.005703941399755614  Accuracy :  0.9980263157894737 Time  1.05 s\n",
            "Epoch :  7  Batch :  305  Loss :   0.007108972033940935  Accuracy :  0.9975409836065574 Time  1.06 s\n",
            "Epoch :  7  Batch :  306  Loss :   0.0070989081710305  Accuracy :  0.9975490196078431 Time  1.06 s\n",
            "Epoch :  7  Batch :  307  Loss :   0.007094917765934004  Accuracy :  0.9975530179445351 Time  0.56 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 99 %\n",
            "[7 epoch] Accuracy of the network on the Training images: 99 %\n",
            "Epoch :  8  Batch :  1  Loss :   0.0008407452842220664  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  8  Batch :  2  Loss :   0.0007869289256632328  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  8  Batch :  3  Loss :   0.000588058692907604  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  8  Batch :  4  Loss :   0.0005654732121911366  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  8  Batch :  5  Loss :   0.0006382202642271295  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  8  Batch :  6  Loss :   0.0071370041235544095  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  8  Batch :  7  Loss :   0.006706808887039577  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  8  Batch :  8  Loss :   0.0060772069064114476  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  8  Batch :  9  Loss :   0.00574318405738773  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  8  Batch :  10  Loss :   0.005233741943084169  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  8  Batch :  11  Loss :   0.01465080307653724  Accuracy :  0.9954545454545455 Time  1.07 s\n",
            "Epoch :  8  Batch :  12  Loss :   0.013547692259938534  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  8  Batch :  13  Loss :   0.012618284472130025  Accuracy :  0.9961538461538462 Time  1.04 s\n",
            "Epoch :  8  Batch :  14  Loss :   0.011766984091082122  Accuracy :  0.9964285714285714 Time  1.04 s\n",
            "Epoch :  8  Batch :  15  Loss :   0.011768411376397125  Accuracy :  0.9966666666666667 Time  1.05 s\n",
            "Epoch :  8  Batch :  16  Loss :   0.011060715999519743  Accuracy :  0.996875 Time  1.06 s\n",
            "Epoch :  8  Batch :  17  Loss :   0.010434315901982379  Accuracy :  0.9970588235294118 Time  1.04 s\n",
            "Epoch :  8  Batch :  18  Loss :   0.009878119907322494  Accuracy :  0.9972222222222222 Time  1.04 s\n",
            "Epoch :  8  Batch :  19  Loss :   0.00949983800053719  Accuracy :  0.9973684210526316 Time  1.05 s\n",
            "Epoch :  8  Batch :  20  Loss :   0.00981691303968546  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  8  Batch :  21  Loss :   0.00944349005710267  Accuracy :  0.9976190476190476 Time  1.07 s\n",
            "Epoch :  8  Batch :  22  Loss :   0.009058891219452065  Accuracy :  0.9977272727272727 Time  1.06 s\n",
            "Epoch :  8  Batch :  23  Loss :   0.009188524503691082  Accuracy :  0.9978260869565218 Time  1.05 s\n",
            "Epoch :  8  Batch :  24  Loss :   0.008910765595525541  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  8  Batch :  25  Loss :   0.008587777356733568  Accuracy :  0.998 Time  1.04 s\n",
            "Epoch :  8  Batch :  26  Loss :   0.008284164649152305  Accuracy :  0.9980769230769231 Time  1.05 s\n",
            "Epoch :  8  Batch :  27  Loss :   0.008009962244412035  Accuracy :  0.9981481481481481 Time  1.05 s\n",
            "Epoch :  8  Batch :  28  Loss :   0.007820809333290006  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  8  Batch :  29  Loss :   0.007657639889532283  Accuracy :  0.9982758620689656 Time  1.05 s\n",
            "Epoch :  8  Batch :  30  Loss :   0.0077183063282670146  Accuracy :  0.9983333333333333 Time  1.05 s\n",
            "Epoch :  8  Batch :  31  Loss :   0.007499749124925133  Accuracy :  0.9983870967741936 Time  1.06 s\n",
            "Epoch :  8  Batch :  32  Loss :   0.007436986254560907  Accuracy :  0.9984375 Time  1.06 s\n",
            "Epoch :  8  Batch :  33  Loss :   0.0078103363952267855  Accuracy :  0.9984848484848485 Time  1.06 s\n",
            "Epoch :  8  Batch :  34  Loss :   0.007734217309916858  Accuracy :  0.9985294117647059 Time  1.05 s\n",
            "Epoch :  8  Batch :  35  Loss :   0.0075529764660002136  Accuracy :  0.9985714285714286 Time  1.05 s\n",
            "Epoch :  8  Batch :  36  Loss :   0.007348510348593764  Accuracy :  0.9986111111111111 Time  1.04 s\n",
            "Epoch :  8  Batch :  37  Loss :   0.00722246479504188  Accuracy :  0.9986486486486487 Time  1.05 s\n",
            "Epoch :  8  Batch :  38  Loss :   0.007064086926555702  Accuracy :  0.9986842105263158 Time  1.05 s\n",
            "Epoch :  8  Batch :  39  Loss :   0.006921181529133509  Accuracy :  0.9987179487179487 Time  1.05 s\n",
            "Epoch :  8  Batch :  40  Loss :   0.006759415875421837  Accuracy :  0.99875 Time  1.04 s\n",
            "Epoch :  8  Batch :  41  Loss :   0.006654589567560612  Accuracy :  0.998780487804878 Time  1.05 s\n",
            "Epoch :  8  Batch :  42  Loss :   0.006501328570829216  Accuracy :  0.9988095238095238 Time  1.05 s\n",
            "Epoch :  8  Batch :  43  Loss :   0.006516733709695684  Accuracy :  0.9988372093023256 Time  1.07 s\n",
            "Epoch :  8  Batch :  44  Loss :   0.006378439675741406  Accuracy :  0.9988636363636364 Time  1.06 s\n",
            "Epoch :  8  Batch :  45  Loss :   0.006254204798541549  Accuracy :  0.9988888888888889 Time  1.05 s\n",
            "Epoch :  8  Batch :  46  Loss :   0.006149784711179445  Accuracy :  0.9989130434782608 Time  1.04 s\n",
            "Epoch :  8  Batch :  47  Loss :   0.006186003944133111  Accuracy :  0.9989361702127659 Time  1.04 s\n",
            "Epoch :  8  Batch :  48  Loss :   0.009459414450248005  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  8  Batch :  49  Loss :   0.009275254608744907  Accuracy :  0.9979591836734694 Time  1.05 s\n",
            "Epoch :  8  Batch :  50  Loss :   0.009126135507249273  Accuracy :  0.998 Time  1.04 s\n",
            "Epoch :  8  Batch :  51  Loss :   0.009177751259937627  Accuracy :  0.9980392156862745 Time  1.05 s\n",
            "Epoch :  8  Batch :  52  Loss :   0.00904988094575506  Accuracy :  0.9980769230769231 Time  1.05 s\n",
            "Epoch :  8  Batch :  53  Loss :   0.009032353864793823  Accuracy :  0.9981132075471698 Time  1.07 s\n",
            "Epoch :  8  Batch :  54  Loss :   0.008872072077119137  Accuracy :  0.9981481481481481 Time  1.06 s\n",
            "Epoch :  8  Batch :  55  Loss :   0.008738340611506084  Accuracy :  0.9981818181818182 Time  1.06 s\n",
            "Epoch :  8  Batch :  56  Loss :   0.008600675698939344  Accuracy :  0.9982142857142857 Time  1.06 s\n",
            "Epoch :  8  Batch :  57  Loss :   0.008483912422538228  Accuracy :  0.9982456140350877 Time  1.05 s\n",
            "Epoch :  8  Batch :  58  Loss :   0.00934823861211169  Accuracy :  0.9974137931034482 Time  1.05 s\n",
            "Epoch :  8  Batch :  59  Loss :   0.00920919667173698  Accuracy :  0.997457627118644 Time  1.05 s\n",
            "Epoch :  8  Batch :  60  Loss :   0.009071073216910008  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  8  Batch :  61  Loss :   0.008942779864959388  Accuracy :  0.9975409836065574 Time  1.06 s\n",
            "Epoch :  8  Batch :  62  Loss :   0.008819372167401467  Accuracy :  0.9975806451612903 Time  1.04 s\n",
            "Epoch :  8  Batch :  63  Loss :   0.00869152698682488  Accuracy :  0.9976190476190476 Time  1.05 s\n",
            "Epoch :  8  Batch :  64  Loss :   0.008561836347780627  Accuracy :  0.99765625 Time  1.06 s\n",
            "Epoch :  8  Batch :  65  Loss :   0.008457330510002347  Accuracy :  0.9976923076923077 Time  1.06 s\n",
            "Epoch :  8  Batch :  66  Loss :   0.008362886608334851  Accuracy :  0.9977272727272727 Time  1.08 s\n",
            "Epoch :  8  Batch :  67  Loss :   0.00838273802534121  Accuracy :  0.9977611940298508 Time  1.06 s\n",
            "Epoch :  8  Batch :  68  Loss :   0.008268510707506827  Accuracy :  0.9977941176470588 Time  1.06 s\n",
            "Epoch :  8  Batch :  69  Loss :   0.008210037589036063  Accuracy :  0.9978260869565218 Time  1.06 s\n",
            "Epoch :  8  Batch :  70  Loss :   0.008097947536485402  Accuracy :  0.9978571428571429 Time  1.06 s\n",
            "Epoch :  8  Batch :  71  Loss :   0.008136231676449108  Accuracy :  0.997887323943662 Time  1.05 s\n",
            "Epoch :  8  Batch :  72  Loss :   0.00820884373832895  Accuracy :  0.9979166666666667 Time  1.04 s\n",
            "Epoch :  8  Batch :  73  Loss :   0.008247115280501519  Accuracy :  0.9979452054794521 Time  1.04 s\n",
            "Epoch :  8  Batch :  74  Loss :   0.008168699737005817  Accuracy :  0.9979729729729729 Time  1.04 s\n",
            "Epoch :  8  Batch :  75  Loss :   0.008109906857134775  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  8  Batch :  76  Loss :   0.008018193502625524  Accuracy :  0.9980263157894737 Time  1.06 s\n",
            "Epoch :  8  Batch :  77  Loss :   0.007920812721312795  Accuracy :  0.9980519480519481 Time  1.07 s\n",
            "Epoch :  8  Batch :  78  Loss :   0.007824758468464447  Accuracy :  0.9980769230769231 Time  1.04 s\n",
            "Epoch :  8  Batch :  79  Loss :   0.007758479342129836  Accuracy :  0.9981012658227848 Time  1.04 s\n",
            "Epoch :  8  Batch :  80  Loss :   0.007671296968328534  Accuracy :  0.998125 Time  1.05 s\n",
            "Epoch :  8  Batch :  81  Loss :   0.007611242458540487  Accuracy :  0.9981481481481481 Time  1.05 s\n",
            "Epoch :  8  Batch :  82  Loss :   0.0075228225280780615  Accuracy :  0.9981707317073171 Time  1.05 s\n",
            "Epoch :  8  Batch :  83  Loss :   0.007445073024515646  Accuracy :  0.9981927710843373 Time  1.04 s\n",
            "Epoch :  8  Batch :  84  Loss :   0.007380511891651189  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  8  Batch :  85  Loss :   0.008218951799961575  Accuracy :  0.9976470588235294 Time  1.04 s\n",
            "Epoch :  8  Batch :  86  Loss :   0.008278977453470403  Accuracy :  0.9976744186046511 Time  1.06 s\n",
            "Epoch :  8  Batch :  87  Loss :   0.008221648416706714  Accuracy :  0.9977011494252873 Time  1.06 s\n",
            "Epoch :  8  Batch :  88  Loss :   0.008131482629819815  Accuracy :  0.9977272727272727 Time  1.06 s\n",
            "Epoch :  8  Batch :  89  Loss :   0.00805968484838671  Accuracy :  0.9977528089887641 Time  1.06 s\n",
            "Epoch :  8  Batch :  90  Loss :   0.007980317532524674  Accuracy :  0.9977777777777778 Time  1.06 s\n",
            "Epoch :  8  Batch :  91  Loss :   0.008079738932492846  Accuracy :  0.9978021978021978 Time  1.05 s\n",
            "Epoch :  8  Batch :  92  Loss :   0.008014388656390467  Accuracy :  0.9978260869565218 Time  1.05 s\n",
            "Epoch :  8  Batch :  93  Loss :   0.007939518833037226  Accuracy :  0.9978494623655914 Time  1.04 s\n",
            "Epoch :  8  Batch :  94  Loss :   0.007860473881629196  Accuracy :  0.997872340425532 Time  1.05 s\n",
            "Epoch :  8  Batch :  95  Loss :   0.007825160025937581  Accuracy :  0.9978947368421053 Time  1.05 s\n",
            "Epoch :  8  Batch :  96  Loss :   0.007748308201371401  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  8  Batch :  97  Loss :   0.007670886071096414  Accuracy :  0.9979381443298969 Time  1.06 s\n",
            "Epoch :  8  Batch :  98  Loss :   0.007598456478207277  Accuracy :  0.9979591836734694 Time  1.07 s\n",
            "Epoch :  8  Batch :  99  Loss :   0.00752411095097407  Accuracy :  0.997979797979798 Time  1.06 s\n",
            "Epoch :  8  Batch :  100  Loss :   0.007465790827118326  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  8  Batch :  101  Loss :   0.007410595408182057  Accuracy :  0.998019801980198 Time  1.05 s\n",
            "Epoch :  8  Batch :  102  Loss :   0.007341704527646595  Accuracy :  0.9980392156862745 Time  1.05 s\n",
            "Epoch :  8  Batch :  103  Loss :   0.00799886187430001  Accuracy :  0.9975728155339806 Time  1.05 s\n",
            "Epoch :  8  Batch :  104  Loss :   0.007932147606669996  Accuracy :  0.9975961538461539 Time  1.05 s\n",
            "Epoch :  8  Batch :  105  Loss :   0.009037988406739065  Accuracy :  0.9971428571428571 Time  1.04 s\n",
            "Epoch :  8  Batch :  106  Loss :   0.008965518872499606  Accuracy :  0.9971698113207547 Time  1.05 s\n",
            "Epoch :  8  Batch :  107  Loss :   0.008885734668871928  Accuracy :  0.997196261682243 Time  1.05 s\n",
            "Epoch :  8  Batch :  108  Loss :   0.008871732687111944  Accuracy :  0.9972222222222222 Time  1.06 s\n",
            "Epoch :  8  Batch :  109  Loss :   0.008836237536401514  Accuracy :  0.9972477064220183 Time  1.06 s\n",
            "Epoch :  8  Batch :  110  Loss :   0.00876566724724729  Accuracy :  0.9972727272727273 Time  1.07 s\n",
            "Epoch :  8  Batch :  111  Loss :   0.008694734129643588  Accuracy :  0.9972972972972973 Time  1.05 s\n",
            "Epoch :  8  Batch :  112  Loss :   0.00862025444439496  Accuracy :  0.9973214285714286 Time  1.04 s\n",
            "Epoch :  8  Batch :  113  Loss :   0.008560595523454744  Accuracy :  0.9973451327433628 Time  1.05 s\n",
            "Epoch :  8  Batch :  114  Loss :   0.008681423962626835  Accuracy :  0.9973684210526316 Time  1.06 s\n",
            "Epoch :  8  Batch :  115  Loss :   0.008609562229274002  Accuracy :  0.9973913043478261 Time  1.06 s\n",
            "Epoch :  8  Batch :  116  Loss :   0.00858181812133086  Accuracy :  0.9974137931034482 Time  1.04 s\n",
            "Epoch :  8  Batch :  117  Loss :   0.008533636485893104  Accuracy :  0.9974358974358974 Time  1.04 s\n",
            "Epoch :  8  Batch :  118  Loss :   0.008477122496993091  Accuracy :  0.997457627118644 Time  1.05 s\n",
            "Epoch :  8  Batch :  119  Loss :   0.008413943368892632  Accuracy :  0.9974789915966387 Time  1.05 s\n",
            "Epoch :  8  Batch :  120  Loss :   0.008348027643660317  Accuracy :  0.9975 Time  1.07 s\n",
            "Epoch :  8  Batch :  121  Loss :   0.008291881009772774  Accuracy :  0.9975206611570248 Time  1.06 s\n",
            "Epoch :  8  Batch :  122  Loss :   0.008270537696127993  Accuracy :  0.9975409836065574 Time  1.05 s\n",
            "Epoch :  8  Batch :  123  Loss :   0.008206380436781839  Accuracy :  0.9975609756097561 Time  1.04 s\n",
            "Epoch :  8  Batch :  124  Loss :   0.00814186895490087  Accuracy :  0.9975806451612903 Time  1.04 s\n",
            "Epoch :  8  Batch :  125  Loss :   0.00809305214381311  Accuracy :  0.9976 Time  1.05 s\n",
            "Epoch :  8  Batch :  126  Loss :   0.008055880110150033  Accuracy :  0.9976190476190476 Time  1.04 s\n",
            "Epoch :  8  Batch :  127  Loss :   0.00808779796180737  Accuracy :  0.9976377952755906 Time  1.05 s\n",
            "Epoch :  8  Batch :  128  Loss :   0.008029824886875758  Accuracy :  0.99765625 Time  1.04 s\n",
            "Epoch :  8  Batch :  129  Loss :   0.007986005592496539  Accuracy :  0.9976744186046511 Time  1.05 s\n",
            "Epoch :  8  Batch :  130  Loss :   0.007954714531549074  Accuracy :  0.9976923076923077 Time  1.06 s\n",
            "Epoch :  8  Batch :  131  Loss :   0.008576872266357132  Accuracy :  0.9973282442748092 Time  1.06 s\n",
            "Epoch :  8  Batch :  132  Loss :   0.00854701356387743  Accuracy :  0.9973484848484848 Time  1.07 s\n",
            "Epoch :  8  Batch :  133  Loss :   0.008485209213983779  Accuracy :  0.9973684210526316 Time  1.04 s\n",
            "Epoch :  8  Batch :  134  Loss :   0.00843026577601463  Accuracy :  0.9973880597014926 Time  1.04 s\n",
            "Epoch :  8  Batch :  135  Loss :   0.008385630663919815  Accuracy :  0.9974074074074074 Time  1.05 s\n",
            "Epoch :  8  Batch :  136  Loss :   0.008332403753015302  Accuracy :  0.9974264705882353 Time  1.05 s\n",
            "Epoch :  8  Batch :  137  Loss :   0.00837944984322594  Accuracy :  0.9974452554744525 Time  1.05 s\n",
            "Epoch :  8  Batch :  138  Loss :   0.008320440051476538  Accuracy :  0.9974637681159421 Time  1.04 s\n",
            "Epoch :  8  Batch :  139  Loss :   0.008271827138022339  Accuracy :  0.9974820143884892 Time  1.05 s\n",
            "Epoch :  8  Batch :  140  Loss :   0.010010405120972012  Accuracy :  0.9967857142857143 Time  1.05 s\n",
            "Epoch :  8  Batch :  141  Loss :   0.010072657359592247  Accuracy :  0.9968085106382979 Time  1.05 s\n",
            "Epoch :  8  Batch :  142  Loss :   0.010011169378166939  Accuracy :  0.9968309859154929 Time  1.06 s\n",
            "Epoch :  8  Batch :  143  Loss :   0.009944745437958492  Accuracy :  0.9968531468531469 Time  1.07 s\n",
            "Epoch :  8  Batch :  144  Loss :   0.009926139332593367  Accuracy :  0.996875 Time  1.04 s\n",
            "Epoch :  8  Batch :  145  Loss :   0.00990192375450941  Accuracy :  0.9968965517241379 Time  1.04 s\n",
            "Epoch :  8  Batch :  146  Loss :   0.009848293190509437  Accuracy :  0.9969178082191781 Time  1.04 s\n",
            "Epoch :  8  Batch :  147  Loss :   0.009821880059981985  Accuracy :  0.996938775510204 Time  1.04 s\n",
            "Epoch :  8  Batch :  148  Loss :   0.0097919540704744  Accuracy :  0.9969594594594594 Time  1.04 s\n",
            "Epoch :  8  Batch :  149  Loss :   0.009757011737393203  Accuracy :  0.9969798657718121 Time  1.04 s\n",
            "Epoch :  8  Batch :  150  Loss :   0.009724017390205215  Accuracy :  0.997 Time  1.06 s\n",
            "Epoch :  8  Batch :  151  Loss :   0.00993140133958162  Accuracy :  0.9970198675496689 Time  1.04 s\n",
            "Epoch :  8  Batch :  152  Loss :   0.009918901373135955  Accuracy :  0.9970394736842105 Time  1.06 s\n",
            "Epoch :  8  Batch :  153  Loss :   0.009885179684680029  Accuracy :  0.9970588235294118 Time  1.06 s\n",
            "Epoch :  8  Batch :  154  Loss :   0.009839930900451183  Accuracy :  0.9970779220779221 Time  1.06 s\n",
            "Epoch :  8  Batch :  155  Loss :   0.009782191513541845  Accuracy :  0.9970967741935484 Time  1.06 s\n",
            "Epoch :  8  Batch :  156  Loss :   0.009726132862105703  Accuracy :  0.9971153846153846 Time  1.04 s\n",
            "Epoch :  8  Batch :  157  Loss :   0.009752529802669883  Accuracy :  0.9971337579617834 Time  1.04 s\n",
            "Epoch :  8  Batch :  158  Loss :   0.009697518114345384  Accuracy :  0.9971518987341772 Time  1.05 s\n",
            "Epoch :  8  Batch :  159  Loss :   0.00964571393509366  Accuracy :  0.9971698113207547 Time  1.05 s\n",
            "Epoch :  8  Batch :  160  Loss :   0.009634661845848314  Accuracy :  0.9971875 Time  1.05 s\n",
            "Epoch :  8  Batch :  161  Loss :   0.009582753359377732  Accuracy :  0.9972049689440994 Time  1.04 s\n",
            "Epoch :  8  Batch :  162  Loss :   0.011375398768534608  Accuracy :  0.9966049382716049 Time  1.04 s\n",
            "Epoch :  8  Batch :  163  Loss :   0.011310921945506307  Accuracy :  0.9966257668711657 Time  1.06 s\n",
            "Epoch :  8  Batch :  164  Loss :   0.01125083722191204  Accuracy :  0.9966463414634147 Time  1.06 s\n",
            "Epoch :  8  Batch :  165  Loss :   0.011812692241404544  Accuracy :  0.9963636363636363 Time  1.06 s\n",
            "Epoch :  8  Batch :  166  Loss :   0.011759335060143867  Accuracy :  0.9963855421686747 Time  1.05 s\n",
            "Epoch :  8  Batch :  167  Loss :   0.011736584975088607  Accuracy :  0.9964071856287425 Time  1.05 s\n",
            "Epoch :  8  Batch :  168  Loss :   0.011709203708007754  Accuracy :  0.9964285714285714 Time  1.05 s\n",
            "Epoch :  8  Batch :  169  Loss :   0.011705423057762652  Accuracy :  0.9964497041420118 Time  1.05 s\n",
            "Epoch :  8  Batch :  170  Loss :   0.011644692899768843  Accuracy :  0.9964705882352941 Time  1.05 s\n",
            "Epoch :  8  Batch :  171  Loss :   0.011694968285921373  Accuracy :  0.9964912280701754 Time  1.04 s\n",
            "Epoch :  8  Batch :  172  Loss :   0.011629355165411632  Accuracy :  0.9965116279069768 Time  1.04 s\n",
            "Epoch :  8  Batch :  173  Loss :   0.01161730178931965  Accuracy :  0.9965317919075144 Time  1.05 s\n",
            "Epoch :  8  Batch :  174  Loss :   0.014496847059532207  Accuracy :  0.9956896551724138 Time  1.06 s\n",
            "Epoch :  8  Batch :  175  Loss :   0.015560210531777037  Accuracy :  0.9954285714285714 Time  1.07 s\n",
            "Epoch :  8  Batch :  176  Loss :   0.015492241547524993  Accuracy :  0.9954545454545455 Time  1.06 s\n",
            "Epoch :  8  Batch :  177  Loss :   0.015438596216506603  Accuracy :  0.9954802259887006 Time  1.05 s\n",
            "Epoch :  8  Batch :  178  Loss :   0.015384096289616015  Accuracy :  0.9955056179775281 Time  1.05 s\n",
            "Epoch :  8  Batch :  179  Loss :   0.015348247625388976  Accuracy :  0.9955307262569832 Time  1.04 s\n",
            "Epoch :  8  Batch :  180  Loss :   0.015277118394604057  Accuracy :  0.9955555555555555 Time  1.05 s\n",
            "Epoch :  8  Batch :  181  Loss :   0.015199510173843814  Accuracy :  0.9955801104972376 Time  1.05 s\n",
            "Epoch :  8  Batch :  182  Loss :   0.015136818357060182  Accuracy :  0.9956043956043956 Time  1.05 s\n",
            "Epoch :  8  Batch :  183  Loss :   0.01505915489896389  Accuracy :  0.9956284153005465 Time  1.05 s\n",
            "Epoch :  8  Batch :  184  Loss :   0.014980074972655806  Accuracy :  0.9956521739130435 Time  1.04 s\n",
            "Epoch :  8  Batch :  185  Loss :   0.014909684994396426  Accuracy :  0.9956756756756757 Time  1.06 s\n",
            "Epoch :  8  Batch :  186  Loss :   0.01490408492991851  Accuracy :  0.9956989247311828 Time  1.06 s\n",
            "Epoch :  8  Batch :  187  Loss :   0.014851133019431808  Accuracy :  0.9957219251336898 Time  1.06 s\n",
            "Epoch :  8  Batch :  188  Loss :   0.01490473580216008  Accuracy :  0.9957446808510638 Time  1.05 s\n",
            "Epoch :  8  Batch :  189  Loss :   0.014831744610866601  Accuracy :  0.9957671957671957 Time  1.05 s\n",
            "Epoch :  8  Batch :  190  Loss :   0.014758215923058359  Accuracy :  0.9957894736842106 Time  1.04 s\n",
            "Epoch :  8  Batch :  191  Loss :   0.014746080771946782  Accuracy :  0.9958115183246073 Time  1.05 s\n",
            "Epoch :  8  Batch :  192  Loss :   0.014712624407062927  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  8  Batch :  193  Loss :   0.014648381688202601  Accuracy :  0.9958549222797928 Time  1.04 s\n",
            "Epoch :  8  Batch :  194  Loss :   0.014579401160686364  Accuracy :  0.9958762886597938 Time  1.05 s\n",
            "Epoch :  8  Batch :  195  Loss :   0.014529730079886623  Accuracy :  0.9958974358974358 Time  1.05 s\n",
            "Epoch :  8  Batch :  196  Loss :   0.014815123947764918  Accuracy :  0.9956632653061225 Time  1.06 s\n",
            "Epoch :  8  Batch :  197  Loss :   0.014852161926978509  Accuracy :  0.9956852791878172 Time  1.06 s\n",
            "Epoch :  8  Batch :  198  Loss :   0.014780440048498072  Accuracy :  0.9957070707070707 Time  1.06 s\n",
            "Epoch :  8  Batch :  199  Loss :   0.014757568400994354  Accuracy :  0.9957286432160805 Time  1.05 s\n",
            "Epoch :  8  Batch :  200  Loss :   0.014684395441508968  Accuracy :  0.99575 Time  1.04 s\n",
            "Epoch :  8  Batch :  201  Loss :   0.014618272249733413  Accuracy :  0.9957711442786069 Time  1.05 s\n",
            "Epoch :  8  Batch :  202  Loss :   0.014637849303518564  Accuracy :  0.9957920792079208 Time  1.05 s\n",
            "Epoch :  8  Batch :  203  Loss :   0.01459274896177672  Accuracy :  0.9958128078817734 Time  1.05 s\n",
            "Epoch :  8  Batch :  204  Loss :   0.014522911666954672  Accuracy :  0.9958333333333333 Time  1.04 s\n",
            "Epoch :  8  Batch :  205  Loss :   0.014457742277626516  Accuracy :  0.9958536585365854 Time  1.05 s\n",
            "Epoch :  8  Batch :  206  Loss :   0.014492987581142744  Accuracy :  0.995873786407767 Time  1.04 s\n",
            "Epoch :  8  Batch :  207  Loss :   0.014661016139038164  Accuracy :  0.9956521739130435 Time  1.05 s\n",
            "Epoch :  8  Batch :  208  Loss :   0.014593869120919688  Accuracy :  0.9956730769230769 Time  1.06 s\n",
            "Epoch :  8  Batch :  209  Loss :   0.01452898166514766  Accuracy :  0.9956937799043062 Time  1.07 s\n",
            "Epoch :  8  Batch :  210  Loss :   0.014489327241469818  Accuracy :  0.9957142857142857 Time  1.06 s\n",
            "Epoch :  8  Batch :  211  Loss :   0.014615156545518385  Accuracy :  0.9954976303317535 Time  1.05 s\n",
            "Epoch :  8  Batch :  212  Loss :   0.014553536955370976  Accuracy :  0.9955188679245283 Time  1.05 s\n",
            "Epoch :  8  Batch :  213  Loss :   0.014487143050446918  Accuracy :  0.9955399061032864 Time  1.05 s\n",
            "Epoch :  8  Batch :  214  Loss :   0.014422735408567018  Accuracy :  0.9955607476635514 Time  1.05 s\n",
            "Epoch :  8  Batch :  215  Loss :   0.014360222909504786  Accuracy :  0.9955813953488372 Time  1.04 s\n",
            "Epoch :  8  Batch :  216  Loss :   0.014320083014589964  Accuracy :  0.9956018518518519 Time  1.06 s\n",
            "Epoch :  8  Batch :  217  Loss :   0.014319656065180748  Accuracy :  0.9956221198156682 Time  1.05 s\n",
            "Epoch :  8  Batch :  218  Loss :   0.014276893655975122  Accuracy :  0.9956422018348624 Time  1.06 s\n",
            "Epoch :  8  Batch :  219  Loss :   0.014214591468489286  Accuracy :  0.995662100456621 Time  1.06 s\n",
            "Epoch :  8  Batch :  220  Loss :   0.014151215407566104  Accuracy :  0.9956818181818182 Time  1.06 s\n",
            "Epoch :  8  Batch :  221  Loss :   0.014112886102836921  Accuracy :  0.9957013574660634 Time  1.05 s\n",
            "Epoch :  8  Batch :  222  Loss :   0.014054108756976642  Accuracy :  0.9957207207207207 Time  1.05 s\n",
            "Epoch :  8  Batch :  223  Loss :   0.014002063636150683  Accuracy :  0.9957399103139013 Time  1.04 s\n",
            "Epoch :  8  Batch :  224  Loss :   0.013954444846409128  Accuracy :  0.9957589285714286 Time  1.05 s\n",
            "Epoch :  8  Batch :  225  Loss :   0.013911491224555195  Accuracy :  0.9957777777777778 Time  1.05 s\n",
            "Epoch :  8  Batch :  226  Loss :   0.013868139986480912  Accuracy :  0.9957964601769912 Time  1.04 s\n",
            "Epoch :  8  Batch :  227  Loss :   0.013818156035498528  Accuracy :  0.9958149779735683 Time  1.04 s\n",
            "Epoch :  8  Batch :  228  Loss :   0.013758147402608813  Accuracy :  0.9958333333333333 Time  1.05 s\n",
            "Epoch :  8  Batch :  229  Loss :   0.013703126344597475  Accuracy :  0.9958515283842795 Time  1.05 s\n",
            "Epoch :  8  Batch :  230  Loss :   0.013649445585084005  Accuracy :  0.9958695652173913 Time  1.07 s\n",
            "Epoch :  8  Batch :  231  Loss :   0.01359388833225239  Accuracy :  0.9958874458874459 Time  1.06 s\n",
            "Epoch :  8  Batch :  232  Loss :   0.013536504139822148  Accuracy :  0.9959051724137931 Time  1.05 s\n",
            "Epoch :  8  Batch :  233  Loss :   0.013484595345156846  Accuracy :  0.9959227467811159 Time  1.04 s\n",
            "Epoch :  8  Batch :  234  Loss :   0.013439660488603473  Accuracy :  0.995940170940171 Time  1.05 s\n",
            "Epoch :  8  Batch :  235  Loss :   0.013384957626360428  Accuracy :  0.9959574468085106 Time  1.05 s\n",
            "Epoch :  8  Batch :  236  Loss :   0.01332925282468303  Accuracy :  0.9959745762711865 Time  1.05 s\n",
            "Epoch :  8  Batch :  237  Loss :   0.013273620093032573  Accuracy :  0.9959915611814346 Time  1.04 s\n",
            "Epoch :  8  Batch :  238  Loss :   0.013218486419340357  Accuracy :  0.9960084033613446 Time  1.05 s\n",
            "Epoch :  8  Batch :  239  Loss :   0.01316495987782245  Accuracy :  0.9960251046025105 Time  1.05 s\n",
            "Epoch :  8  Batch :  240  Loss :   0.013117588094307091  Accuracy :  0.9960416666666667 Time  1.06 s\n",
            "Epoch :  8  Batch :  241  Loss :   0.013129756550381396  Accuracy :  0.9960580912863071 Time  1.06 s\n",
            "Epoch :  8  Batch :  242  Loss :   0.013078609960284066  Accuracy :  0.9960743801652893 Time  1.06 s\n",
            "Epoch :  8  Batch :  243  Loss :   0.013045699904476859  Accuracy :  0.9960905349794239 Time  1.04 s\n",
            "Epoch :  8  Batch :  244  Loss :   0.012994235573461962  Accuracy :  0.9961065573770492 Time  1.05 s\n",
            "Epoch :  8  Batch :  245  Loss :   0.012948198436595481  Accuracy :  0.9961224489795918 Time  1.06 s\n",
            "Epoch :  8  Batch :  246  Loss :   0.013006558674621884  Accuracy :  0.9961382113821138 Time  1.04 s\n",
            "Epoch :  8  Batch :  247  Loss :   0.013050782266777548  Accuracy :  0.9961538461538462 Time  1.04 s\n",
            "Epoch :  8  Batch :  248  Loss :   0.013000156497758824  Accuracy :  0.9961693548387097 Time  1.04 s\n",
            "Epoch :  8  Batch :  249  Loss :   0.013041910719316749  Accuracy :  0.9961847389558233 Time  1.04 s\n",
            "Epoch :  8  Batch :  250  Loss :   0.013076987371023278  Accuracy :  0.9962 Time  1.05 s\n",
            "Epoch :  8  Batch :  251  Loss :   0.013026792675651428  Accuracy :  0.9962151394422311 Time  1.05 s\n",
            "Epoch :  8  Batch :  252  Loss :   0.01321658637047337  Accuracy :  0.996031746031746 Time  1.05 s\n",
            "Epoch :  8  Batch :  253  Loss :   0.013168172984834477  Accuracy :  0.9960474308300395 Time  1.06 s\n",
            "Epoch :  8  Batch :  254  Loss :   0.013194362244669139  Accuracy :  0.9960629921259843 Time  1.05 s\n",
            "Epoch :  8  Batch :  255  Loss :   0.013147136712773228  Accuracy :  0.996078431372549 Time  1.06 s\n",
            "Epoch :  8  Batch :  256  Loss :   0.013104743719338785  Accuracy :  0.99609375 Time  1.04 s\n",
            "Epoch :  8  Batch :  257  Loss :   0.013437996566300606  Accuracy :  0.9959143968871595 Time  1.05 s\n",
            "Epoch :  8  Batch :  258  Loss :   0.013439124214041946  Accuracy :  0.9959302325581395 Time  1.05 s\n",
            "Epoch :  8  Batch :  259  Loss :   0.013393990646427026  Accuracy :  0.995945945945946 Time  1.05 s\n",
            "Epoch :  8  Batch :  260  Loss :   0.013360768434302792  Accuracy :  0.9959615384615385 Time  1.05 s\n",
            "Epoch :  8  Batch :  261  Loss :   0.013325019656995871  Accuracy :  0.9959770114942529 Time  1.05 s\n",
            "Epoch :  8  Batch :  262  Loss :   0.013275199257784242  Accuracy :  0.9959923664122138 Time  1.06 s\n",
            "Epoch :  8  Batch :  263  Loss :   0.01377407848889658  Accuracy :  0.9958174904942966 Time  1.06 s\n",
            "Epoch :  8  Batch :  264  Loss :   0.013726118751781707  Accuracy :  0.9958333333333333 Time  1.06 s\n",
            "Epoch :  8  Batch :  265  Loss :   0.013683634995480665  Accuracy :  0.9958490566037735 Time  1.04 s\n",
            "Epoch :  8  Batch :  266  Loss :   0.0136458847742975  Accuracy :  0.9958646616541353 Time  1.05 s\n",
            "Epoch :  8  Batch :  267  Loss :   0.013595332754267928  Accuracy :  0.9958801498127341 Time  1.04 s\n",
            "Epoch :  8  Batch :  268  Loss :   0.013546855412777635  Accuracy :  0.9958955223880597 Time  1.04 s\n",
            "Epoch :  8  Batch :  269  Loss :   0.013501836401295717  Accuracy :  0.995910780669145 Time  1.05 s\n",
            "Epoch :  8  Batch :  270  Loss :   0.01346986340418139  Accuracy :  0.9959259259259259 Time  1.05 s\n",
            "Epoch :  8  Batch :  271  Loss :   0.013424517567227835  Accuracy :  0.9959409594095942 Time  1.05 s\n",
            "Epoch :  8  Batch :  272  Loss :   0.013380723442316915  Accuracy :  0.9959558823529412 Time  1.06 s\n",
            "Epoch :  8  Batch :  273  Loss :   0.013350417888832053  Accuracy :  0.995970695970696 Time  1.07 s\n",
            "Epoch :  8  Batch :  274  Loss :   0.013304067526365728  Accuracy :  0.995985401459854 Time  1.07 s\n",
            "Epoch :  8  Batch :  275  Loss :   0.013259294699528254  Accuracy :  0.996 Time  1.06 s\n",
            "Epoch :  8  Batch :  276  Loss :   0.013228818625066197  Accuracy :  0.9960144927536232 Time  1.06 s\n",
            "Epoch :  8  Batch :  277  Loss :   0.013184210866517792  Accuracy :  0.996028880866426 Time  1.05 s\n",
            "Epoch :  8  Batch :  278  Loss :   0.01316934515714349  Accuracy :  0.996043165467626 Time  1.05 s\n",
            "Epoch :  8  Batch :  279  Loss :   0.013860364193520274  Accuracy :  0.9958781362007169 Time  1.06 s\n",
            "Epoch :  8  Batch :  280  Loss :   0.013818834972490939  Accuracy :  0.9958928571428571 Time  1.05 s\n",
            "Epoch :  8  Batch :  281  Loss :   0.013934418755602937  Accuracy :  0.9959074733096085 Time  1.05 s\n",
            "Epoch :  8  Batch :  282  Loss :   0.013887794797070545  Accuracy :  0.9959219858156029 Time  1.05 s\n",
            "Epoch :  8  Batch :  283  Loss :   0.013845668294186896  Accuracy :  0.9959363957597173 Time  1.04 s\n",
            "Epoch :  8  Batch :  284  Loss :   0.013799598724341501  Accuracy :  0.9959507042253521 Time  1.04 s\n",
            "Epoch :  8  Batch :  285  Loss :   0.013759118394687233  Accuracy :  0.9959649122807017 Time  1.06 s\n",
            "Epoch :  8  Batch :  286  Loss :   0.013719165549681512  Accuracy :  0.995979020979021 Time  1.06 s\n",
            "Epoch :  8  Batch :  287  Loss :   0.013679041075493852  Accuracy :  0.9959930313588851 Time  1.06 s\n",
            "Epoch :  8  Batch :  288  Loss :   0.013632573583739335  Accuracy :  0.9960069444444445 Time  1.05 s\n",
            "Epoch :  8  Batch :  289  Loss :   0.01362818172613399  Accuracy :  0.9960207612456747 Time  1.06 s\n",
            "Epoch :  8  Batch :  290  Loss :   0.01358290506323101  Accuracy :  0.9960344827586207 Time  1.05 s\n",
            "Epoch :  8  Batch :  291  Loss :   0.013549715106959992  Accuracy :  0.9960481099656358 Time  1.04 s\n",
            "Epoch :  8  Batch :  292  Loss :   0.013515951134288788  Accuracy :  0.9960616438356165 Time  1.04 s\n",
            "Epoch :  8  Batch :  293  Loss :   0.013470565668518222  Accuracy :  0.9960750853242321 Time  1.05 s\n",
            "Epoch :  8  Batch :  294  Loss :   0.01343683179084419  Accuracy :  0.9960884353741497 Time  1.05 s\n",
            "Epoch :  8  Batch :  295  Loss :   0.013419706561016629  Accuracy :  0.9961016949152542 Time  1.04 s\n",
            "Epoch :  8  Batch :  296  Loss :   0.013379207105842667  Accuracy :  0.9961148648648649 Time  1.06 s\n",
            "Epoch :  8  Batch :  297  Loss :   0.013364216596503924  Accuracy :  0.9961279461279461 Time  1.07 s\n",
            "Epoch :  8  Batch :  298  Loss :   0.013331782085234658  Accuracy :  0.9961409395973154 Time  1.07 s\n",
            "Epoch :  8  Batch :  299  Loss :   0.013289563921558589  Accuracy :  0.9961538461538462 Time  1.06 s\n",
            "Epoch :  8  Batch :  300  Loss :   0.013262404296498668  Accuracy :  0.9961666666666666 Time  1.04 s\n",
            "Epoch :  8  Batch :  301  Loss :   0.013230304478114994  Accuracy :  0.9961794019933555 Time  1.05 s\n",
            "Epoch :  8  Batch :  302  Loss :   0.013309505706395112  Accuracy :  0.9961920529801325 Time  1.05 s\n",
            "Epoch :  8  Batch :  303  Loss :   0.013528702633020086  Accuracy :  0.996039603960396 Time  1.05 s\n",
            "Epoch :  8  Batch :  304  Loss :   0.013488795976244498  Accuracy :  0.9960526315789474 Time  1.05 s\n",
            "Epoch :  8  Batch :  305  Loss :   0.013446771391413603  Accuracy :  0.9960655737704918 Time  1.05 s\n",
            "Epoch :  8  Batch :  306  Loss :   0.013456283588929854  Accuracy :  0.996078431372549 Time  1.05 s\n",
            "Epoch :  8  Batch :  307  Loss :   0.013417886398459257  Accuracy :  0.9960848287112561 Time  0.56 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 99 %\n",
            "[8 epoch] Accuracy of the network on the Training images: 99 %\n",
            "Epoch :  9  Batch :  1  Loss :   0.00043633003951981664  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  9  Batch :  2  Loss :   0.0017632224189583212  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  9  Batch :  3  Loss :   0.0016040108360660572  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  9  Batch :  4  Loss :   0.0013810953678330407  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  9  Batch :  5  Loss :   0.00937299121869728  Accuracy :  0.99 Time  1.05 s\n",
            "Epoch :  9  Batch :  6  Loss :   0.007910505548352376  Accuracy :  0.9916666666666667 Time  1.05 s\n",
            "Epoch :  9  Batch :  7  Loss :   0.007089531639524337  Accuracy :  0.9928571428571429 Time  1.05 s\n",
            "Epoch :  9  Batch :  8  Loss :   0.00631518368027173  Accuracy :  0.99375 Time  1.05 s\n",
            "Epoch :  9  Batch :  9  Loss :   0.005775632000424796  Accuracy :  0.9944444444444445 Time  1.05 s\n",
            "Epoch :  9  Batch :  10  Loss :   0.005289304372854531  Accuracy :  0.995 Time  1.05 s\n",
            "Epoch :  9  Batch :  11  Loss :   0.005115139107643204  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  9  Batch :  12  Loss :   0.004759553388187972  Accuracy :  0.9958333333333333 Time  1.06 s\n",
            "Epoch :  9  Batch :  13  Loss :   0.0044818061287514865  Accuracy :  0.9961538461538462 Time  1.06 s\n",
            "Epoch :  9  Batch :  14  Loss :   0.004182394088794743  Accuracy :  0.9964285714285714 Time  1.04 s\n",
            "Epoch :  9  Batch :  15  Loss :   0.004004688632752125  Accuracy :  0.9966666666666667 Time  1.04 s\n",
            "Epoch :  9  Batch :  16  Loss :   0.0038129353288240964  Accuracy :  0.996875 Time  1.05 s\n",
            "Epoch :  9  Batch :  17  Loss :   0.003706965521868209  Accuracy :  0.9970588235294118 Time  1.04 s\n",
            "Epoch :  9  Batch :  18  Loss :   0.0037480250030057505  Accuracy :  0.9972222222222222 Time  1.04 s\n",
            "Epoch :  9  Batch :  19  Loss :   0.0037854957519907898  Accuracy :  0.9973684210526316 Time  1.05 s\n",
            "Epoch :  9  Batch :  20  Loss :   0.0036847774012130686  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  9  Batch :  21  Loss :   0.003539701659021722  Accuracy :  0.9976190476190476 Time  1.05 s\n",
            "Epoch :  9  Batch :  22  Loss :   0.003401498192529702  Accuracy :  0.9977272727272727 Time  1.04 s\n",
            "Epoch :  9  Batch :  23  Loss :   0.003378873132928477  Accuracy :  0.9978260869565218 Time  1.06 s\n",
            "Epoch :  9  Batch :  24  Loss :   0.0032703287518719057  Accuracy :  0.9979166666666667 Time  1.06 s\n",
            "Epoch :  9  Batch :  25  Loss :   0.0032636272895615547  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  9  Batch :  26  Loss :   0.0031474183555334234  Accuracy :  0.9980769230769231 Time  1.05 s\n",
            "Epoch :  9  Batch :  27  Loss :   0.008400534275332812  Accuracy :  0.9962962962962963 Time  1.05 s\n",
            "Epoch :  9  Batch :  28  Loss :   0.008125869810880561  Accuracy :  0.9964285714285714 Time  1.04 s\n",
            "Epoch :  9  Batch :  29  Loss :   0.00786883450827786  Accuracy :  0.996551724137931 Time  1.04 s\n",
            "Epoch :  9  Batch :  30  Loss :   0.007629823277238756  Accuracy :  0.9966666666666667 Time  1.04 s\n",
            "Epoch :  9  Batch :  31  Loss :   0.007628832654576869  Accuracy :  0.9967741935483871 Time  1.05 s\n",
            "Epoch :  9  Batch :  32  Loss :   0.007400339613923279  Accuracy :  0.996875 Time  1.05 s\n",
            "Epoch :  9  Batch :  33  Loss :   0.007185969446171922  Accuracy :  0.996969696969697 Time  1.05 s\n",
            "Epoch :  9  Batch :  34  Loss :   0.007034655435264166  Accuracy :  0.9970588235294118 Time  1.06 s\n",
            "Epoch :  9  Batch :  35  Loss :   0.007131759124708229  Accuracy :  0.9971428571428571 Time  1.08 s\n",
            "Epoch :  9  Batch :  36  Loss :   0.007015130063033818  Accuracy :  0.9972222222222222 Time  1.07 s\n",
            "Epoch :  9  Batch :  37  Loss :   0.00683593177362471  Accuracy :  0.9972972972972973 Time  1.04 s\n",
            "Epoch :  9  Batch :  38  Loss :   0.006660522757508596  Accuracy :  0.9973684210526316 Time  1.05 s\n",
            "Epoch :  9  Batch :  39  Loss :   0.006649738284371172  Accuracy :  0.9974358974358974 Time  1.05 s\n",
            "Epoch :  9  Batch :  40  Loss :   0.006535412603261648  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  9  Batch :  41  Loss :   0.006410913410590871  Accuracy :  0.9975609756097561 Time  1.05 s\n",
            "Epoch :  9  Batch :  42  Loss :   0.0062947714350663035  Accuracy :  0.9976190476190476 Time  1.05 s\n",
            "Epoch :  9  Batch :  43  Loss :   0.006249882448004385  Accuracy :  0.9976744186046511 Time  1.05 s\n",
            "Epoch :  9  Batch :  44  Loss :   0.006179796890964181  Accuracy :  0.9977272727272727 Time  1.04 s\n",
            "Epoch :  9  Batch :  45  Loss :   0.006052624778951415  Accuracy :  0.9977777777777778 Time  1.05 s\n",
            "Epoch :  9  Batch :  46  Loss :   0.006179602060634035  Accuracy :  0.9978260869565218 Time  1.06 s\n",
            "Epoch :  9  Batch :  47  Loss :   0.006071808466197368  Accuracy :  0.997872340425532 Time  1.05 s\n",
            "Epoch :  9  Batch :  48  Loss :   0.0059565860216631945  Accuracy :  0.9979166666666667 Time  1.04 s\n",
            "Epoch :  9  Batch :  49  Loss :   0.005870115048161765  Accuracy :  0.9979591836734694 Time  1.05 s\n",
            "Epoch :  9  Batch :  50  Loss :   0.005769238458597102  Accuracy :  0.998 Time  1.04 s\n",
            "Epoch :  9  Batch :  51  Loss :   0.006874625795342795  Accuracy :  0.9970588235294118 Time  1.05 s\n",
            "Epoch :  9  Batch :  52  Loss :   0.006821749387689544  Accuracy :  0.9971153846153846 Time  1.04 s\n",
            "Epoch :  9  Batch :  53  Loss :   0.006753834211025914  Accuracy :  0.9971698113207547 Time  1.05 s\n",
            "Epoch :  9  Batch :  54  Loss :   0.006683181363903849  Accuracy :  0.9972222222222222 Time  1.05 s\n",
            "Epoch :  9  Batch :  55  Loss :   0.006574559256709604  Accuracy :  0.9972727272727273 Time  1.05 s\n",
            "Epoch :  9  Batch :  56  Loss :   0.007002429082993851  Accuracy :  0.9973214285714286 Time  1.06 s\n",
            "Epoch :  9  Batch :  57  Loss :   0.006885843040531893  Accuracy :  0.9973684210526316 Time  1.07 s\n",
            "Epoch :  9  Batch :  58  Loss :   0.007019399456049572  Accuracy :  0.9974137931034482 Time  1.06 s\n",
            "Epoch :  9  Batch :  59  Loss :   0.007080731565681271  Accuracy :  0.997457627118644 Time  1.06 s\n",
            "Epoch :  9  Batch :  60  Loss :   0.006965742130220557  Accuracy :  0.9975 Time  1.05 s\n",
            "Epoch :  9  Batch :  61  Loss :   0.006871495857743211  Accuracy :  0.9975409836065574 Time  1.05 s\n",
            "Epoch :  9  Batch :  62  Loss :   0.006786194973580179  Accuracy :  0.9975806451612903 Time  1.05 s\n",
            "Epoch :  9  Batch :  63  Loss :   0.006680887162690938  Accuracy :  0.9976190476190476 Time  1.05 s\n",
            "Epoch :  9  Batch :  64  Loss :   0.006711628054972607  Accuracy :  0.99765625 Time  1.05 s\n",
            "Epoch :  9  Batch :  65  Loss :   0.006614286941700042  Accuracy :  0.9976923076923077 Time  1.05 s\n",
            "Epoch :  9  Batch :  66  Loss :   0.0065165605860370715  Accuracy :  0.9977272727272727 Time  1.05 s\n",
            "Epoch :  9  Batch :  67  Loss :   0.006426501868217975  Accuracy :  0.9977611940298508 Time  1.06 s\n",
            "Epoch :  9  Batch :  68  Loss :   0.006361241242035141  Accuracy :  0.9977941176470588 Time  1.06 s\n",
            "Epoch :  9  Batch :  69  Loss :   0.006280438043388025  Accuracy :  0.9978260869565218 Time  1.07 s\n",
            "Epoch :  9  Batch :  70  Loss :   0.006359385659327797  Accuracy :  0.9978571428571429 Time  1.05 s\n",
            "Epoch :  9  Batch :  71  Loss :   0.006274192855721274  Accuracy :  0.997887323943662 Time  1.05 s\n",
            "Epoch :  9  Batch :  72  Loss :   0.006187777476295903  Accuracy :  0.9979166666666667 Time  1.04 s\n",
            "Epoch :  9  Batch :  73  Loss :   0.0061272233835726895  Accuracy :  0.9979452054794521 Time  1.05 s\n",
            "Epoch :  9  Batch :  74  Loss :   0.006058781403004793  Accuracy :  0.9979729729729729 Time  1.04 s\n",
            "Epoch :  9  Batch :  75  Loss :   0.006006168004387291  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  9  Batch :  76  Loss :   0.005944823305454568  Accuracy :  0.9980263157894737 Time  1.05 s\n",
            "Epoch :  9  Batch :  77  Loss :   0.005869071803317638  Accuracy :  0.9980519480519481 Time  1.05 s\n",
            "Epoch :  9  Batch :  78  Loss :   0.0058037966429811865  Accuracy :  0.9980769230769231 Time  1.06 s\n",
            "Epoch :  9  Batch :  79  Loss :   0.005735419303831966  Accuracy :  0.9981012658227848 Time  1.06 s\n",
            "Epoch :  9  Batch :  80  Loss :   0.0057262364564394375  Accuracy :  0.998125 Time  1.06 s\n",
            "Epoch :  9  Batch :  81  Loss :   0.005672838988063216  Accuracy :  0.9981481481481481 Time  1.05 s\n",
            "Epoch :  9  Batch :  82  Loss :   0.005927136975027146  Accuracy :  0.9981707317073171 Time  1.05 s\n",
            "Epoch :  9  Batch :  83  Loss :   0.006305959693575909  Accuracy :  0.9981927710843373 Time  1.05 s\n",
            "Epoch :  9  Batch :  84  Loss :   0.006232603973720481  Accuracy :  0.9982142857142857 Time  1.04 s\n",
            "Epoch :  9  Batch :  85  Loss :   0.006161954839284162  Accuracy :  0.9982352941176471 Time  1.05 s\n",
            "Epoch :  9  Batch :  86  Loss :   0.006095477780755298  Accuracy :  0.9982558139534884 Time  1.05 s\n",
            "Epoch :  9  Batch :  87  Loss :   0.006030997226154231  Accuracy :  0.9982758620689656 Time  1.05 s\n",
            "Epoch :  9  Batch :  88  Loss :   0.005997181409218849  Accuracy :  0.9982954545454545 Time  1.04 s\n",
            "Epoch :  9  Batch :  89  Loss :   0.005930702119084334  Accuracy :  0.998314606741573 Time  1.06 s\n",
            "Epoch :  9  Batch :  90  Loss :   0.0059192917265641475  Accuracy :  0.9983333333333333 Time  1.06 s\n",
            "Epoch :  9  Batch :  91  Loss :   0.00585788113376411  Accuracy :  0.9983516483516484 Time  1.07 s\n",
            "Epoch :  9  Batch :  92  Loss :   0.005809600417666408  Accuracy :  0.9983695652173913 Time  1.05 s\n",
            "Epoch :  9  Batch :  93  Loss :   0.005757715028041901  Accuracy :  0.9983870967741936 Time  1.05 s\n",
            "Epoch :  9  Batch :  94  Loss :   0.005724460320747544  Accuracy :  0.9984042553191489 Time  1.04 s\n",
            "Epoch :  9  Batch :  95  Loss :   0.005666787956809123  Accuracy :  0.998421052631579 Time  1.05 s\n",
            "Epoch :  9  Batch :  96  Loss :   0.005610186111539406  Accuracy :  0.9984375 Time  1.05 s\n",
            "Epoch :  9  Batch :  97  Loss :   0.005552822997508217  Accuracy :  0.9984536082474227 Time  1.05 s\n",
            "Epoch :  9  Batch :  98  Loss :   0.0055136501833119985  Accuracy :  0.9984693877551021 Time  1.05 s\n",
            "Epoch :  9  Batch :  99  Loss :   0.005476288571420203  Accuracy :  0.9984848484848485 Time  1.05 s\n",
            "Epoch :  9  Batch :  100  Loss :   0.0054360118763361245  Accuracy :  0.9985 Time  1.05 s\n",
            "Epoch :  9  Batch :  101  Loss :   0.005468919455878595  Accuracy :  0.9985148514851485 Time  1.06 s\n",
            "Epoch :  9  Batch :  102  Loss :   0.005420793995939426  Accuracy :  0.9985294117647059 Time  1.07 s\n",
            "Epoch :  9  Batch :  103  Loss :   0.00537253942107358  Accuracy :  0.9985436893203884 Time  1.05 s\n",
            "Epoch :  9  Batch :  104  Loss :   0.005327381054450178  Accuracy :  0.9985576923076923 Time  1.05 s\n",
            "Epoch :  9  Batch :  105  Loss :   0.00528149493499465  Accuracy :  0.9985714285714286 Time  1.05 s\n",
            "Epoch :  9  Batch :  106  Loss :   0.005239848576477599  Accuracy :  0.9985849056603774 Time  1.05 s\n",
            "Epoch :  9  Batch :  107  Loss :   0.005201425932835468  Accuracy :  0.9985981308411215 Time  1.04 s\n",
            "Epoch :  9  Batch :  108  Loss :   0.005172535292735524  Accuracy :  0.9986111111111111 Time  1.05 s\n",
            "Epoch :  9  Batch :  109  Loss :   0.005131853182236461  Accuracy :  0.9986238532110092 Time  1.04 s\n",
            "Epoch :  9  Batch :  110  Loss :   0.00511087523948597  Accuracy :  0.9986363636363637 Time  1.05 s\n",
            "Epoch :  9  Batch :  111  Loss :   0.005079237382020225  Accuracy :  0.9986486486486487 Time  1.06 s\n",
            "Epoch :  9  Batch :  112  Loss :   0.00503554479957659  Accuracy :  0.9986607142857142 Time  1.06 s\n",
            "Epoch :  9  Batch :  113  Loss :   0.005801226832370523  Accuracy :  0.9982300884955753 Time  1.07 s\n",
            "Epoch :  9  Batch :  114  Loss :   0.005752151702453327  Accuracy :  0.9982456140350877 Time  1.05 s\n",
            "Epoch :  9  Batch :  115  Loss :   0.005705259521346306  Accuracy :  0.9982608695652174 Time  1.04 s\n",
            "Epoch :  9  Batch :  116  Loss :   0.005658046330044554  Accuracy :  0.9982758620689656 Time  1.04 s\n",
            "Epoch :  9  Batch :  117  Loss :   0.005630762268184399  Accuracy :  0.9982905982905983 Time  1.05 s\n",
            "Epoch :  9  Batch :  118  Loss :   0.0055869570088839665  Accuracy :  0.9983050847457627 Time  1.05 s\n",
            "Epoch :  9  Batch :  119  Loss :   0.005540841308557145  Accuracy :  0.9983193277310924 Time  1.05 s\n",
            "Epoch :  9  Batch :  120  Loss :   0.005499210652093704  Accuracy :  0.9983333333333333 Time  1.05 s\n",
            "Epoch :  9  Batch :  121  Loss :   0.005890246766540251  Accuracy :  0.9979338842975206 Time  1.05 s\n",
            "Epoch :  9  Batch :  122  Loss :   0.005843090156796264  Accuracy :  0.9979508196721312 Time  1.06 s\n",
            "Epoch :  9  Batch :  123  Loss :   0.005797672494071132  Accuracy :  0.9979674796747967 Time  1.06 s\n",
            "Epoch :  9  Batch :  124  Loss :   0.005752390237977388  Accuracy :  0.9979838709677419 Time  1.07 s\n",
            "Epoch :  9  Batch :  125  Loss :   0.005707467271218775  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  9  Batch :  126  Loss :   0.005670015028665639  Accuracy :  0.998015873015873 Time  1.05 s\n",
            "Epoch :  9  Batch :  127  Loss :   0.0056380521073581295  Accuracy :  0.9980314960629921 Time  1.05 s\n",
            "Epoch :  9  Batch :  128  Loss :   0.005647392036934207  Accuracy :  0.998046875 Time  1.04 s\n",
            "Epoch :  9  Batch :  129  Loss :   0.0057046712412444295  Accuracy :  0.998062015503876 Time  1.05 s\n",
            "Epoch :  9  Batch :  130  Loss :   0.005662340678519775  Accuracy :  0.9980769230769231 Time  1.04 s\n",
            "Epoch :  9  Batch :  131  Loss :   0.00563162799003398  Accuracy :  0.9980916030534351 Time  1.05 s\n",
            "Epoch :  9  Batch :  132  Loss :   0.005591254841282066  Accuracy :  0.9981060606060606 Time  1.04 s\n",
            "Epoch :  9  Batch :  133  Loss :   0.005628810107791787  Accuracy :  0.9981203007518797 Time  1.06 s\n",
            "Epoch :  9  Batch :  134  Loss :   0.005588363691852315  Accuracy :  0.9981343283582089 Time  1.06 s\n",
            "Epoch :  9  Batch :  135  Loss :   0.00554756115724794  Accuracy :  0.9981481481481481 Time  1.06 s\n",
            "Epoch :  9  Batch :  136  Loss :   0.005550322381890617  Accuracy :  0.9981617647058824 Time  1.05 s\n",
            "Epoch :  9  Batch :  137  Loss :   0.005511413478409577  Accuracy :  0.9981751824817519 Time  1.04 s\n",
            "Epoch :  9  Batch :  138  Loss :   0.005474386927269971  Accuracy :  0.9981884057971014 Time  1.05 s\n",
            "Epoch :  9  Batch :  139  Loss :   0.005448582264624675  Accuracy :  0.9982014388489209 Time  1.04 s\n",
            "Epoch :  9  Batch :  140  Loss :   0.005423038740445918  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  9  Batch :  141  Loss :   0.00539307248045782  Accuracy :  0.99822695035461 Time  1.04 s\n",
            "Epoch :  9  Batch :  142  Loss :   0.005358140224185699  Accuracy :  0.9982394366197183 Time  1.05 s\n",
            "Epoch :  9  Batch :  143  Loss :   0.005321350670872473  Accuracy :  0.9982517482517482 Time  1.05 s\n",
            "Epoch :  9  Batch :  144  Loss :   0.005285382658485105  Accuracy :  0.9982638888888888 Time  1.07 s\n",
            "Epoch :  9  Batch :  145  Loss :   0.0052924490337318275  Accuracy :  0.9982758620689656 Time  1.06 s\n",
            "Epoch :  9  Batch :  146  Loss :   0.005265922761269668  Accuracy :  0.9982876712328768 Time  1.06 s\n",
            "Epoch :  9  Batch :  147  Loss :   0.005314968479267633  Accuracy :  0.9982993197278912 Time  1.05 s\n",
            "Epoch :  9  Batch :  148  Loss :   0.005279497498982842  Accuracy :  0.9983108108108109 Time  1.05 s\n",
            "Epoch :  9  Batch :  149  Loss :   0.005249483524793376  Accuracy :  0.9983221476510067 Time  1.05 s\n",
            "Epoch :  9  Batch :  150  Loss :   0.005246775188982914  Accuracy :  0.9983333333333333 Time  1.04 s\n",
            "Epoch :  9  Batch :  151  Loss :   0.005904870355899395  Accuracy :  0.9980132450331126 Time  1.04 s\n",
            "Epoch :  9  Batch :  152  Loss :   0.005913817343537774  Accuracy :  0.9980263157894737 Time  1.05 s\n",
            "Epoch :  9  Batch :  153  Loss :   0.0058758577731697825  Accuracy :  0.9980392156862745 Time  1.05 s\n",
            "Epoch :  9  Batch :  154  Loss :   0.005840319572291571  Accuracy :  0.9980519480519481 Time  1.04 s\n",
            "Epoch :  9  Batch :  155  Loss :   0.006154728813670508  Accuracy :  0.997741935483871 Time  1.06 s\n",
            "Epoch :  9  Batch :  156  Loss :   0.006140504053898137  Accuracy :  0.9977564102564103 Time  1.06 s\n",
            "Epoch :  9  Batch :  157  Loss :   0.006103189880539816  Accuracy :  0.9977707006369426 Time  1.06 s\n",
            "Epoch :  9  Batch :  158  Loss :   0.006091916290124918  Accuracy :  0.9977848101265823 Time  1.05 s\n",
            "Epoch :  9  Batch :  159  Loss :   0.006094065291726333  Accuracy :  0.9977987421383647 Time  1.04 s\n",
            "Epoch :  9  Batch :  160  Loss :   0.006060123074053081  Accuracy :  0.9978125 Time  1.05 s\n",
            "Epoch :  9  Batch :  161  Loss :   0.0060234184959469155  Accuracy :  0.9978260869565218 Time  1.05 s\n",
            "Epoch :  9  Batch :  162  Loss :   0.005988988713813151  Accuracy :  0.9978395061728395 Time  1.05 s\n",
            "Epoch :  9  Batch :  163  Loss :   0.0059548915025092125  Accuracy :  0.9978527607361963 Time  1.05 s\n",
            "Epoch :  9  Batch :  164  Loss :   0.005932489476036586  Accuracy :  0.9978658536585366 Time  1.05 s\n",
            "Epoch :  9  Batch :  165  Loss :   0.005897922781679716  Accuracy :  0.9978787878787879 Time  1.05 s\n",
            "Epoch :  9  Batch :  166  Loss :   0.005913361818882944  Accuracy :  0.9978915662650603 Time  1.06 s\n",
            "Epoch :  9  Batch :  167  Loss :   0.005880386779717592  Accuracy :  0.9979041916167665 Time  1.06 s\n",
            "Epoch :  9  Batch :  168  Loss :   0.0058522559091519115  Accuracy :  0.9979166666666667 Time  1.06 s\n",
            "Epoch :  9  Batch :  169  Loss :   0.005887866714453847  Accuracy :  0.9979289940828402 Time  1.06 s\n",
            "Epoch :  9  Batch :  170  Loss :   0.005855534962949391  Accuracy :  0.9979411764705882 Time  1.06 s\n",
            "Epoch :  9  Batch :  171  Loss :   0.005822739546075402  Accuracy :  0.997953216374269 Time  1.05 s\n",
            "Epoch :  9  Batch :  172  Loss :   0.00579444049145782  Accuracy :  0.9979651162790698 Time  1.05 s\n",
            "Epoch :  9  Batch :  173  Loss :   0.0058172842025017634  Accuracy :  0.9979768786127168 Time  1.05 s\n",
            "Epoch :  9  Batch :  174  Loss :   0.005797755444751371  Accuracy :  0.9979885057471264 Time  1.05 s\n",
            "Epoch :  9  Batch :  175  Loss :   0.005768116705813944  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  9  Batch :  176  Loss :   0.005738948244201806  Accuracy :  0.9980113636363637 Time  1.05 s\n",
            "Epoch :  9  Batch :  177  Loss :   0.00570943947091135  Accuracy :  0.9980225988700565 Time  1.05 s\n",
            "Epoch :  9  Batch :  178  Loss :   0.0056781358154203805  Accuracy :  0.9980337078651685 Time  1.06 s\n",
            "Epoch :  9  Batch :  179  Loss :   0.005655507181894506  Accuracy :  0.9980446927374301 Time  1.06 s\n",
            "Epoch :  9  Batch :  180  Loss :   0.005626038969199322  Accuracy :  0.9980555555555556 Time  1.07 s\n",
            "Epoch :  9  Batch :  181  Loss :   0.005596464816496151  Accuracy :  0.9980662983425415 Time  1.04 s\n",
            "Epoch :  9  Batch :  182  Loss :   0.005583488406925456  Accuracy :  0.9980769230769231 Time  1.05 s\n",
            "Epoch :  9  Batch :  183  Loss :   0.005571054577374356  Accuracy :  0.9980874316939891 Time  1.04 s\n",
            "Epoch :  9  Batch :  184  Loss :   0.0055454170267733825  Accuracy :  0.9980978260869565 Time  1.05 s\n",
            "Epoch :  9  Batch :  185  Loss :   0.005517957122218512  Accuracy :  0.9981081081081081 Time  1.04 s\n",
            "Epoch :  9  Batch :  186  Loss :   0.00549628128605799  Accuracy :  0.9981182795698925 Time  1.05 s\n",
            "Epoch :  9  Batch :  187  Loss :   0.005471783272954256  Accuracy :  0.9981283422459893 Time  1.06 s\n",
            "Epoch :  9  Batch :  188  Loss :   0.005443187432726344  Accuracy :  0.9981382978723404 Time  1.04 s\n",
            "Epoch :  9  Batch :  189  Loss :   0.005416244666997396  Accuracy :  0.9981481481481481 Time  1.06 s\n",
            "Epoch :  9  Batch :  190  Loss :   0.005388661756354084  Accuracy :  0.9981578947368421 Time  1.07 s\n",
            "Epoch :  9  Batch :  191  Loss :   0.005369356298158329  Accuracy :  0.9981675392670157 Time  1.06 s\n",
            "Epoch :  9  Batch :  192  Loss :   0.005502575907693578  Accuracy :  0.9981770833333333 Time  1.05 s\n",
            "Epoch :  9  Batch :  193  Loss :   0.0054752756343661975  Accuracy :  0.9981865284974093 Time  1.05 s\n",
            "Epoch :  9  Batch :  194  Loss :   0.005470774072556505  Accuracy :  0.9981958762886598 Time  1.05 s\n",
            "Epoch :  9  Batch :  195  Loss :   0.005446133677506646  Accuracy :  0.9982051282051282 Time  1.05 s\n",
            "Epoch :  9  Batch :  196  Loss :   0.005418478705079758  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  9  Batch :  197  Loss :   0.005393594442120311  Accuracy :  0.9982233502538072 Time  1.05 s\n",
            "Epoch :  9  Batch :  198  Loss :   0.00536672882004275  Accuracy :  0.9982323232323232 Time  1.04 s\n",
            "Epoch :  9  Batch :  199  Loss :   0.00534730144661707  Accuracy :  0.9982412060301508 Time  1.04 s\n",
            "Epoch :  9  Batch :  200  Loss :   0.0053254204781023875  Accuracy :  0.99825 Time  1.05 s\n",
            "Epoch :  9  Batch :  201  Loss :   0.005300328900072297  Accuracy :  0.9982587064676617 Time  1.06 s\n",
            "Epoch :  9  Batch :  202  Loss :   0.005283577175814031  Accuracy :  0.9982673267326733 Time  1.06 s\n",
            "Epoch :  9  Batch :  203  Loss :   0.0052579474012252585  Accuracy :  0.9982758620689656 Time  1.05 s\n",
            "Epoch :  9  Batch :  204  Loss :   0.005293689927108133  Accuracy :  0.9982843137254902 Time  1.04 s\n",
            "Epoch :  9  Batch :  205  Loss :   0.0052696144717895264  Accuracy :  0.9982926829268293 Time  1.05 s\n",
            "Epoch :  9  Batch :  206  Loss :   0.0052493348845036435  Accuracy :  0.9983009708737864 Time  1.05 s\n",
            "Epoch :  9  Batch :  207  Loss :   0.005224988684524624  Accuracy :  0.9983091787439613 Time  1.04 s\n",
            "Epoch :  9  Batch :  208  Loss :   0.005201095032545639  Accuracy :  0.9983173076923076 Time  1.05 s\n",
            "Epoch :  9  Batch :  209  Loss :   0.0051778699239962  Accuracy :  0.9983253588516746 Time  1.05 s\n",
            "Epoch :  9  Batch :  210  Loss :   0.0051541243674191285  Accuracy :  0.9983333333333333 Time  1.04 s\n",
            "Epoch :  9  Batch :  211  Loss :   0.005138230491840263  Accuracy :  0.9983412322274882 Time  1.06 s\n",
            "Epoch :  9  Batch :  212  Loss :   0.005120680755312005  Accuracy :  0.9983490566037736 Time  1.06 s\n",
            "Epoch :  9  Batch :  213  Loss :   0.005099920595292329  Accuracy :  0.998356807511737 Time  1.07 s\n",
            "Epoch :  9  Batch :  214  Loss :   0.005091434169473836  Accuracy :  0.9983644859813084 Time  1.05 s\n",
            "Epoch :  9  Batch :  215  Loss :   0.005227465666605493  Accuracy :  0.9983720930232558 Time  1.04 s\n",
            "Epoch :  9  Batch :  216  Loss :   0.00521262831385785  Accuracy :  0.9983796296296297 Time  1.05 s\n",
            "Epoch :  9  Batch :  217  Loss :   0.005192422967348535  Accuracy :  0.9983870967741936 Time  1.05 s\n",
            "Epoch :  9  Batch :  218  Loss :   0.005174288312028952  Accuracy :  0.9983944954128441 Time  1.04 s\n",
            "Epoch :  9  Batch :  219  Loss :   0.005151402630415502  Accuracy :  0.9984018264840183 Time  1.04 s\n",
            "Epoch :  9  Batch :  220  Loss :   0.00513032743175989  Accuracy :  0.9984090909090909 Time  1.04 s\n",
            "Epoch :  9  Batch :  221  Loss :   0.005121988224526917  Accuracy :  0.9984162895927602 Time  1.04 s\n",
            "Epoch :  9  Batch :  222  Loss :   0.005101286681549011  Accuracy :  0.9984234234234234 Time  1.06 s\n",
            "Epoch :  9  Batch :  223  Loss :   0.005093410667720772  Accuracy :  0.9984304932735426 Time  1.06 s\n",
            "Epoch :  9  Batch :  224  Loss :   0.005075068125401165  Accuracy :  0.9984375 Time  1.06 s\n",
            "Epoch :  9  Batch :  225  Loss :   0.005065405791707841  Accuracy :  0.9984444444444445 Time  1.05 s\n",
            "Epoch :  9  Batch :  226  Loss :   0.005043739854737452  Accuracy :  0.9984513274336283 Time  1.04 s\n",
            "Epoch :  9  Batch :  227  Loss :   0.005024164592755108  Accuracy :  0.9984581497797357 Time  1.05 s\n",
            "Epoch :  9  Batch :  228  Loss :   0.005003627026884746  Accuracy :  0.9984649122807018 Time  1.05 s\n",
            "Epoch :  9  Batch :  229  Loss :   0.005001463549860484  Accuracy :  0.998471615720524 Time  1.05 s\n",
            "Epoch :  9  Batch :  230  Loss :   0.00498066998564394  Accuracy :  0.9984782608695653 Time  1.05 s\n",
            "Epoch :  9  Batch :  231  Loss :   0.004961729446419168  Accuracy :  0.9984848484848485 Time  1.04 s\n",
            "Epoch :  9  Batch :  232  Loss :   0.004942389691826787  Accuracy :  0.9984913793103448 Time  1.05 s\n",
            "Epoch :  9  Batch :  233  Loss :   0.004930679600422214  Accuracy :  0.9984978540772532 Time  1.06 s\n",
            "Epoch :  9  Batch :  234  Loss :   0.004911831095030829  Accuracy :  0.9985042735042735 Time  1.06 s\n",
            "Epoch :  9  Batch :  235  Loss :   0.004891174394373245  Accuracy :  0.9985106382978723 Time  1.07 s\n",
            "Epoch :  9  Batch :  236  Loss :   0.004872668676113281  Accuracy :  0.9985169491525424 Time  1.05 s\n",
            "Epoch :  9  Batch :  237  Loss :   0.004854877119099381  Accuracy :  0.9985232067510549 Time  1.04 s\n",
            "Epoch :  9  Batch :  238  Loss :   0.004836260886032742  Accuracy :  0.9985294117647059 Time  1.04 s\n",
            "Epoch :  9  Batch :  239  Loss :   0.004817474674272181  Accuracy :  0.9985355648535564 Time  1.05 s\n",
            "Epoch :  9  Batch :  240  Loss :   0.004801825182607899  Accuracy :  0.9985416666666667 Time  1.05 s\n",
            "Epoch :  9  Batch :  241  Loss :   0.004782504383284689  Accuracy :  0.9985477178423237 Time  1.05 s\n",
            "Epoch :  9  Batch :  242  Loss :   0.004762932196916319  Accuracy :  0.9985537190082645 Time  1.05 s\n",
            "Epoch :  9  Batch :  243  Loss :   0.004743872501234581  Accuracy :  0.998559670781893 Time  1.04 s\n",
            "Epoch :  9  Batch :  244  Loss :   0.004724686432553903  Accuracy :  0.9985655737704918 Time  1.07 s\n",
            "Epoch :  9  Batch :  245  Loss :   0.004712328571035962  Accuracy :  0.9985714285714286 Time  1.06 s\n",
            "Epoch :  9  Batch :  246  Loss :   0.004693740291369933  Accuracy :  0.9985772357723577 Time  1.06 s\n",
            "Epoch :  9  Batch :  247  Loss :   0.004679088185394693  Accuracy :  0.998582995951417 Time  1.05 s\n",
            "Epoch :  9  Batch :  248  Loss :   0.004661217369344271  Accuracy :  0.9985887096774193 Time  1.04 s\n",
            "Epoch :  9  Batch :  249  Loss :   0.004644811344771012  Accuracy :  0.9985943775100402 Time  1.04 s\n",
            "Epoch :  9  Batch :  250  Loss :   0.004633935269572248  Accuracy :  0.9986 Time  1.04 s\n",
            "Epoch :  9  Batch :  251  Loss :   0.004616378731648491  Accuracy :  0.9986055776892431 Time  1.05 s\n",
            "Epoch :  9  Batch :  252  Loss :   0.004780266510893159  Accuracy :  0.9984126984126984 Time  1.05 s\n",
            "Epoch :  9  Batch :  253  Loss :   0.004763478527360413  Accuracy :  0.9984189723320158 Time  1.04 s\n",
            "Epoch :  9  Batch :  254  Loss :   0.004745231006489221  Accuracy :  0.9984251968503937 Time  1.05 s\n",
            "Epoch :  9  Batch :  255  Loss :   0.0047350365295535405  Accuracy :  0.9984313725490196 Time  1.06 s\n",
            "Epoch :  9  Batch :  256  Loss :   0.004732407552317852  Accuracy :  0.9984375 Time  1.06 s\n",
            "Epoch :  9  Batch :  257  Loss :   0.004718230461658661  Accuracy :  0.998443579766537 Time  1.06 s\n",
            "Epoch :  9  Batch :  258  Loss :   0.0047020848039541615  Accuracy :  0.9984496124031008 Time  1.05 s\n",
            "Epoch :  9  Batch :  259  Loss :   0.004685889517718017  Accuracy :  0.9984555984555985 Time  1.05 s\n",
            "Epoch :  9  Batch :  260  Loss :   0.004669215009555326  Accuracy :  0.9984615384615385 Time  1.04 s\n",
            "Epoch :  9  Batch :  261  Loss :   0.004651439539968082  Accuracy :  0.9984674329501916 Time  1.04 s\n",
            "Epoch :  9  Batch :  262  Loss :   0.0046355674845847  Accuracy :  0.9984732824427481 Time  1.05 s\n",
            "Epoch :  9  Batch :  263  Loss :   0.004630824451045617  Accuracy :  0.9984790874524715 Time  1.05 s\n",
            "Epoch :  9  Batch :  264  Loss :   0.004701389203643031  Accuracy :  0.9984848484848485 Time  1.04 s\n",
            "Epoch :  9  Batch :  265  Loss :   0.0046843942485827695  Accuracy :  0.9984905660377359 Time  1.04 s\n",
            "Epoch :  9  Batch :  266  Loss :   0.004818918857689228  Accuracy :  0.9984962406015038 Time  1.06 s\n",
            "Epoch :  9  Batch :  267  Loss :   0.005173824157489857  Accuracy :  0.998314606741573 Time  1.07 s\n",
            "Epoch :  9  Batch :  268  Loss :   0.005156159719711047  Accuracy :  0.998320895522388 Time  1.06 s\n",
            "Epoch :  9  Batch :  269  Loss :   0.005137578039598321  Accuracy :  0.9983271375464684 Time  1.05 s\n",
            "Epoch :  9  Batch :  270  Loss :   0.005119416385625502  Accuracy :  0.9983333333333333 Time  1.05 s\n",
            "Epoch :  9  Batch :  271  Loss :   0.005123353345655021  Accuracy :  0.9983394833948339 Time  1.04 s\n",
            "Epoch :  9  Batch :  272  Loss :   0.005372506572110283  Accuracy :  0.9981617647058824 Time  1.05 s\n",
            "Epoch :  9  Batch :  273  Loss :   0.005353049245655418  Accuracy :  0.9981684981684982 Time  1.05 s\n",
            "Epoch :  9  Batch :  274  Loss :   0.005606585376164438  Accuracy :  0.997992700729927 Time  1.05 s\n",
            "Epoch :  9  Batch :  275  Loss :   0.005592766781509537  Accuracy :  0.998 Time  1.04 s\n",
            "Epoch :  9  Batch :  276  Loss :   0.0055737203578035514  Accuracy :  0.9980072463768116 Time  1.05 s\n",
            "Epoch :  9  Batch :  277  Loss :   0.005554146100017297  Accuracy :  0.998014440433213 Time  1.05 s\n",
            "Epoch :  9  Batch :  278  Loss :   0.005541080680428003  Accuracy :  0.9980215827338129 Time  1.06 s\n",
            "Epoch :  9  Batch :  279  Loss :   0.005527028131595914  Accuracy :  0.9980286738351255 Time  1.06 s\n",
            "Epoch :  9  Batch :  280  Loss :   0.005509851045605631  Accuracy :  0.9980357142857142 Time  1.05 s\n",
            "Epoch :  9  Batch :  281  Loss :   0.005500540494473152  Accuracy :  0.9980427046263345 Time  1.05 s\n",
            "Epoch :  9  Batch :  282  Loss :   0.00548179785953267  Accuracy :  0.9980496453900709 Time  1.05 s\n",
            "Epoch :  9  Batch :  283  Loss :   0.005463925230956177  Accuracy :  0.9980565371024736 Time  1.05 s\n",
            "Epoch :  9  Batch :  284  Loss :   0.005454272526510746  Accuracy :  0.9980633802816902 Time  1.04 s\n",
            "Epoch :  9  Batch :  285  Loss :   0.005440094152410829  Accuracy :  0.9980701754385964 Time  1.05 s\n",
            "Epoch :  9  Batch :  286  Loss :   0.005423052034504533  Accuracy :  0.9980769230769231 Time  1.04 s\n",
            "Epoch :  9  Batch :  287  Loss :   0.005409245143966682  Accuracy :  0.9980836236933798 Time  1.04 s\n",
            "Epoch :  9  Batch :  288  Loss :   0.005394831537829405  Accuracy :  0.9980902777777778 Time  1.07 s\n",
            "Epoch :  9  Batch :  289  Loss :   0.005398183624757309  Accuracy :  0.9980968858131488 Time  1.06 s\n",
            "Epoch :  9  Batch :  290  Loss :   0.005385954964643541  Accuracy :  0.9981034482758621 Time  1.06 s\n",
            "Epoch :  9  Batch :  291  Loss :   0.005367915058953874  Accuracy :  0.9981099656357388 Time  1.05 s\n",
            "Epoch :  9  Batch :  292  Loss :   0.005375955352027544  Accuracy :  0.9981164383561644 Time  1.05 s\n",
            "Epoch :  9  Batch :  293  Loss :   0.005377384893701562  Accuracy :  0.9981228668941979 Time  1.05 s\n",
            "Epoch :  9  Batch :  294  Loss :   0.005968224744385819  Accuracy :  0.9979591836734694 Time  1.04 s\n",
            "Epoch :  9  Batch :  295  Loss :   0.005951079361891015  Accuracy :  0.9979661016949153 Time  1.04 s\n",
            "Epoch :  9  Batch :  296  Loss :   0.005932844458006789  Accuracy :  0.9979729729729729 Time  1.04 s\n",
            "Epoch :  9  Batch :  297  Loss :   0.005921379177116649  Accuracy :  0.997979797979798 Time  1.05 s\n",
            "Epoch :  9  Batch :  298  Loss :   0.005903794425813619  Accuracy :  0.9979865771812081 Time  1.05 s\n",
            "Epoch :  9  Batch :  299  Loss :   0.005889033466838101  Accuracy :  0.9979933110367893 Time  1.05 s\n",
            "Epoch :  9  Batch :  300  Loss :   0.0058899513984336715  Accuracy :  0.998 Time  1.06 s\n",
            "Epoch :  9  Batch :  301  Loss :   0.005872875982787683  Accuracy :  0.9980066445182725 Time  1.07 s\n",
            "Epoch :  9  Batch :  302  Loss :   0.005866370020567347  Accuracy :  0.9980132450331126 Time  1.05 s\n",
            "Epoch :  9  Batch :  303  Loss :   0.0058707636628433845  Accuracy :  0.998019801980198 Time  1.05 s\n",
            "Epoch :  9  Batch :  304  Loss :   0.005851789970991281  Accuracy :  0.9980263157894737 Time  1.05 s\n",
            "Epoch :  9  Batch :  305  Loss :   0.005840125833206226  Accuracy :  0.9980327868852459 Time  1.04 s\n",
            "Epoch :  9  Batch :  306  Loss :   0.005822578081932947  Accuracy :  0.9980392156862745 Time  1.05 s\n",
            "Epoch :  9  Batch :  307  Loss :   0.005809097886581792  Accuracy :  0.998042414355628 Time  0.54 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 99 %\n",
            "[9 epoch] Accuracy of the network on the Training images: 99 %\n",
            "Epoch :  10  Batch :  1  Loss :   0.0006190404528751969  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  2  Loss :   0.00942912680329755  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  3  Loss :   0.008231679016413787  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  4  Loss :   0.006953015719773248  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  10  Batch :  5  Loss :   0.005589546507690102  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  10  Batch :  6  Loss :   0.004699768061982468  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  7  Loss :   0.005018397458895508  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  8  Loss :   0.0055285478811129  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  9  Loss :   0.004938148718792945  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  10  Loss :   0.0044713760871673  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  11  Loss :   0.004074822755435228  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  12  Loss :   0.0037533112335950136  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  13  Loss :   0.004133618006912561  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  14  Loss :   0.0039297281265524885  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  15  Loss :   0.0036954451197137436  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  10  Batch :  16  Loss :   0.0035069555160589516  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  10  Batch :  17  Loss :   0.0033110363883144386  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  10  Batch :  18  Loss :   0.003137676170606735  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  19  Loss :   0.0030534999088175888  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  20  Loss :   0.002945581127278274  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  21  Loss :   0.0028174211363962273  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  22  Loss :   0.002752041835099755  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  23  Loss :   0.002674611064519369  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  24  Loss :   0.002652444053940902  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  25  Loss :   0.002548250324034598  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  26  Loss :   0.002473678711584608  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  10  Batch :  27  Loss :   0.002519283791587912  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  10  Batch :  28  Loss :   0.0025590558598196367  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  10  Batch :  29  Loss :   0.0024883601830063933  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  30  Loss :   0.0026084717253979763  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  31  Loss :   0.0025833458609169258  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  32  Loss :   0.0025046659086456202  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  33  Loss :   0.0025865381363528836  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  34  Loss :   0.002516734363512917  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  35  Loss :   0.0025121420532481613  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  36  Loss :   0.0024576628099263567  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  37  Loss :   0.0023930787489941418  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  10  Batch :  38  Loss :   0.00233429510128764  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  10  Batch :  39  Loss :   0.002469369261234533  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  10  Batch :  40  Loss :   0.0024101249404338888  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  41  Loss :   0.0023537561895109987  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  42  Loss :   0.002300653589329505  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  43  Loss :   0.0022674772981825003  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  10  Batch :  44  Loss :   0.00223181685710601  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  10  Batch :  45  Loss :   0.0031479415076319127  Accuracy :  0.9988888888888889 Time  1.05 s\n",
            "Epoch :  10  Batch :  46  Loss :   0.0030987540787622415  Accuracy :  0.9989130434782608 Time  1.04 s\n",
            "Epoch :  10  Batch :  47  Loss :   0.0030360891275662692  Accuracy :  0.9989361702127659 Time  1.05 s\n",
            "Epoch :  10  Batch :  48  Loss :   0.0029742195592916687  Accuracy :  0.9989583333333333 Time  1.06 s\n",
            "Epoch :  10  Batch :  49  Loss :   0.0029175339649165316  Accuracy :  0.9989795918367347 Time  1.06 s\n",
            "Epoch :  10  Batch :  50  Loss :   0.003824011195683852  Accuracy :  0.998 Time  1.08 s\n",
            "Epoch :  10  Batch :  51  Loss :   0.003755736351926245  Accuracy :  0.9980392156862745 Time  1.05 s\n",
            "Epoch :  10  Batch :  52  Loss :   0.0037095678607539204  Accuracy :  0.9980769230769231 Time  1.05 s\n",
            "Epoch :  10  Batch :  53  Loss :   0.003642743481114454  Accuracy :  0.9981132075471698 Time  1.05 s\n",
            "Epoch :  10  Batch :  54  Loss :   0.0035763874742163657  Accuracy :  0.9981481481481481 Time  1.05 s\n",
            "Epoch :  10  Batch :  55  Loss :   0.0035123367094043218  Accuracy :  0.9981818181818182 Time  1.05 s\n",
            "Epoch :  10  Batch :  56  Loss :   0.0034522921558683656  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  10  Batch :  57  Loss :   0.003393511523889876  Accuracy :  0.9982456140350877 Time  1.05 s\n",
            "Epoch :  10  Batch :  58  Loss :   0.003350105707795592  Accuracy :  0.9982758620689656 Time  1.05 s\n",
            "Epoch :  10  Batch :  59  Loss :   0.003316532596634993  Accuracy :  0.9983050847457627 Time  1.05 s\n",
            "Epoch :  10  Batch :  60  Loss :   0.0032698216354522932  Accuracy :  0.9983333333333333 Time  1.07 s\n",
            "Epoch :  10  Batch :  61  Loss :   0.0032168094170867603  Accuracy :  0.9983606557377049 Time  1.06 s\n",
            "Epoch :  10  Batch :  62  Loss :   0.003167501878652406  Accuracy :  0.9983870967741936 Time  1.05 s\n",
            "Epoch :  10  Batch :  63  Loss :   0.003120209812925803  Accuracy :  0.9984126984126984 Time  1.05 s\n",
            "Epoch :  10  Batch :  64  Loss :   0.003085226674556907  Accuracy :  0.9984375 Time  1.05 s\n",
            "Epoch :  10  Batch :  65  Loss :   0.00304703845982798  Accuracy :  0.9984615384615385 Time  1.05 s\n",
            "Epoch :  10  Batch :  66  Loss :   0.0030714982695468334  Accuracy :  0.9984848484848485 Time  1.06 s\n",
            "Epoch :  10  Batch :  67  Loss :   0.0030271910809053443  Accuracy :  0.9985074626865672 Time  1.06 s\n",
            "Epoch :  10  Batch :  68  Loss :   0.002984396187029608  Accuracy :  0.9985294117647059 Time  1.07 s\n",
            "Epoch :  10  Batch :  69  Loss :   0.002945359019234446  Accuracy :  0.9985507246376811 Time  1.05 s\n",
            "Epoch :  10  Batch :  70  Loss :   0.0029045329376198685  Accuracy :  0.9985714285714286 Time  1.07 s\n",
            "Epoch :  10  Batch :  71  Loss :   0.00286571129746194  Accuracy :  0.9985915492957746 Time  1.05 s\n",
            "Epoch :  10  Batch :  72  Loss :   0.0028294173574370993  Accuracy :  0.9986111111111111 Time  1.07 s\n",
            "Epoch :  10  Batch :  73  Loss :   0.0028296537602229416  Accuracy :  0.9986301369863013 Time  1.04 s\n",
            "Epoch :  10  Batch :  74  Loss :   0.0028578028769344027  Accuracy :  0.9986486486486487 Time  1.05 s\n",
            "Epoch :  10  Batch :  75  Loss :   0.002832286117967063  Accuracy :  0.9986666666666667 Time  1.04 s\n",
            "Epoch :  10  Batch :  76  Loss :   0.002804006612917108  Accuracy :  0.9986842105263158 Time  1.05 s\n",
            "Epoch :  10  Batch :  77  Loss :   0.0027681001723409847  Accuracy :  0.9987012987012988 Time  1.05 s\n",
            "Epoch :  10  Batch :  78  Loss :   0.0027359583931310903  Accuracy :  0.9987179487179487 Time  1.05 s\n",
            "Epoch :  10  Batch :  79  Loss :   0.0027018188998284396  Accuracy :  0.9987341772151899 Time  1.04 s\n",
            "Epoch :  10  Batch :  80  Loss :   0.002809870747569221  Accuracy :  0.99875 Time  1.04 s\n",
            "Epoch :  10  Batch :  81  Loss :   0.0032719633477146415  Accuracy :  0.9981481481481481 Time  1.06 s\n",
            "Epoch :  10  Batch :  82  Loss :   0.003235181117705041  Accuracy :  0.9981707317073171 Time  1.06 s\n",
            "Epoch :  10  Batch :  83  Loss :   0.003204440536641611  Accuracy :  0.9981927710843373 Time  1.06 s\n",
            "Epoch :  10  Batch :  84  Loss :   0.003169291743775137  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  10  Batch :  85  Loss :   0.003135734959762297  Accuracy :  0.9982352941176471 Time  1.05 s\n",
            "Epoch :  10  Batch :  86  Loss :   0.0031157256821326123  Accuracy :  0.9982558139534884 Time  1.05 s\n",
            "Epoch :  10  Batch :  87  Loss :   0.00308045721972487  Accuracy :  0.9982758620689656 Time  1.05 s\n",
            "Epoch :  10  Batch :  88  Loss :   0.0033446456029030906  Accuracy :  0.9982954545454545 Time  1.05 s\n",
            "Epoch :  10  Batch :  89  Loss :   0.003307913120847792  Accuracy :  0.998314606741573 Time  1.04 s\n",
            "Epoch :  10  Batch :  90  Loss :   0.0032911299528980936  Accuracy :  0.9983333333333333 Time  1.05 s\n",
            "Epoch :  10  Batch :  91  Loss :   0.0032567986127905097  Accuracy :  0.9983516483516484 Time  1.05 s\n",
            "Epoch :  10  Batch :  92  Loss :   0.0032236398675076603  Accuracy :  0.9983695652173913 Time  1.06 s\n",
            "Epoch :  10  Batch :  93  Loss :   0.0031965112969872633  Accuracy :  0.9983870967741936 Time  1.06 s\n",
            "Epoch :  10  Batch :  94  Loss :   0.003178823944410319  Accuracy :  0.9984042553191489 Time  1.07 s\n",
            "Epoch :  10  Batch :  95  Loss :   0.0031521864763865443  Accuracy :  0.998421052631579 Time  1.05 s\n",
            "Epoch :  10  Batch :  96  Loss :   0.0031457623179752168  Accuracy :  0.9984375 Time  1.04 s\n",
            "Epoch :  10  Batch :  97  Loss :   0.0031239299762804425  Accuracy :  0.9984536082474227 Time  1.05 s\n",
            "Epoch :  10  Batch :  98  Loss :   0.0030933755784941725  Accuracy :  0.9984693877551021 Time  1.05 s\n",
            "Epoch :  10  Batch :  99  Loss :   0.003065493204478043  Accuracy :  0.9984848484848485 Time  1.05 s\n",
            "Epoch :  10  Batch :  100  Loss :   0.0030450502128223887  Accuracy :  0.9985 Time  1.05 s\n",
            "Epoch :  10  Batch :  101  Loss :   0.0035503894401285427  Accuracy :  0.998019801980198 Time  1.05 s\n",
            "Epoch :  10  Batch :  102  Loss :   0.0037347681032306535  Accuracy :  0.9980392156862745 Time  1.05 s\n",
            "Epoch :  10  Batch :  103  Loss :   0.0037243700090646564  Accuracy :  0.9980582524271845 Time  1.06 s\n",
            "Epoch :  10  Batch :  104  Loss :   0.003832912781945197  Accuracy :  0.9980769230769231 Time  1.07 s\n",
            "Epoch :  10  Batch :  105  Loss :   0.004338762653891795  Accuracy :  0.9976190476190476 Time  1.07 s\n",
            "Epoch :  10  Batch :  106  Loss :   0.004299096938306114  Accuracy :  0.9976415094339622 Time  1.05 s\n",
            "Epoch :  10  Batch :  107  Loss :   0.00426451551997773  Accuracy :  0.9976635514018691 Time  1.05 s\n",
            "Epoch :  10  Batch :  108  Loss :   0.004225845305107349  Accuracy :  0.9976851851851852 Time  1.05 s\n",
            "Epoch :  10  Batch :  109  Loss :   0.004243953891299823  Accuracy :  0.9977064220183486 Time  1.05 s\n",
            "Epoch :  10  Batch :  110  Loss :   0.004211884402916026  Accuracy :  0.9977272727272727 Time  1.04 s\n",
            "Epoch :  10  Batch :  111  Loss :   0.004190444224002784  Accuracy :  0.9977477477477478 Time  1.05 s\n",
            "Epoch :  10  Batch :  112  Loss :   0.004154592215529972  Accuracy :  0.9977678571428571 Time  1.04 s\n",
            "Epoch :  10  Batch :  113  Loss :   0.004119728664768164  Accuracy :  0.9977876106194691 Time  1.04 s\n",
            "Epoch :  10  Batch :  114  Loss :   0.004085309346440849  Accuracy :  0.9978070175438597 Time  1.06 s\n",
            "Epoch :  10  Batch :  115  Loss :   0.004054331908992026  Accuracy :  0.9978260869565218 Time  1.06 s\n",
            "Epoch :  10  Batch :  116  Loss :   0.004021843356330747  Accuracy :  0.9978448275862069 Time  1.06 s\n",
            "Epoch :  10  Batch :  117  Loss :   0.003990907472462302  Accuracy :  0.9978632478632479 Time  1.04 s\n",
            "Epoch :  10  Batch :  118  Loss :   0.003959692660381256  Accuracy :  0.9978813559322034 Time  1.04 s\n",
            "Epoch :  10  Batch :  119  Loss :   0.0039280627430695455  Accuracy :  0.9978991596638656 Time  1.05 s\n",
            "Epoch :  10  Batch :  120  Loss :   0.0038985160914914254  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  10  Batch :  121  Loss :   0.003878655288118796  Accuracy :  0.9979338842975206 Time  1.04 s\n",
            "Epoch :  10  Batch :  122  Loss :   0.0038641551803519805  Accuracy :  0.9979508196721312 Time  1.04 s\n",
            "Epoch :  10  Batch :  123  Loss :   0.0038823864290351614  Accuracy :  0.9979674796747967 Time  1.05 s\n",
            "Epoch :  10  Batch :  124  Loss :   0.0038518160627356865  Accuracy :  0.9979838709677419 Time  1.05 s\n",
            "Epoch :  10  Batch :  125  Loss :   0.004087752883206122  Accuracy :  0.998 Time  1.06 s\n",
            "Epoch :  10  Batch :  126  Loss :   0.00405720850771075  Accuracy :  0.998015873015873 Time  1.06 s\n",
            "Epoch :  10  Batch :  127  Loss :   0.004027033819497094  Accuracy :  0.9980314960629921 Time  1.07 s\n",
            "Epoch :  10  Batch :  128  Loss :   0.0040126809079765735  Accuracy :  0.998046875 Time  1.05 s\n",
            "Epoch :  10  Batch :  129  Loss :   0.004120649615471439  Accuracy :  0.998062015503876 Time  1.04 s\n",
            "Epoch :  10  Batch :  130  Loss :   0.004240133159216314  Accuracy :  0.9980769230769231 Time  1.05 s\n",
            "Epoch :  10  Batch :  131  Loss :   0.004695648744143368  Accuracy :  0.9977099236641221 Time  1.05 s\n",
            "Epoch :  10  Batch :  132  Loss :   0.004671803764557064  Accuracy :  0.9977272727272727 Time  1.05 s\n",
            "Epoch :  10  Batch :  133  Loss :   0.004676325916189217  Accuracy :  0.9977443609022556 Time  1.04 s\n",
            "Epoch :  10  Batch :  134  Loss :   0.00464358128928284  Accuracy :  0.9977611940298508 Time  1.05 s\n",
            "Epoch :  10  Batch :  135  Loss :   0.004632500936255652  Accuracy :  0.9977777777777778 Time  1.05 s\n",
            "Epoch :  10  Batch :  136  Loss :   0.0045991067223868035  Accuracy :  0.9977941176470588 Time  1.06 s\n",
            "Epoch :  10  Batch :  137  Loss :   0.0045794143603734815  Accuracy :  0.9978102189781022 Time  1.05 s\n",
            "Epoch :  10  Batch :  138  Loss :   0.004549198789681069  Accuracy :  0.9978260869565218 Time  1.06 s\n",
            "Epoch :  10  Batch :  139  Loss :   0.004517865686486023  Accuracy :  0.9978417266187051 Time  1.05 s\n",
            "Epoch :  10  Batch :  140  Loss :   0.004505855534547923  Accuracy :  0.9978571428571429 Time  1.04 s\n",
            "Epoch :  10  Batch :  141  Loss :   0.004724831258577166  Accuracy :  0.9975177304964539 Time  1.04 s\n",
            "Epoch :  10  Batch :  142  Loss :   0.004694422824350497  Accuracy :  0.9975352112676056 Time  1.04 s\n",
            "Epoch :  10  Batch :  143  Loss :   0.004701313571798879  Accuracy :  0.9975524475524475 Time  1.05 s\n",
            "Epoch :  10  Batch :  144  Loss :   0.004669706371866293  Accuracy :  0.9975694444444444 Time  1.05 s\n",
            "Epoch :  10  Batch :  145  Loss :   0.004638733099072224  Accuracy :  0.9975862068965518 Time  1.05 s\n",
            "Epoch :  10  Batch :  146  Loss :   0.0047898584959748655  Accuracy :  0.9976027397260274 Time  1.05 s\n",
            "Epoch :  10  Batch :  147  Loss :   0.0047717607813450936  Accuracy :  0.9976190476190476 Time  1.06 s\n",
            "Epoch :  10  Batch :  148  Loss :   0.004740112361006951  Accuracy :  0.9976351351351351 Time  1.07 s\n",
            "Epoch :  10  Batch :  149  Loss :   0.004710371112019283  Accuracy :  0.9976510067114094 Time  1.07 s\n",
            "Epoch :  10  Batch :  150  Loss :   0.004680794638261432  Accuracy :  0.9976666666666667 Time  1.04 s\n",
            "Epoch :  10  Batch :  151  Loss :   0.0046663586636107035  Accuracy :  0.997682119205298 Time  1.05 s\n",
            "Epoch :  10  Batch :  152  Loss :   0.004635831583527761  Accuracy :  0.9976973684210526 Time  1.05 s\n",
            "Epoch :  10  Batch :  153  Loss :   0.004605971997912122  Accuracy :  0.9977124183006536 Time  1.05 s\n",
            "Epoch :  10  Batch :  154  Loss :   0.004586960665879555  Accuracy :  0.9977272727272727 Time  1.04 s\n",
            "Epoch :  10  Batch :  155  Loss :   0.004559155328282823  Accuracy :  0.997741935483871 Time  1.05 s\n",
            "Epoch :  10  Batch :  156  Loss :   0.004566424693457987  Accuracy :  0.9977564102564103 Time  1.04 s\n",
            "Epoch :  10  Batch :  157  Loss :   0.004542517197003423  Accuracy :  0.9977707006369426 Time  1.05 s\n",
            "Epoch :  10  Batch :  158  Loss :   0.004515476249103307  Accuracy :  0.9977848101265823 Time  1.06 s\n",
            "Epoch :  10  Batch :  159  Loss :   0.004499742485797538  Accuracy :  0.9977987421383647 Time  1.06 s\n",
            "Epoch :  10  Batch :  160  Loss :   0.004472970025676659  Accuracy :  0.9978125 Time  1.06 s\n",
            "Epoch :  10  Batch :  161  Loss :   0.004677887468296718  Accuracy :  0.9978260869565218 Time  1.05 s\n",
            "Epoch :  10  Batch :  162  Loss :   0.00465186792677775  Accuracy :  0.9978395061728395 Time  1.04 s\n",
            "Epoch :  10  Batch :  163  Loss :   0.0046239393273166  Accuracy :  0.9978527607361963 Time  1.04 s\n",
            "Epoch :  10  Batch :  164  Loss :   0.004597054429678992  Accuracy :  0.9978658536585366 Time  1.04 s\n",
            "Epoch :  10  Batch :  165  Loss :   0.00457122154266196  Accuracy :  0.9978787878787879 Time  1.05 s\n",
            "Epoch :  10  Batch :  166  Loss :   0.004553675052927455  Accuracy :  0.9978915662650603 Time  1.05 s\n",
            "Epoch :  10  Batch :  167  Loss :   0.004585419811067362  Accuracy :  0.9979041916167665 Time  1.04 s\n",
            "Epoch :  10  Batch :  168  Loss :   0.004711811964604013  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  10  Batch :  169  Loss :   0.00470292289251308  Accuracy :  0.9979289940828402 Time  1.06 s\n",
            "Epoch :  10  Batch :  170  Loss :   0.0046775622921011424  Accuracy :  0.9979411764705882 Time  1.07 s\n",
            "Epoch :  10  Batch :  171  Loss :   0.004651777765497311  Accuracy :  0.997953216374269 Time  1.08 s\n",
            "Epoch :  10  Batch :  172  Loss :   0.00462565250735108  Accuracy :  0.9979651162790698 Time  1.05 s\n",
            "Epoch :  10  Batch :  173  Loss :   0.004668963520698999  Accuracy :  0.9979768786127168 Time  1.05 s\n",
            "Epoch :  10  Batch :  174  Loss :   0.00464463085796294  Accuracy :  0.9979885057471264 Time  1.04 s\n",
            "Epoch :  10  Batch :  175  Loss :   0.004619438479276141  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  10  Batch :  176  Loss :   0.004624818087925384  Accuracy :  0.9980113636363637 Time  1.05 s\n",
            "Epoch :  10  Batch :  177  Loss :   0.004600281979597475  Accuracy :  0.9980225988700565 Time  1.04 s\n",
            "Epoch :  10  Batch :  178  Loss :   0.004582325048769041  Accuracy :  0.9980337078651685 Time  1.05 s\n",
            "Epoch :  10  Batch :  179  Loss :   0.004556899040540864  Accuracy :  0.9980446927374301 Time  1.05 s\n",
            "Epoch :  10  Batch :  180  Loss :   0.004538826434489036  Accuracy :  0.9980555555555556 Time  1.06 s\n",
            "Epoch :  10  Batch :  181  Loss :   0.004515507771188647  Accuracy :  0.9980662983425415 Time  1.06 s\n",
            "Epoch :  10  Batch :  182  Loss :   0.004491795794300353  Accuracy :  0.9980769230769231 Time  1.07 s\n",
            "Epoch :  10  Batch :  183  Loss :   0.004470822318903803  Accuracy :  0.9980874316939891 Time  1.05 s\n",
            "Epoch :  10  Batch :  184  Loss :   0.004467063800071312  Accuracy :  0.9980978260869565 Time  1.04 s\n",
            "Epoch :  10  Batch :  185  Loss :   0.004449841130553978  Accuracy :  0.9981081081081081 Time  1.05 s\n",
            "Epoch :  10  Batch :  186  Loss :   0.004465019147251188  Accuracy :  0.9981182795698925 Time  1.05 s\n",
            "Epoch :  10  Batch :  187  Loss :   0.004444192799514335  Accuracy :  0.9981283422459893 Time  1.04 s\n",
            "Epoch :  10  Batch :  188  Loss :   0.00444782485757311  Accuracy :  0.9981382978723404 Time  1.04 s\n",
            "Epoch :  10  Batch :  189  Loss :   0.004475507856518056  Accuracy :  0.9981481481481481 Time  1.04 s\n",
            "Epoch :  10  Batch :  190  Loss :   0.0044532944469162766  Accuracy :  0.9981578947368421 Time  1.04 s\n",
            "Epoch :  10  Batch :  191  Loss :   0.004430447272322944  Accuracy :  0.9981675392670157 Time  1.06 s\n",
            "Epoch :  10  Batch :  192  Loss :   0.004463001679425815  Accuracy :  0.9981770833333333 Time  1.06 s\n",
            "Epoch :  10  Batch :  193  Loss :   0.004453425612909101  Accuracy :  0.9981865284974093 Time  1.06 s\n",
            "Epoch :  10  Batch :  194  Loss :   0.00443103615110623  Accuracy :  0.9981958762886598 Time  1.05 s\n",
            "Epoch :  10  Batch :  195  Loss :   0.004411595714303145  Accuracy :  0.9982051282051282 Time  1.05 s\n",
            "Epoch :  10  Batch :  196  Loss :   0.004389701950124302  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  10  Batch :  197  Loss :   0.0043736199347642404  Accuracy :  0.9982233502538072 Time  1.05 s\n",
            "Epoch :  10  Batch :  198  Loss :   0.004352093936268839  Accuracy :  0.9982323232323232 Time  1.05 s\n",
            "Epoch :  10  Batch :  199  Loss :   0.00433498896970592  Accuracy :  0.9982412060301508 Time  1.05 s\n",
            "Epoch :  10  Batch :  200  Loss :   0.0043850686014957315  Accuracy :  0.99825 Time  1.04 s\n",
            "Epoch :  10  Batch :  201  Loss :   0.0043636681115802375  Accuracy :  0.9982587064676617 Time  1.04 s\n",
            "Epoch :  10  Batch :  202  Loss :   0.004394861193840469  Accuracy :  0.9982673267326733 Time  1.06 s\n",
            "Epoch :  10  Batch :  203  Loss :   0.004373729028096027  Accuracy :  0.9982758620689656 Time  1.06 s\n",
            "Epoch :  10  Batch :  204  Loss :   0.004664053867510993  Accuracy :  0.9980392156862745 Time  1.06 s\n",
            "Epoch :  10  Batch :  205  Loss :   0.0046415828553840474  Accuracy :  0.9980487804878049 Time  1.04 s\n",
            "Epoch :  10  Batch :  206  Loss :   0.004623135022219447  Accuracy :  0.9980582524271845 Time  1.04 s\n",
            "Epoch :  10  Batch :  207  Loss :   0.004601297095437097  Accuracy :  0.9980676328502416 Time  1.04 s\n",
            "Epoch :  10  Batch :  208  Loss :   0.004590404266519517  Accuracy :  0.9980769230769231 Time  1.04 s\n",
            "Epoch :  10  Batch :  209  Loss :   0.004568732036056936  Accuracy :  0.9980861244019139 Time  1.04 s\n",
            "Epoch :  10  Batch :  210  Loss :   0.004550797289560987  Accuracy :  0.9980952380952381 Time  1.05 s\n",
            "Epoch :  10  Batch :  211  Loss :   0.004621893336515188  Accuracy :  0.9981042654028436 Time  1.05 s\n",
            "Epoch :  10  Batch :  212  Loss :   0.004600586008838842  Accuracy :  0.9981132075471698 Time  1.05 s\n",
            "Epoch :  10  Batch :  213  Loss :   0.004579466197266273  Accuracy :  0.9981220657276996 Time  1.06 s\n",
            "Epoch :  10  Batch :  214  Loss :   0.004562013854623405  Accuracy :  0.9981308411214953 Time  1.06 s\n",
            "Epoch :  10  Batch :  215  Loss :   0.004541394252822122  Accuracy :  0.998139534883721 Time  1.07 s\n",
            "Epoch :  10  Batch :  216  Loss :   0.0045217635887032275  Accuracy :  0.9981481481481481 Time  1.05 s\n",
            "Epoch :  10  Batch :  217  Loss :   0.0045014818214302  Accuracy :  0.9981566820276497 Time  1.04 s\n",
            "Epoch :  10  Batch :  218  Loss :   0.004482072845014979  Accuracy :  0.998165137614679 Time  1.05 s\n",
            "Epoch :  10  Batch :  219  Loss :   0.004464729390531426  Accuracy :  0.9981735159817352 Time  1.04 s\n",
            "Epoch :  10  Batch :  220  Loss :   0.004444689529803079  Accuracy :  0.9981818181818182 Time  1.06 s\n",
            "Epoch :  10  Batch :  221  Loss :   0.004424860749641413  Accuracy :  0.9981900452488688 Time  1.05 s\n",
            "Epoch :  10  Batch :  222  Loss :   0.0044098728609111145  Accuracy :  0.9981981981981982 Time  1.04 s\n",
            "Epoch :  10  Batch :  223  Loss :   0.00439075633814371  Accuracy :  0.9982062780269059 Time  1.05 s\n",
            "Epoch :  10  Batch :  224  Loss :   0.004371933982400021  Accuracy :  0.9982142857142857 Time  1.06 s\n",
            "Epoch :  10  Batch :  225  Loss :   0.0043638806503764095  Accuracy :  0.9982222222222222 Time  1.06 s\n",
            "Epoch :  10  Batch :  226  Loss :   0.004344969082093026  Accuracy :  0.9982300884955753 Time  1.06 s\n",
            "Epoch :  10  Batch :  227  Loss :   0.004325905628476727  Accuracy :  0.9982378854625551 Time  1.05 s\n",
            "Epoch :  10  Batch :  228  Loss :   0.004315023484467243  Accuracy :  0.9982456140350877 Time  1.05 s\n",
            "Epoch :  10  Batch :  229  Loss :   0.004297131589892438  Accuracy :  0.9982532751091703 Time  1.05 s\n",
            "Epoch :  10  Batch :  230  Loss :   0.004283588688122108  Accuracy :  0.9982608695652174 Time  1.05 s\n",
            "Epoch :  10  Batch :  231  Loss :   0.004265959978983556  Accuracy :  0.9982683982683983 Time  1.05 s\n",
            "Epoch :  10  Batch :  232  Loss :   0.004249560607949489  Accuracy :  0.9982758620689656 Time  1.05 s\n",
            "Epoch :  10  Batch :  233  Loss :   0.004232176572415186  Accuracy :  0.9982832618025751 Time  1.05 s\n",
            "Epoch :  10  Batch :  234  Loss :   0.004222970801026513  Accuracy :  0.9982905982905983 Time  1.04 s\n",
            "Epoch :  10  Batch :  235  Loss :   0.00420929262590436  Accuracy :  0.9982978723404256 Time  1.05 s\n",
            "Epoch :  10  Batch :  236  Loss :   0.004206438739641009  Accuracy :  0.9983050847457627 Time  1.06 s\n",
            "Epoch :  10  Batch :  237  Loss :   0.004189122229937821  Accuracy :  0.9983122362869198 Time  1.06 s\n",
            "Epoch :  10  Batch :  238  Loss :   0.004176198495510367  Accuracy :  0.9983193277310924 Time  1.05 s\n",
            "Epoch :  10  Batch :  239  Loss :   0.004160783174153024  Accuracy :  0.998326359832636 Time  1.04 s\n",
            "Epoch :  10  Batch :  240  Loss :   0.004143801944186028  Accuracy :  0.9983333333333333 Time  1.04 s\n",
            "Epoch :  10  Batch :  241  Loss :   0.004126911660910176  Accuracy :  0.9983402489626556 Time  1.04 s\n",
            "Epoch :  10  Batch :  242  Loss :   0.004109988939377247  Accuracy :  0.9983471074380166 Time  1.05 s\n",
            "Epoch :  10  Batch :  243  Loss :   0.0040933697063535975  Accuracy :  0.9983539094650206 Time  1.05 s\n",
            "Epoch :  10  Batch :  244  Loss :   0.004079314335613759  Accuracy :  0.9983606557377049 Time  1.05 s\n",
            "Epoch :  10  Batch :  245  Loss :   0.004063264188539042  Accuracy :  0.9983673469387755 Time  1.05 s\n",
            "Epoch :  10  Batch :  246  Loss :   0.004064773187786979  Accuracy :  0.9983739837398374 Time  1.06 s\n",
            "Epoch :  10  Batch :  247  Loss :   0.004048754596834232  Accuracy :  0.9983805668016195 Time  1.06 s\n",
            "Epoch :  10  Batch :  248  Loss :   0.004032539353961658  Accuracy :  0.9983870967741936 Time  1.06 s\n",
            "Epoch :  10  Batch :  249  Loss :   0.0040165765954390305  Accuracy :  0.9983935742971888 Time  1.06 s\n",
            "Epoch :  10  Batch :  250  Loss :   0.004006704114101012  Accuracy :  0.9984 Time  1.04 s\n",
            "Epoch :  10  Batch :  251  Loss :   0.004208866766270967  Accuracy :  0.998207171314741 Time  1.05 s\n",
            "Epoch :  10  Batch :  252  Loss :   0.0041922557818560606  Accuracy :  0.9982142857142857 Time  1.05 s\n",
            "Epoch :  10  Batch :  253  Loss :   0.004183401997708908  Accuracy :  0.9982213438735178 Time  1.05 s\n",
            "Epoch :  10  Batch :  254  Loss :   0.004167050968113815  Accuracy :  0.9982283464566929 Time  1.04 s\n",
            "Epoch :  10  Batch :  255  Loss :   0.0041592640744912545  Accuracy :  0.9982352941176471 Time  1.04 s\n",
            "Epoch :  10  Batch :  256  Loss :   0.004143339972614513  Accuracy :  0.9982421875 Time  1.04 s\n",
            "Epoch :  10  Batch :  257  Loss :   0.004128827726198243  Accuracy :  0.9982490272373541 Time  1.05 s\n",
            "Epoch :  10  Batch :  258  Loss :   0.004113343043870774  Accuracy :  0.9982558139534884 Time  1.06 s\n",
            "Epoch :  10  Batch :  259  Loss :   0.004099212235245902  Accuracy :  0.9982625482625482 Time  1.07 s\n",
            "Epoch :  10  Batch :  260  Loss :   0.0040859119293600085  Accuracy :  0.9982692307692308 Time  1.05 s\n",
            "Epoch :  10  Batch :  261  Loss :   0.004070602319056349  Accuracy :  0.9982758620689656 Time  1.04 s\n",
            "Epoch :  10  Batch :  262  Loss :   0.0040555982202598526  Accuracy :  0.9982824427480916 Time  1.04 s\n",
            "Epoch :  10  Batch :  263  Loss :   0.004044841841820888  Accuracy :  0.9982889733840304 Time  1.04 s\n",
            "Epoch :  10  Batch :  264  Loss :   0.004030861299890437  Accuracy :  0.9982954545454545 Time  1.05 s\n",
            "Epoch :  10  Batch :  265  Loss :   0.0040224674485902796  Accuracy :  0.9983018867924528 Time  1.05 s\n",
            "Epoch :  10  Batch :  266  Loss :   0.004008172206374724  Accuracy :  0.9983082706766917 Time  1.04 s\n",
            "Epoch :  10  Batch :  267  Loss :   0.003993877699228714  Accuracy :  0.998314606741573 Time  1.05 s\n",
            "Epoch :  10  Batch :  268  Loss :   0.003981300742479881  Accuracy :  0.998320895522388 Time  1.06 s\n",
            "Epoch :  10  Batch :  269  Loss :   0.003967718042269458  Accuracy :  0.9983271375464684 Time  1.06 s\n",
            "Epoch :  10  Batch :  270  Loss :   0.00395659835040634  Accuracy :  0.9983333333333333 Time  1.07 s\n",
            "Epoch :  10  Batch :  271  Loss :   0.003945973460755622  Accuracy :  0.9983394833948339 Time  1.06 s\n",
            "Epoch :  10  Batch :  272  Loss :   0.003932917368381502  Accuracy :  0.9983455882352941 Time  1.06 s\n",
            "Epoch :  10  Batch :  273  Loss :   0.0039190279907342675  Accuracy :  0.9983516483516484 Time  1.05 s\n",
            "Epoch :  10  Batch :  274  Loss :   0.003904901205150565  Accuracy :  0.9983576642335766 Time  1.05 s\n",
            "Epoch :  10  Batch :  275  Loss :   0.0038986080638080075  Accuracy :  0.9983636363636363 Time  1.04 s\n",
            "Epoch :  10  Batch :  276  Loss :   0.003894045376053301  Accuracy :  0.9983695652173913 Time  1.05 s\n",
            "Epoch :  10  Batch :  277  Loss :   0.0038810002167374213  Accuracy :  0.9983754512635379 Time  1.05 s\n",
            "Epoch :  10  Batch :  278  Loss :   0.0038681349791358176  Accuracy :  0.9983812949640288 Time  1.04 s\n",
            "Epoch :  10  Batch :  279  Loss :   0.00385469995997867  Accuracy :  0.9983870967741936 Time  1.05 s\n",
            "Epoch :  10  Batch :  280  Loss :   0.003885580505889915  Accuracy :  0.9983928571428572 Time  1.06 s\n",
            "Epoch :  10  Batch :  281  Loss :   0.003872609643139868  Accuracy :  0.9983985765124556 Time  1.06 s\n",
            "Epoch :  10  Batch :  282  Loss :   0.003859026195731795  Accuracy :  0.9984042553191489 Time  1.06 s\n",
            "Epoch :  10  Batch :  283  Loss :   0.0038513687744750245  Accuracy :  0.9984098939929329 Time  1.05 s\n",
            "Epoch :  10  Batch :  284  Loss :   0.0038386615401129385  Accuracy :  0.9984154929577465 Time  1.05 s\n",
            "Epoch :  10  Batch :  285  Loss :   0.003825278919624939  Accuracy :  0.998421052631579 Time  1.05 s\n",
            "Epoch :  10  Batch :  286  Loss :   0.003812358724759064  Accuracy :  0.9984265734265734 Time  1.04 s\n",
            "Epoch :  10  Batch :  287  Loss :   0.0037994471928491625  Accuracy :  0.998432055749129 Time  1.04 s\n",
            "Epoch :  10  Batch :  288  Loss :   0.0037878053453255234  Accuracy :  0.9984375 Time  1.05 s\n",
            "Epoch :  10  Batch :  289  Loss :   0.003779377376263271  Accuracy :  0.9984429065743945 Time  1.05 s\n",
            "Epoch :  10  Batch :  290  Loss :   0.004080517372112181  Accuracy :  0.9982758620689656 Time  1.05 s\n",
            "Epoch :  10  Batch :  291  Loss :   0.0040749473544915305  Accuracy :  0.9982817869415808 Time  1.06 s\n",
            "Epoch :  10  Batch :  292  Loss :   0.004062957555411791  Accuracy :  0.9982876712328768 Time  1.06 s\n",
            "Epoch :  10  Batch :  293  Loss :   0.004059146307039052  Accuracy :  0.9982935153583617 Time  1.06 s\n",
            "Epoch :  10  Batch :  294  Loss :   0.0040469438195991116  Accuracy :  0.9982993197278912 Time  1.05 s\n",
            "Epoch :  10  Batch :  295  Loss :   0.004037556403470216  Accuracy :  0.9983050847457627 Time  1.04 s\n",
            "Epoch :  10  Batch :  296  Loss :   0.004024362546008863  Accuracy :  0.9983108108108109 Time  1.05 s\n",
            "Epoch :  10  Batch :  297  Loss :   0.004010876602239817  Accuracy :  0.9983164983164983 Time  1.05 s\n",
            "Epoch :  10  Batch :  298  Loss :   0.00399977989213431  Accuracy :  0.9983221476510067 Time  1.04 s\n",
            "Epoch :  10  Batch :  299  Loss :   0.003987924566072525  Accuracy :  0.9983277591973244 Time  1.04 s\n",
            "Epoch :  10  Batch :  300  Loss :   0.003975504623740562  Accuracy :  0.9983333333333333 Time  1.04 s\n",
            "Epoch :  10  Batch :  301  Loss :   0.0039627829198352294  Accuracy :  0.9983388704318937 Time  1.04 s\n",
            "Epoch :  10  Batch :  302  Loss :   0.003950311113834121  Accuracy :  0.9983443708609272 Time  1.06 s\n",
            "Epoch :  10  Batch :  303  Loss :   0.003938340202138933  Accuracy :  0.9983498349834984 Time  1.06 s\n",
            "Epoch :  10  Batch :  304  Loss :   0.003925707544998372  Accuracy :  0.9983552631578947 Time  1.06 s\n",
            "Epoch :  10  Batch :  305  Loss :   0.0039141897137179495  Accuracy :  0.9983606557377049 Time  1.05 s\n",
            "Epoch :  10  Batch :  306  Loss :   0.003904261862201155  Accuracy :  0.9983660130718954 Time  1.05 s\n",
            "Epoch :  10  Batch :  307  Loss :   0.003897117907417436  Accuracy :  0.99836867862969 Time  0.55 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 99 %\n",
            "[10 epoch] Accuracy of the network on the Training images: 99 %\n",
            "Epoch :  11  Batch :  1  Loss :   3.4365319152129814e-05  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  2  Loss :   0.0006345488272927469  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  3  Loss :   0.000606547355952595  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  4  Loss :   0.0005180776888664695  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  5  Loss :   0.0005034400695876684  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  6  Loss :   0.0004284552317888786  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  7  Loss :   0.0003964448800875938  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  8  Loss :   0.00037017935210315045  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  11  Batch :  9  Loss :   0.00036113370475100563  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  10  Loss :   0.0005890620188438334  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  11  Loss :   0.0005693253042409196  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  12  Loss :   0.0005492020900419448  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  13  Loss :   0.0005549598259117025  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  14  Loss :   0.000527882536906483  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  15  Loss :   0.0006263613739671807  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  16  Loss :   0.0006751272485416848  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  17  Loss :   0.0006713308085797026  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  18  Loss :   0.0014945974075494127  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  19  Loss :   0.001470649709900547  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  20  Loss :   0.0013987588015879737  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  21  Loss :   0.0013377253576653033  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  22  Loss :   0.001345292320117799  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  23  Loss :   0.001289472476460303  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  24  Loss :   0.0012382660788716748  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  25  Loss :   0.001195331031922251  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  26  Loss :   0.0011529869510783241  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  27  Loss :   0.001125615536929453  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  28  Loss :   0.001096846430300502  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  11  Batch :  29  Loss :   0.0010650304039731494  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  11  Batch :  30  Loss :   0.0010299195942025108  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  31  Loss :   0.0010177001127123085  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  32  Loss :   0.000988084054739602  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  33  Loss :   0.0009623919568735855  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  34  Loss :   0.0012019132631233814  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  35  Loss :   0.0011919853867116866  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  36  Loss :   0.001198653341033504  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  37  Loss :   0.0012083271276915675  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  38  Loss :   0.0012175160157695292  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  39  Loss :   0.0011947406373087105  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  40  Loss :   0.0011657255142608846  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  41  Loss :   0.0011646481361156742  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  42  Loss :   0.001145471413478628  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  43  Loss :   0.0011196302106919723  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  44  Loss :   0.0011451736735869583  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  45  Loss :   0.001134564564341013  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  46  Loss :   0.001467404484470614  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  47  Loss :   0.0014654284445025545  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  48  Loss :   0.0014855612023628357  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  49  Loss :   0.0014972342152425565  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  50  Loss :   0.001588064019833837  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  51  Loss :   0.0015604066333538618  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  52  Loss :   0.0015865184715704134  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  11  Batch :  53  Loss :   0.001568323007391745  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  54  Loss :   0.0015406689423663785  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  55  Loss :   0.0015133460569069774  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  56  Loss :   0.0015467645226538349  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  57  Loss :   0.0015221512297725177  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  58  Loss :   0.0015039095770777655  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  59  Loss :   0.0014795727396692926  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  60  Loss :   0.0014594559798600433  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  61  Loss :   0.0014575963359228078  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  62  Loss :   0.0014345672782603229  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  63  Loss :   0.001413349276137726  Accuracy :  1.0 Time  1.07 s\n",
            "Epoch :  11  Batch :  64  Loss :   0.0013941657119147521  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  65  Loss :   0.0013920679897077768  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  66  Loss :   0.0013725142325573402  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  67  Loss :   0.0013524715054360845  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  68  Loss :   0.0013333150794176832  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  69  Loss :   0.0013288509697014124  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  70  Loss :   0.001311775877664851  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  71  Loss :   0.0012964630337899533  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  72  Loss :   0.0012862027877342068  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  73  Loss :   0.0012688264741259508  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  74  Loss :   0.0012523849487798863  Accuracy :  1.0 Time  1.06 s\n",
            "Epoch :  11  Batch :  75  Loss :   0.001236385843573468  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  76  Loss :   0.001235142271322107  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  77  Loss :   0.0012214900483646582  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  78  Loss :   0.001207029627377592  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  79  Loss :   0.001197336818349177  Accuracy :  1.0 Time  1.04 s\n",
            "Epoch :  11  Batch :  80  Loss :   0.0011830582882907947  Accuracy :  1.0 Time  1.05 s\n",
            "Epoch :  11  Batch :  81  Loss :   0.004892080621557173  Accuracy :  0.9993827160493827 Time  1.05 s\n",
            "Epoch :  11  Batch :  82  Loss :   0.004871081346752872  Accuracy :  0.999390243902439 Time  1.04 s\n",
            "Epoch :  11  Batch :  83  Loss :   0.004814264144607226  Accuracy :  0.9993975903614458 Time  1.06 s\n",
            "Epoch :  11  Batch :  84  Loss :   0.004775126754588027  Accuracy :  0.9994047619047619 Time  1.07 s\n",
            "Epoch :  11  Batch :  85  Loss :   0.004746777974062698  Accuracy :  0.9994117647058823 Time  1.06 s\n",
            "Epoch :  11  Batch :  86  Loss :   0.0047462954943892875  Accuracy :  0.9994186046511628 Time  1.05 s\n",
            "Epoch :  11  Batch :  87  Loss :   0.004726856078775001  Accuracy :  0.9994252873563219 Time  1.05 s\n",
            "Epoch :  11  Batch :  88  Loss :   0.004692476330350556  Accuracy :  0.9994318181818181 Time  1.05 s\n",
            "Epoch :  11  Batch :  89  Loss :   0.005755747388780599  Accuracy :  0.998876404494382 Time  1.04 s\n",
            "Epoch :  11  Batch :  90  Loss :   0.0057814487387986445  Accuracy :  0.9988888888888889 Time  1.05 s\n",
            "Epoch :  11  Batch :  91  Loss :   0.00583018704629057  Accuracy :  0.9989010989010989 Time  1.05 s\n",
            "Epoch :  11  Batch :  92  Loss :   0.005803640605622802  Accuracy :  0.9989130434782608 Time  1.05 s\n",
            "Epoch :  11  Batch :  93  Loss :   0.005764933901592749  Accuracy :  0.9989247311827957 Time  1.05 s\n",
            "Epoch :  11  Batch :  94  Loss :   0.006274047582850316  Accuracy :  0.9984042553191489 Time  1.06 s\n",
            "Epoch :  11  Batch :  95  Loss :   0.006317641103487403  Accuracy :  0.998421052631579 Time  1.06 s\n",
            "Epoch :  11  Batch :  96  Loss :   0.006257195502494521  Accuracy :  0.9984375 Time  1.06 s\n",
            "Epoch :  11  Batch :  97  Loss :   0.006205500541825328  Accuracy :  0.9984536082474227 Time  1.05 s\n",
            "Epoch :  11  Batch :  98  Loss :   0.006153721999304921  Accuracy :  0.9984693877551021 Time  1.05 s\n",
            "Epoch :  11  Batch :  99  Loss :   0.006096571769954939  Accuracy :  0.9984848484848485 Time  1.04 s\n",
            "Epoch :  11  Batch :  100  Loss :   0.0060473139679470475  Accuracy :  0.9985 Time  1.05 s\n",
            "Epoch :  11  Batch :  101  Loss :   0.006016296689798908  Accuracy :  0.9985148514851485 Time  1.05 s\n",
            "Epoch :  11  Batch :  102  Loss :   0.0059647338715894515  Accuracy :  0.9985294117647059 Time  1.05 s\n",
            "Epoch :  11  Batch :  103  Loss :   0.006113112489551939  Accuracy :  0.9985436893203884 Time  1.05 s\n",
            "Epoch :  11  Batch :  104  Loss :   0.006060335990775037  Accuracy :  0.9985576923076923 Time  1.04 s\n",
            "Epoch :  11  Batch :  105  Loss :   0.006243310469264023  Accuracy :  0.9985714285714286 Time  1.06 s\n",
            "Epoch :  11  Batch :  106  Loss :   0.006209321991856035  Accuracy :  0.9985849056603774 Time  1.07 s\n",
            "Epoch :  11  Batch :  107  Loss :   0.007343966796419001  Accuracy :  0.9981308411214953 Time  1.07 s\n",
            "Epoch :  11  Batch :  108  Loss :   0.007278937753662157  Accuracy :  0.9981481481481481 Time  1.06 s\n",
            "Epoch :  11  Batch :  109  Loss :   0.007344665332562026  Accuracy :  0.998165137614679 Time  1.05 s\n",
            "Epoch :  11  Batch :  110  Loss :   0.007351691464158317  Accuracy :  0.9981818181818182 Time  1.04 s\n",
            "Epoch :  11  Batch :  111  Loss :   0.00728612999818824  Accuracy :  0.9981981981981982 Time  1.04 s\n",
            "Epoch :  11  Batch :  112  Loss :   0.007248832911313586  Accuracy :  0.9982142857142857 Time  1.04 s\n",
            "Epoch :  11  Batch :  113  Loss :   0.007353539043487643  Accuracy :  0.9982300884955753 Time  1.05 s\n",
            "Epoch :  11  Batch :  114  Loss :   0.007290526254867776  Accuracy :  0.9982456140350877 Time  1.05 s\n",
            "Epoch :  11  Batch :  115  Loss :   0.007233771983903906  Accuracy :  0.9982608695652174 Time  1.04 s\n",
            "Epoch :  11  Batch :  116  Loss :   0.007728569701776936  Accuracy :  0.9978448275862069 Time  1.06 s\n",
            "Epoch :  11  Batch :  117  Loss :   0.0077172130943750725  Accuracy :  0.9978632478632479 Time  1.06 s\n",
            "Epoch :  11  Batch :  118  Loss :   0.0076545428654815435  Accuracy :  0.9978813559322034 Time  1.07 s\n",
            "Epoch :  11  Batch :  119  Loss :   0.00769084578173113  Accuracy :  0.9978991596638656 Time  1.05 s\n",
            "Epoch :  11  Batch :  120  Loss :   0.0077797844893893854  Accuracy :  0.9979166666666667 Time  1.05 s\n",
            "Epoch :  11  Batch :  121  Loss :   0.007716815581892856  Accuracy :  0.9979338842975206 Time  1.05 s\n",
            "Epoch :  11  Batch :  122  Loss :   0.007897287776992673  Accuracy :  0.9979508196721312 Time  1.05 s\n",
            "Epoch :  11  Batch :  123  Loss :   0.007833629225521091  Accuracy :  0.9979674796747967 Time  1.05 s\n",
            "Epoch :  11  Batch :  124  Loss :   0.007779352876542039  Accuracy :  0.9979838709677419 Time  1.04 s\n",
            "Epoch :  11  Batch :  125  Loss :   0.00793479061016842  Accuracy :  0.998 Time  1.05 s\n",
            "Epoch :  11  Batch :  126  Loss :   0.007873352657281344  Accuracy :  0.998015873015873 Time  1.04 s\n",
            "Epoch :  11  Batch :  127  Loss :   0.008040769740281268  Accuracy :  0.9980314960629921 Time  1.06 s\n",
            "Epoch :  11  Batch :  128  Loss :   0.008548782697808122  Accuracy :  0.99765625 Time  1.06 s\n",
            "Epoch :  11  Batch :  129  Loss :   0.008493058074441074  Accuracy :  0.9976744186046511 Time  1.06 s\n",
            "Epoch :  11  Batch :  130  Loss :   0.008431057451140917  Accuracy :  0.9976923076923077 Time  1.05 s\n",
            "Epoch :  11  Batch :  131  Loss :   0.009166399647992112  Accuracy :  0.9973282442748092 Time  1.04 s\n",
            "Epoch :  11  Batch :  132  Loss :   0.009097822967753404  Accuracy :  0.9973484848484848 Time  1.05 s\n",
            "Epoch :  11  Batch :  133  Loss :   0.009031329313286807  Accuracy :  0.9973684210526316 Time  1.05 s\n",
            "Epoch :  11  Batch :  134  Loss :   0.0089734496324914  Accuracy :  0.9973880597014926 Time  1.04 s\n",
            "Epoch :  11  Batch :  135  Loss :   0.008980178588650165  Accuracy :  0.9974074074074074 Time  1.04 s\n",
            "Epoch :  11  Batch :  136  Loss :   0.00891449174731834  Accuracy :  0.9974264705882353 Time  1.04 s\n",
            "Epoch :  11  Batch :  137  Loss :   0.00899147188954845  Accuracy :  0.9974452554744525 Time  1.04 s\n",
            "Epoch :  11  Batch :  138  Loss :   0.008926772925755411  Accuracy :  0.9974637681159421 Time  1.06 s\n",
            "Epoch :  11  Batch :  139  Loss :   0.008873344472638616  Accuracy :  0.9974820143884892 Time  1.06 s\n",
            "Epoch :  11  Batch :  140  Loss :   0.008814072355231213  Accuracy :  0.9975 Time  1.06 s\n",
            "Epoch :  11  Batch :  141  Loss :   0.0088901864817878  Accuracy :  0.9975177304964539 Time  1.05 s\n",
            "Epoch :  11  Batch :  142  Loss :   0.008850724419030292  Accuracy :  0.9975352112676056 Time  1.04 s\n",
            "Epoch :  11  Batch :  143  Loss :   0.00879039387243713  Accuracy :  0.9975524475524475 Time  1.04 s\n",
            "Epoch :  11  Batch :  144  Loss :   0.00873096221504789  Accuracy :  0.9975694444444444 Time  1.04 s\n",
            "Epoch :  11  Batch :  145  Loss :   0.008671006555088187  Accuracy :  0.9975862068965518 Time  1.05 s\n",
            "Epoch :  11  Batch :  146  Loss :   0.008625492818351033  Accuracy :  0.9976027397260274 Time  1.04 s\n",
            "Epoch :  11  Batch :  147  Loss :   0.008577630214840239  Accuracy :  0.9976190476190476 Time  1.05 s\n",
            "Epoch :  11  Batch :  148  Loss :   0.008524581163826312  Accuracy :  0.9976351351351351 Time  1.06 s\n",
            "Epoch :  11  Batch :  149  Loss :   0.008467952410915162  Accuracy :  0.9976510067114094 Time  1.06 s\n",
            "Epoch :  11  Batch :  150  Loss :   0.008411765319015102  Accuracy :  0.9976666666666667 Time  1.07 s\n",
            "Epoch :  11  Batch :  151  Loss :   0.008356302776525465  Accuracy :  0.997682119205298 Time  1.06 s\n",
            "Epoch :  11  Batch :  152  Loss :   0.008407558654922516  Accuracy :  0.9976973684210526 Time  1.05 s\n",
            "Epoch :  11  Batch :  153  Loss :   0.011459051196527749  Accuracy :  0.9973856209150327 Time  1.05 s\n",
            "Epoch :  11  Batch :  154  Loss :   0.014928261371834822  Accuracy :  0.9967532467532467 Time  1.05 s\n",
            "Epoch :  11  Batch :  155  Loss :   0.014834345196123326  Accuracy :  0.9967741935483871 Time  1.05 s\n",
            "Epoch :  11  Batch :  156  Loss :   0.014739489409579712  Accuracy :  0.9967948717948718 Time  1.04 s\n",
            "Epoch :  11  Batch :  157  Loss :   0.01477712675158768  Accuracy :  0.9968152866242038 Time  1.05 s\n",
            "Epoch :  11  Batch :  158  Loss :   0.014688964307499445  Accuracy :  0.9968354430379747 Time  1.05 s\n",
            "Epoch :  11  Batch :  159  Loss :   0.014673709774825917  Accuracy :  0.9968553459119497 Time  1.05 s\n",
            "Epoch :  11  Batch :  160  Loss :   0.014628420914851859  Accuracy :  0.996875 Time  1.06 s\n",
            "Epoch :  11  Batch :  161  Loss :   0.014545213378228  Accuracy :  0.9968944099378882 Time  1.06 s\n",
            "Epoch :  11  Batch :  162  Loss :   0.014473391769283975  Accuracy :  0.9969135802469136 Time  1.06 s\n",
            "Epoch :  11  Batch :  163  Loss :   0.014646752643795726  Accuracy :  0.9969325153374233 Time  1.07 s\n",
            "Epoch :  11  Batch :  164  Loss :   0.014687470322874882  Accuracy :  0.9969512195121951 Time  1.05 s\n",
            "Epoch :  11  Batch :  165  Loss :   0.014749029372488263  Accuracy :  0.996969696969697 Time  1.06 s\n",
            "Epoch :  11  Batch :  166  Loss :   0.014800099883803297  Accuracy :  0.9969879518072289 Time  1.05 s\n",
            "Epoch :  11  Batch :  167  Loss :   0.015287140019630854  Accuracy :  0.9967065868263473 Time  1.05 s\n",
            "Epoch :  11  Batch :  168  Loss :   0.015219155932435396  Accuracy :  0.9967261904761905 Time  1.06 s\n",
            "Epoch :  11  Batch :  169  Loss :   0.01512998489839802  Accuracy :  0.9967455621301775 Time  1.06 s\n",
            "Epoch :  11  Batch :  170  Loss :   0.015113392052299947  Accuracy :  0.9967647058823529 Time  1.06 s\n",
            "Epoch :  11  Batch :  171  Loss :   0.015033318531616805  Accuracy :  0.9967836257309941 Time  1.07 s\n",
            "Epoch :  11  Batch :  172  Loss :   0.016129758973626618  Accuracy :  0.9965116279069768 Time  1.07 s\n",
            "Epoch :  11  Batch :  173  Loss :   0.016255606919870623  Accuracy :  0.9965317919075144 Time  1.08 s\n",
            "Epoch :  11  Batch :  174  Loss :   0.01790683548049584  Accuracy :  0.9959770114942529 Time  1.04 s\n",
            "Epoch :  11  Batch :  175  Loss :   0.017812803493896873  Accuracy :  0.996 Time  1.05 s\n",
            "Epoch :  11  Batch :  176  Loss :   0.01771370457901805  Accuracy :  0.9960227272727272 Time  1.05 s\n",
            "Epoch :  11  Batch :  177  Loss :   0.017628259092585506  Accuracy :  0.996045197740113 Time  1.04 s\n",
            "Epoch :  11  Batch :  178  Loss :   0.01753521336534024  Accuracy :  0.9960674157303371 Time  1.05 s\n",
            "Epoch :  11  Batch :  179  Loss :   0.017438123449596405  Accuracy :  0.9960893854748604 Time  1.05 s\n",
            "Epoch :  11  Batch :  180  Loss :   0.017605064291420704  Accuracy :  0.9958333333333333 Time  1.04 s\n",
            "Epoch :  11  Batch :  181  Loss :   0.017516471134407748  Accuracy :  0.9958563535911602 Time  1.05 s\n",
            "Epoch :  11  Batch :  182  Loss :   0.017456219738985486  Accuracy :  0.9958791208791209 Time  1.06 s\n",
            "Epoch :  11  Batch :  183  Loss :   0.01896492737140975  Accuracy :  0.9953551912568306 Time  1.06 s\n",
            "Epoch :  11  Batch :  184  Loss :   0.018958916971299637  Accuracy :  0.9953804347826087 Time  1.06 s\n",
            "Epoch :  11  Batch :  185  Loss :   0.019014990493495722  Accuracy :  0.9954054054054055 Time  1.05 s\n",
            "Epoch :  11  Batch :  186  Loss :   0.01891987495606685  Accuracy :  0.9954301075268818 Time  1.04 s\n",
            "Epoch :  11  Batch :  187  Loss :   0.01882630079826948  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  11  Batch :  188  Loss :   0.01875101960899883  Accuracy :  0.9954787234042554 Time  1.05 s\n",
            "Epoch :  11  Batch :  189  Loss :   0.018734943000468275  Accuracy :  0.9955026455026456 Time  1.04 s\n",
            "Epoch :  11  Batch :  190  Loss :   0.018961948231859233  Accuracy :  0.9952631578947368 Time  1.05 s\n",
            "Epoch :  11  Batch :  191  Loss :   0.018865620715106345  Accuracy :  0.9952879581151832 Time  1.04 s\n",
            "Epoch :  11  Batch :  192  Loss :   0.01877973923560698  Accuracy :  0.9953125 Time  1.04 s\n",
            "Epoch :  11  Batch :  193  Loss :   0.018686037324384976  Accuracy :  0.9953367875647668 Time  1.06 s\n",
            "Epoch :  11  Batch :  194  Loss :   0.019613665967389136  Accuracy :  0.9951030927835052 Time  1.06 s\n",
            "Epoch :  11  Batch :  195  Loss :   0.01951492354764923  Accuracy :  0.9951282051282051 Time  1.06 s\n",
            "Epoch :  11  Batch :  196  Loss :   0.019452623814022817  Accuracy :  0.9951530612244898 Time  1.05 s\n",
            "Epoch :  11  Batch :  197  Loss :   0.019360779320274288  Accuracy :  0.9951776649746192 Time  1.04 s\n",
            "Epoch :  11  Batch :  198  Loss :   0.019276664904075667  Accuracy :  0.9952020202020202 Time  1.05 s\n",
            "Epoch :  11  Batch :  199  Loss :   0.019193823932657516  Accuracy :  0.9952261306532664 Time  1.05 s\n",
            "Epoch :  11  Batch :  200  Loss :   0.019102429368490447  Accuracy :  0.99525 Time  1.05 s\n",
            "Epoch :  11  Batch :  201  Loss :   0.02047143401434812  Accuracy :  0.9945273631840796 Time  1.04 s\n",
            "Epoch :  11  Batch :  202  Loss :   0.020511549900215913  Accuracy :  0.9945544554455445 Time  1.05 s\n",
            "Epoch :  11  Batch :  203  Loss :   0.02042663869319924  Accuracy :  0.9945812807881773 Time  1.05 s\n",
            "Epoch :  11  Batch :  204  Loss :   0.020342855851635356  Accuracy :  0.9946078431372549 Time  1.06 s\n",
            "Epoch :  11  Batch :  205  Loss :   0.02025182846145759  Accuracy :  0.9946341463414634 Time  1.06 s\n",
            "Epoch :  11  Batch :  206  Loss :   0.020162051586388223  Accuracy :  0.9946601941747573 Time  1.06 s\n",
            "Epoch :  11  Batch :  207  Loss :   0.020687335847772676  Accuracy :  0.9944444444444445 Time  1.05 s\n",
            "Epoch :  11  Batch :  208  Loss :   0.021381759933096697  Accuracy :  0.9942307692307693 Time  1.04 s\n",
            "Epoch :  11  Batch :  209  Loss :   0.02128239183278193  Accuracy :  0.9942583732057416 Time  1.04 s\n",
            "Epoch :  11  Batch :  210  Loss :   0.021191301310582708  Accuracy :  0.9942857142857143 Time  1.04 s\n",
            "Epoch :  11  Batch :  211  Loss :   0.021327398336282027  Accuracy :  0.9943127962085309 Time  1.04 s\n",
            "Epoch :  11  Batch :  212  Loss :   0.02126868308000495  Accuracy :  0.9943396226415094 Time  1.05 s\n",
            "Epoch :  11  Batch :  213  Loss :   0.02143810308023885  Accuracy :  0.994131455399061 Time  1.05 s\n",
            "Epoch :  11  Batch :  214  Loss :   0.021883140864935747  Accuracy :  0.9939252336448599 Time  1.04 s\n",
            "Epoch :  11  Batch :  215  Loss :   0.021799255296342423  Accuracy :  0.9939534883720931 Time  1.05 s\n",
            "Epoch :  11  Batch :  216  Loss :   0.02176127986319898  Accuracy :  0.9939814814814815 Time  1.06 s\n",
            "Epoch :  11  Batch :  217  Loss :   0.02166931427842839  Accuracy :  0.9940092165898617 Time  1.07 s\n",
            "Epoch :  11  Batch :  218  Loss :   0.021612581319655506  Accuracy :  0.9940366972477064 Time  1.05 s\n",
            "Epoch :  11  Batch :  219  Loss :   0.0215656507355366  Accuracy :  0.9940639269406393 Time  1.05 s\n",
            "Epoch :  11  Batch :  220  Loss :   0.021470240263959734  Accuracy :  0.9940909090909091 Time  1.06 s\n",
            "Epoch :  11  Batch :  221  Loss :   0.021451631507837632  Accuracy :  0.9941176470588236 Time  1.04 s\n",
            "Epoch :  11  Batch :  222  Loss :   0.021370238658717525  Accuracy :  0.9941441441441441 Time  1.05 s\n",
            "Epoch :  11  Batch :  223  Loss :   0.02129243557810533  Accuracy :  0.9941704035874439 Time  1.04 s\n",
            "Epoch :  11  Batch :  224  Loss :   0.021211396468958452  Accuracy :  0.9941964285714285 Time  1.04 s\n",
            "Epoch :  11  Batch :  225  Loss :   0.021180734575015472  Accuracy :  0.9942222222222222 Time  1.04 s\n",
            "Epoch :  11  Batch :  226  Loss :   0.021133716505207616  Accuracy :  0.9942477876106195 Time  1.06 s\n",
            "Epoch :  11  Batch :  227  Loss :   0.021046659814429016  Accuracy :  0.994273127753304 Time  1.07 s\n",
            "Epoch :  11  Batch :  228  Loss :   0.02095811841959722  Accuracy :  0.9942982456140351 Time  1.06 s\n",
            "Epoch :  11  Batch :  229  Loss :   0.020867882591536286  Accuracy :  0.9943231441048035 Time  1.05 s\n",
            "Epoch :  11  Batch :  230  Loss :   0.020818796189257187  Accuracy :  0.9943478260869565 Time  1.05 s\n",
            "Epoch :  11  Batch :  231  Loss :   0.02073168763975355  Accuracy :  0.9943722943722944 Time  1.04 s\n",
            "Epoch :  11  Batch :  232  Loss :   0.020651350405250138  Accuracy :  0.9943965517241379 Time  1.05 s\n",
            "Epoch :  11  Batch :  233  Loss :   0.020564591292731842  Accuracy :  0.9944206008583691 Time  1.04 s\n",
            "Epoch :  11  Batch :  234  Loss :   0.02059179637231693  Accuracy :  0.9944444444444445 Time  1.04 s\n",
            "Epoch :  11  Batch :  235  Loss :   0.020567859150953077  Accuracy :  0.994468085106383 Time  1.04 s\n",
            "Epoch :  11  Batch :  236  Loss :   0.020483623977227933  Accuracy :  0.9944915254237288 Time  1.04 s\n",
            "Epoch :  11  Batch :  237  Loss :   0.020422206089559056  Accuracy :  0.9945147679324895 Time  1.06 s\n",
            "Epoch :  11  Batch :  238  Loss :   0.020340161222685012  Accuracy :  0.9945378151260504 Time  1.06 s\n",
            "Epoch :  11  Batch :  239  Loss :   0.02027415610516518  Accuracy :  0.9945606694560669 Time  1.06 s\n",
            "Epoch :  11  Batch :  240  Loss :   0.020194568228653984  Accuracy :  0.9945833333333334 Time  1.04 s\n",
            "Epoch :  11  Batch :  241  Loss :   0.0201305068632633  Accuracy :  0.9946058091286307 Time  1.05 s\n",
            "Epoch :  11  Batch :  242  Loss :   0.02005772156161591  Accuracy :  0.9946280991735538 Time  1.05 s\n",
            "Epoch :  11  Batch :  243  Loss :   0.01997850853431261  Accuracy :  0.9946502057613169 Time  1.04 s\n",
            "Epoch :  11  Batch :  244  Loss :   0.019959673549793167  Accuracy :  0.9946721311475409 Time  1.04 s\n",
            "Epoch :  11  Batch :  245  Loss :   0.019878590209172754  Accuracy :  0.9946938775510205 Time  1.04 s\n",
            "Epoch :  11  Batch :  246  Loss :   0.01980082915127692  Accuracy :  0.9947154471544716 Time  1.05 s\n",
            "Epoch :  11  Batch :  247  Loss :   0.019723196936770125  Accuracy :  0.9947368421052631 Time  1.04 s\n",
            "Epoch :  11  Batch :  248  Loss :   0.019646794756831988  Accuracy :  0.994758064516129 Time  1.05 s\n",
            "Epoch :  11  Batch :  249  Loss :   0.019678403720696135  Accuracy :  0.9947791164658635 Time  1.06 s\n",
            "Epoch :  11  Batch :  250  Loss :   0.019603434168315288  Accuracy :  0.9948 Time  1.07 s\n",
            "Epoch :  11  Batch :  251  Loss :   0.01952676725299337  Accuracy :  0.9948207171314741 Time  1.05 s\n",
            "Epoch :  11  Batch :  252  Loss :   0.019451034452632007  Accuracy :  0.9948412698412699 Time  1.05 s\n",
            "Epoch :  11  Batch :  253  Loss :   0.01938388428403313  Accuracy :  0.9948616600790514 Time  1.06 s\n",
            "Epoch :  11  Batch :  254  Loss :   0.019312311168788277  Accuracy :  0.9948818897637796 Time  1.05 s\n",
            "Epoch :  11  Batch :  255  Loss :   0.019246957362128003  Accuracy :  0.9949019607843137 Time  1.05 s\n",
            "Epoch :  11  Batch :  256  Loss :   0.01917383699792552  Accuracy :  0.994921875 Time  1.05 s\n",
            "Epoch :  11  Batch :  257  Loss :   0.019100731153948693  Accuracy :  0.9949416342412452 Time  1.05 s\n",
            "Epoch :  11  Batch :  258  Loss :   0.01903995181844088  Accuracy :  0.9949612403100775 Time  1.04 s\n",
            "Epoch :  11  Batch :  259  Loss :   0.018974504875344418  Accuracy :  0.994980694980695 Time  1.06 s\n",
            "Epoch :  11  Batch :  260  Loss :   0.019060755506216782  Accuracy :  0.995 Time  1.06 s\n",
            "Epoch :  11  Batch :  261  Loss :   0.019080886838024367  Accuracy :  0.9950191570881226 Time  1.06 s\n",
            "Epoch :  11  Batch :  262  Loss :   0.019013596456858513  Accuracy :  0.9950381679389313 Time  1.05 s\n",
            "Epoch :  11  Batch :  263  Loss :   0.01894377389966629  Accuracy :  0.9950570342205323 Time  1.05 s\n",
            "Epoch :  11  Batch :  264  Loss :   0.018874034831617682  Accuracy :  0.9950757575757576 Time  1.05 s\n",
            "Epoch :  11  Batch :  265  Loss :   0.01880425053097394  Accuracy :  0.9950943396226415 Time  1.05 s\n",
            "Epoch :  11  Batch :  266  Loss :   0.018733794706804474  Accuracy :  0.9951127819548872 Time  1.05 s\n",
            "Epoch :  11  Batch :  267  Loss :   0.018681496043812532  Accuracy :  0.9951310861423222 Time  1.04 s\n",
            "Epoch :  11  Batch :  268  Loss :   0.018613034952438025  Accuracy :  0.9951492537313433 Time  1.04 s\n",
            "Epoch :  11  Batch :  269  Loss :   0.018546320579096973  Accuracy :  0.9951672862453531 Time  1.05 s\n",
            "Epoch :  11  Batch :  270  Loss :   0.018477999320900076  Accuracy :  0.9951851851851852 Time  1.06 s\n",
            "Epoch :  11  Batch :  271  Loss :   0.018410862240389212  Accuracy :  0.9952029520295202 Time  1.06 s\n",
            "Epoch :  11  Batch :  272  Loss :   0.018344323244240755  Accuracy :  0.9952205882352941 Time  1.06 s\n",
            "Epoch :  11  Batch :  273  Loss :   0.01827897655175048  Accuracy :  0.9952380952380953 Time  1.05 s\n",
            "Epoch :  11  Batch :  274  Loss :   0.018214736713039168  Accuracy :  0.9952554744525547 Time  1.05 s\n",
            "Epoch :  11  Batch :  275  Loss :   0.018154989246147373  Accuracy :  0.9952727272727273 Time  1.05 s\n",
            "Epoch :  11  Batch :  276  Loss :   0.018090464091986745  Accuracy :  0.9952898550724638 Time  1.05 s\n",
            "Epoch :  11  Batch :  277  Loss :   0.018025336552990164  Accuracy :  0.9953068592057762 Time  1.05 s\n",
            "Epoch :  11  Batch :  278  Loss :   0.017974788290777345  Accuracy :  0.9953237410071942 Time  1.05 s\n",
            "Epoch :  11  Batch :  279  Loss :   0.017914656129906562  Accuracy :  0.9953405017921146 Time  1.04 s\n",
            "Epoch :  11  Batch :  280  Loss :   0.017896355273611723  Accuracy :  0.9953571428571428 Time  1.04 s\n",
            "Epoch :  11  Batch :  281  Loss :   0.017832935130185648  Accuracy :  0.995373665480427 Time  1.07 s\n",
            "Epoch :  11  Batch :  282  Loss :   0.01777818743734805  Accuracy :  0.9953900709219858 Time  1.06 s\n",
            "Epoch :  11  Batch :  283  Loss :   0.017718793987551693  Accuracy :  0.9954063604240283 Time  1.06 s\n",
            "Epoch :  11  Batch :  284  Loss :   0.017688931922213644  Accuracy :  0.9954225352112676 Time  1.05 s\n",
            "Epoch :  11  Batch :  285  Loss :   0.017629898281433142  Accuracy :  0.9954385964912281 Time  1.05 s\n",
            "Epoch :  11  Batch :  286  Loss :   0.017572235554901332  Accuracy :  0.9954545454545455 Time  1.05 s\n",
            "Epoch :  11  Batch :  287  Loss :   0.0175128632178937  Accuracy :  0.9954703832752613 Time  1.04 s\n",
            "Epoch :  11  Batch :  288  Loss :   0.01745458702915749  Accuracy :  0.9954861111111111 Time  1.04 s\n",
            "Epoch :  11  Batch :  289  Loss :   0.017395050568952575  Accuracy :  0.9955017301038063 Time  1.05 s\n",
            "Epoch :  11  Batch :  290  Loss :   0.017335483831971817  Accuracy :  0.9955172413793103 Time  1.04 s\n",
            "Epoch :  11  Batch :  291  Loss :   0.017281777048939677  Accuracy :  0.99553264604811 Time  1.05 s\n",
            "Epoch :  11  Batch :  292  Loss :   0.017223601946248543  Accuracy :  0.9955479452054794 Time  1.06 s\n",
            "Epoch :  11  Batch :  293  Loss :   0.017192030810987947  Accuracy :  0.9955631399317406 Time  1.06 s\n",
            "Epoch :  11  Batch :  294  Loss :   0.017135477962525254  Accuracy :  0.995578231292517 Time  1.06 s\n",
            "Epoch :  11  Batch :  295  Loss :   0.01707858410153661  Accuracy :  0.995593220338983 Time  1.05 s\n",
            "Epoch :  11  Batch :  296  Loss :   0.017021549812831247  Accuracy :  0.9956081081081081 Time  1.04 s\n",
            "Epoch :  11  Batch :  297  Loss :   0.016968545038182636  Accuracy :  0.9956228956228956 Time  1.04 s\n",
            "Epoch :  11  Batch :  298  Loss :   0.016911959972218248  Accuracy :  0.9956375838926175 Time  1.04 s\n",
            "Epoch :  11  Batch :  299  Loss :   0.016871549546978414  Accuracy :  0.9956521739130435 Time  1.05 s\n",
            "Epoch :  11  Batch :  300  Loss :   0.016857405105914342  Accuracy :  0.9956666666666667 Time  1.05 s\n",
            "Epoch :  11  Batch :  301  Loss :   0.01680654132370331  Accuracy :  0.9956810631229236 Time  1.04 s\n",
            "Epoch :  11  Batch :  302  Loss :   0.01675586495016342  Accuracy :  0.9956953642384105 Time  1.05 s\n",
            "Epoch :  11  Batch :  303  Loss :   0.01670143221600474  Accuracy :  0.9957095709570957 Time  1.06 s\n",
            "Epoch :  11  Batch :  304  Loss :   0.016647148370772788  Accuracy :  0.9957236842105263 Time  1.07 s\n",
            "Epoch :  11  Batch :  305  Loss :   0.01661890597871639  Accuracy :  0.9957377049180328 Time  1.06 s\n",
            "Epoch :  11  Batch :  306  Loss :   0.016566959108188836  Accuracy :  0.9957516339869281 Time  1.05 s\n",
            "Epoch :  11  Batch :  307  Loss :   0.016514997169438484  Accuracy :  0.9957585644371941 Time  0.54 s\n",
            "Accuracy of     0 : 99 %\n",
            "Accuracy of     1 : 99 %\n",
            "Accuracy of     2 : 99 %\n",
            "[11 epoch] Accuracy of the network on the Training images: 99 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation of accuracy and loss"
      ],
      "metadata": {
        "id": "4zHz8umjDcyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OWre-m2htCtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist_loss = []\n",
        "for i in history_loss:\n",
        "  j=i.cpu().detach().numpy()\n",
        "  hist_loss.append(j)"
      ],
      "metadata": {
        "id": "cksuKNXXuCvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist_loss"
      ],
      "metadata": {
        "id": "2ftBUd45uqZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(history_accuracy)\n",
        "plt.plot(hist_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "nXQBAdWDvAi9",
        "outputId": "1e6a6e94-b678-4841-dd53-50dcd20db867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f291ac04670>]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig(\"LossAndAccuracy\",dpi=2048, format = png)"
      ],
      "metadata": {
        "id": "JOavE2vT71TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "9f7LzOiLDndQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC6c552AtA5x",
        "outputId": "7939ad2b-27bb-42b2-df42-10d3986adf42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (_conv_stem): Conv2dStaticSamePadding(\n",
              "    3, 48, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
              "    (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "  )\n",
              "  (_bn0): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_blocks): ModuleList(\n",
              "    (0): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        48, 48, kernel_size=(3, 3), stride=[1, 1], groups=48, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        48, 12, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        12, 48, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (1): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (2): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (3): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (4): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (5): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (6): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (7): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (8): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (9): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(5, 5), stride=(1, 1), groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 56, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(56, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (10): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        56, 336, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        336, 336, kernel_size=(3, 3), stride=[2, 2], groups=336, bias=False\n",
              "        (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(336, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        336, 14, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        14, 336, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        336, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (11): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (12): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (13): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (14): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (15): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(3, 3), stride=(1, 1), groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (16): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        672, 672, kernel_size=(5, 5), stride=[1, 1], groups=672, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (17): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (18): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (19): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (20): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (21): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=(1, 1), groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (22): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        960, 960, kernel_size=(5, 5), stride=[2, 2], groups=960, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        960, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (23): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (24): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (25): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (26): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (27): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (28): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (29): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(5, 5), stride=(1, 1), groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 272, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(272, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (30): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        272, 1632, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1632, 1632, kernel_size=(3, 3), stride=[1, 1], groups=1632, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1632, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1632, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        68, 1632, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1632, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (31): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        448, 2688, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        2688, 2688, kernel_size=(3, 3), stride=(1, 1), groups=2688, bias=False\n",
              "        (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(2688, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        2688, 112, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        112, 2688, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        2688, 448, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(448, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "  )\n",
              "  (_conv_head): Conv2dStaticSamePadding(\n",
              "    448, 1792, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "    (static_padding): Identity()\n",
              "  )\n",
              "  (_bn1): BatchNorm2d(1792, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
              "  (_dropout): Dropout(p=0.4, inplace=False)\n",
              "  (_fc): Linear(in_features=1792, out_features=3, bias=True)\n",
              "  (_swish): MemoryEfficientSwish()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "predictions=[]\n",
        "actuals=[]\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images,labels = images.to(device),labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        pred_arr = predicted.cpu().detach().numpy()\n",
        "        actual_arr = labels.cpu().detach().numpy()\n",
        "        predictions.append(pred_arr)\n",
        "        actuals.append(actual_arr)\n",
        "\n",
        "print('Accuracy of the network on the test images: %f %%'% (100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZTSacoKvVW_",
        "outputId": "8be7c260-8de1-4c28-ce09-13feaa9300b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 99.282453 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred=np.concatenate(predictions)\n",
        "act=np.concatenate(actuals)"
      ],
      "metadata": {
        "id": "6b2tfQ7ssygo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_arr = predicted.cpu().detach().numpy()\n",
        "actual_arr = labels.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "azXaaWOBsyeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(pred,act,rownames=['Predicted'], colnames=['Actual'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "5iE5Mxy7sybc",
        "outputId": "a57ffb9b-1534-49cd-9c46-2e08d880306b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Actual       0    1    2\n",
              "Predicted               \n",
              "0          702    2    0\n",
              "1            0  576    3\n",
              "2            1    5  244"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2be1c70d-c494-4a83-b5a7-f52f1601137a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Actual</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Predicted</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>702</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>576</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2be1c70d-c494-4a83-b5a7-f52f1601137a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2be1c70d-c494-4a83-b5a7-f52f1601137a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2be1c70d-c494-4a83-b5a7-f52f1601137a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datavar.classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTMP4GsPsyY1",
        "outputId": "11dbf1bc-f32f-4884-d8d0-e1ba6392610a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Covid', 'Normal', 'Pneumonia']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datavar.class_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-mOtYeBsyWN",
        "outputId": "6b4a4935-27a0-4056-ec2b-7b3e425980b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Covid': 0, 'Normal': 1, 'Pneumonia': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pretrained model into file"
      ],
      "metadata": {
        "id": "sPFgAo8y0hEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = '/content/EN4-epoch 10 model.pth'"
      ],
      "metadata": {
        "id": "ThCIRBFsz6Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(file)"
      ],
      "metadata": {
        "id": "8bsj7noozJqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = en4.from_pretrained('efficientnet-b4', num_classes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsxuBs8BsyTL",
        "outputId": "c83aa75d-9a03-49e8-ab2d-c921bf96fb15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer2 = optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "x1UI27CssyQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer2.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint[\"epoch\"]\n",
        "loss = checkpoint['loss']"
      ],
      "metadata": {
        "id": "TdiPRQIhsyOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_save = '/content/drive/MyDrive/Import to Colab/Models/E4-10epoch-batch16.pth'\n",
        "# '/content/drive/MyDrive/Capstone Data (Shared)/Capstone Data/TrainedModelCheckpoints/E4-10epoch-batch16.pth'"
      ],
      "metadata": {
        "id": "F1fF-yax4y5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, file_save)"
      ],
      "metadata": {
        "id": "8GAVfAKf4uXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "\n",
        "correct2 = 0\n",
        "total2 = 0\n",
        "predictions2=[]\n",
        "actuals2=[]\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images,labels = images.to(device),labels.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = torch.argmax(outputs.data, 1)\n",
        "        total2 += labels.size(0)\n",
        "        correct2 += (predicted == labels).sum().item()\n",
        "        pred_arr2 = predicted.cpu().detach().numpy()\n",
        "        actual_arr2 = labels.cpu().detach().numpy()\n",
        "        predictions2.append(pred_arr2)\n",
        "        actuals2.append(actual_arr2)\n",
        "\n",
        "print('Accuracy of the network on the test images: %f %%'% (100 * correct2 / total2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2zMs9Y9syLp",
        "outputId": "3583c9f0-cf2d-4090-e24f-1fd3e0d811f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test images: 99.282453 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred2=np.concatenate(predictions2)\n",
        "act2=np.concatenate(actuals2)"
      ],
      "metadata": {
        "id": "j3-Z9tnmsyJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_arr2 = predicted.cpu().detach().numpy()\n",
        "actual_arr2 = labels.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "aFOcjrA5syGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(pred2,act2,rownames=['Predicted'], colnames=['Actual'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "QNiTtJlWzdfz",
        "outputId": "34b4dfc5-1e09-4e32-91c9-8ef80c89ce3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Actual       0    1    2\n",
              "Predicted               \n",
              "0          702    2    0\n",
              "1            0  576    3\n",
              "2            1    5  244"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cb05f0ee-e5fd-48fb-87b0-29da891b2b64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Actual</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Predicted</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>702</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>576</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb05f0ee-e5fd-48fb-87b0-29da891b2b64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cb05f0ee-e5fd-48fb-87b0-29da891b2b64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cb05f0ee-e5fd-48fb-87b0-29da891b2b64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ]
}